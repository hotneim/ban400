[
  {
    "objectID": "01-intro-to-r.html",
    "href": "01-intro-to-r.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "Welcome to the first taste of BAN400. We will start by downloading and installing the tools that we need to start coding, and then we will explore some of the most basic aspects of the R programming language. After most of the videos we have included a small problem that we encourage you to try before you move on to the next topic.\nBefore we start, you need to install two items on your computer. Please do the installations in the following order:\n\nThe R Programming Language: Navigate to cran.uib.no and download the version of R that corresponds to your operating system. Run the installation as you would for any other program that you install on your system.\nRStudio: Navigate to posit.co/download/rstudio-desktop/, and download the version of “RStudio Desktop” that corresponds to your operating system. Run the installation as you would for any other program that you install on your computer.\n\nBoth R and RStudio are free to download and free to use.\n\n\n\n\nIn this video we open up RStudio for the first time and take a small tour of the user interface.\n\n\n\n\n\nWe move on to write our first R commands. It is critical that you already now start to feel the programming, and you do that best by typing in the code lines just as in the video above (no copy/paste!), making sure that you get the same results.\n\n# We can use R as a calculator:\n2+2\n\n# Pretty simple! We must use paratheses if we have more complicated expressions:\n(2+8)/2\n2+8/2\n\n# Variables are important in R. We can save just about anything inside the\n# computer memory by giving them names:\na <- 5\na\na*4 \n\nb <- 3\n\n# R performs all operations on the right hand side before assigning the value to c:\nc <- a+b\nc\n\n# No errors, warnings or questions when overwriting!\nc <- 4\n\nc <- c + 2\nc\n\n# Let's make an error!\nd\n\n# We can name things more or less what we want. Not a non-trivial problem in\n# large projects!\nwhatever_we_want <- \"hello world\"\nwhatever_we_want\n\nExercise:\nPick your favorite three integers and store them in three different variables. Calculate yor magic number, which is the sum of these three integers. Store your magic number in a new variable. Give your new variable a name that clearly identifies what it is.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nnumber1 <- 1\nnumber2 <- 87\nnumber3 <- 101\n\nmagic_number <- number1 + number2 + number3\n\n\n\n\n\n\n\n\n\nVectors are very important in R. We remember perhaps from our math classes that vectors may represent points in space; in R it is a way to store more than one number (or string, or some other data type) under a single variable name. When doing statistics, this may for example be a set of observations.\nIn this video we first create a vector of numbers using the c()-function, and then we look at various ways to extract/pick out the elements: to subset. Python coders will notice two important distinctions from what they are used to:\n\nIn R we start counting on 1, and not 0!\nWhen trying to subset using an index that out of the range of the vector, we do not get an error message, we just get back the empty value NA.\n\nFurthermore, we use the Up-arrow to get back the last command that we have executed in the console. You can even tap the up-arrow again in order go further back in your command history (and of course use the down-arrow to navigate the other way).\n\n# We can make a vector in the following way:\nvector1 <- c(3, 5, 7.8, 10, 2, 0.16, -3)\n\n# Print out\nvector1\n\n# Subsetting (The first item has index 1!)\nvector1[1]        # Square brackets to subset\nvector1[10]       # Out-of-range error\nvector1[2:5]      # Subset a sequence\nvector1[c(1,3)]   # Subset using another vector!\n\n# The letter \"c\" stands for \"combine\". R makes it very easy to work with\n# vectors:\nvector1 - 1\nvector1*3\n\n# We can use *functions* to calculate various things:\nlength(vector1)\nmean(vector1)\nsum(vector1)\nsd(vector1)\n\n# We can make a vector of strings as well:\nvector2 <- c(\"hello\", \"world\")\n\n# A vector can only contain one data type!\n\n# Perhaps we need the standard deviation later?\nsd_vector1 <- sd(vector1)\nsd_vector1\n\nExercise:\nCalculate the maximum and minimum values of vector1, as well as the median. (Hint, and this will be the most important lesson you will learn in this course: If you do not know the name of the function, Google it!)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# Relevant Google searches: \"minimum value r\", \"maximum r\", \"median r\"\n\nmin(vector1)\nmax(vector1)\nmedian(vector1)\n\n\n\n\n\n\n\n\n\nIn this video we install our first package in R. There are two major takeaways from this:\n\nWe install the package on our computer using the install.packages()-function. We only have to do this once per computer.\nIf we are going to use some of the functions in a package we need to load it using the library()-command. You have to do that every time you restart R (and we will later see that we will typically load all the packages we need in the beginning of the scripts that we write).\n\n\n# In order to install the package readxl, we run the following command. \n# We run this command only once.\ninstall.packages(\"readxl\")\n\n# When we are going to use it, we load it using the \"library()\"-function, \n# and we need to repreat this every time we restart R.\nlibrary(readxl)\n\nExercise: Install the following packages. We will make use of them (and several others) later in the course: ggplot2, dplyr, tidyr and lubridate.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\n\n\nWe introduce the concept of a working directory, which is the folder where R looks for files that we are going to read into the memory, and where R puts the files that we create, for instance image files of plots.\nThere are two central functions:\n\ngetwd() prints out the current working directory.\nsetwd(\"C:/path/to/folder\") sets the working directory to the specified folder. We will as a general rule not use setwd() in our scripts (the reason for that will become clear later), but rather use RStudio’s menu system for changing the working directory (we will in practice not need to do that as a general rule as well, which will also become clear in a short while).\n\nWe may however have to deal with file paths in our code, and make the following technical notes:\n\nOn UNIX systems (such as Mac or Linux) the file paths look differently, they do not start with a drive letter such as C:\\.\nOn Windows, we always use the backslash / to separate between the folders in R code, and not the usual forward slash \\. This may be counter-intuitive to some, but in programming the forward slash usually has special meaning (the escape character) and must not be used for anything else. On UNIX systems we also use the backslash, but that is the system standard for writing file paths, so it does not require any special attention to users of those operating systems.\n\nExercise: Make sure that you have completed the following tasks before proceding to the next lesson:\n\nYou have created a dedicated folder on your computer where you will collected all material that we will use today.\nYou have downloaded the file testdata.xls and put in in your newly created folder.\nYou have changed your working directory to this folder.\nYou have positively confirmed that your working directory now is correctly set.\n\n\n\n\n\n\nWe read our first small data file into the memory of R and apply some simple operations to it. We will spend much more time working with data in R in later lessons.\n\n# The data is in the .xls-format, so we need the readxl-package in order to load \n# it into R.\nlibrary(readxl)\n\n# Inside this package, there is a function called read_excel:\nread_excel(\"testdata.xls\")\n\n# That's fine, but in order to use this data, we need to save it in a variable\ntestdata <- read_excel(\"testdata.xls\")\n\n# Print out the (top of the) data set.\ntestdata\n\n# Now we see the data in the environment. We can look at it by typing the name that we\n# gave it. We can also pick out individual columns using the $-sign:\ntestdata$X1\n\n# Calaculate the mean for X1 and X2:\nmean(testdata$X1)\nmean(testdata$X2)\n\n# How many rows/observations do we have?\nnrow(testdata)\n\nExercise:\n\nHow many columns does our data set have?\nCan you find a way to print out a vector that contains the sum of the X1 and X2 columns in testdata?\nWhat is the total sum of all the numbers in the X1 and X2 columns of testdata?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nncol(testdata)\n\n# 2 \ntestdata$X1 + testdata$X2\n\n# 3\nsum(testdata$X1 + testdata$X2)\n\n\n\n\n\n\n\n\n\nWe introduce the main package for the plotting engine that we will use in this course; ggplot2, and its basic syntax.\n\n# A simple scatterplot\nplot(testdata$X1, testdata$X2)\n\n# Making adjustments to the plot\nplot(testdata$X1, testdata$X2,\n     pch = 20,\n     bty = \"l\",\n     xlab = \"X1\",\n     ylab = \"X2\")\n\n# Load the ggplot2-package\nlibrary(ggplot2)\n\n# Here is the code for creating a simple scatterplot of the X1 and X2 columns in\n# our data set:\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\n\n# First make the plot, then save it to a file\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\")\n\n# A more flexible way to do it is to save the plot in a variable, and then\n# supply the name of the plot to the ggsave-finction:\np <- ggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\", p)\n\n# That way, we can save the plot p at any time, we do not have to do it directly\n# after the plotting commands.\n\nExercise: Can you figure out how to make the dots in the plot bigger and blue?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nggplot(testdata, aes(x = X1, y = X2)) + geom_point(colour = \"blue\", size = 5)\n\n\n\n\n\n\n\n\n\nIn this video we stop writing code directly in the console, and rather write our code in a script file, which is simply a pure text file containing commands. There are two important new concepts that we have to pay attention to when writing scripts:\n\nThe comment character #: Evertything after this character in an R-script is ignored when executing the script. We can use the comment character to add small comments to our code, briefly explaining what is going on. This is a great help for other people trying to understand what you have done, in particular, and perhaps most importantly: the future you returning to a project. The comment character # is the same as in Python.\nThe keybinding Ctrl - Enter (Cmd - Enter on a Mac) executes the line where your cursor is located in the script. You can also select several lines and execute all of them using this shortcut.\n\n\n# Introduction to R\n# -------------------\n\n# Load packages \nlibrary(readxl)\nlibrary(ggplot2)\n\n# Read our data set\ntestdata <- read_xls(\"testdata.xls\")\n\n# Make a scatterplot of the X1 and X2-variables\nplot <- ggplot(testdata, aes(x = X1, y = X2)) + \n    geom_point() + \n    ggtitle(\"Scatterplot of testdata\") +\n    theme_classic()\nggsave(\"testplot.pdf\", plot)\n\nExercise: Make sure to save your script as an .R-file in the folder that we created for this session. Close RStudio. Then navigate to the folder and double click on the script file. Hopefully RStudio opens (if not, right click, select “Open in” and then Rstudio, confirm if prompted to set RStudio as default program for opening .R-files).\nFind out what your working directory is now. What just happened? How is this useful?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nOpening RStudio by double clicking on the script file automatically sets the working directory to the location of the script file. Very useful when returning to a project.\n\n\n\n\n\n\n\n\nWe introduce the pipe operator %>%, which allows us to apply a sequence of functions in a practical way that allows us to read the code from left to right, just as we read normal text.\nIf you installed the dplyr-package in the “Packages”-section, then you have already installed the magrittr-package that includes the pipe. The shortcut for the pipe in RStudio is Ctrl - Shift - M on Windows, and Cmd - Shift - M on a Mac.\nWe will use this technique extensively throughout the course!\n\n# For this lesson, you need to have magrittr installed. Run the line \n# below if you don't have it already: \n# install.packages(\"magrittr\")\n\n# Create some data:\nx <- (-500):500\n\n# Want to calculate mean of sqrt of abs values of x...\n# one way: via temporary variables: \nabs.x <- abs(x)\nsqrt.abs.x <- sqrt(abs.x)\nalt.1 <- mean(sqrt.abs.x)\n\n# ...or collapse them all with nested functions: \nalt.2 <- mean(sqrt(abs(x)))\n\nlibrary(magrittr)\n# With Magrittr: \n# x %>% f() is equivalent to  f(x)\n# Why would we want to do this: \n#  - code becomes more easy to read\n#  - easier to develop code, particularly when dealing with data sets\n\n#  Example 1\nsqrt(2)\n2 %>% sqrt\n\n#  Example 2\nmean(c(1,2,NA), na.rm=T)\nc(1,2,NA) %>% mean(na.rm=T)\n\n# Example 3\n(-2)^2\n-2 %>% .^2\n\n# Example 4\natan2(1,2)\n2 %>% atan2(1,.)\n\nExercise:\nUse magrittr to calculate the equivalent of alt.1 and alt.2\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nalt.3 <- \n  x %>% \n  abs %>% \n  sqrt %>% \n  mean\n\nalt.1\nalt.2\nalt.3"
  },
  {
    "objectID": "02-data-wrangling.html",
    "href": "02-data-wrangling.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "In this section we will learn how to handle data in a very efficient way. We will learn to\n\nfilter a data set based on variable values,\nselect variables,\ncreate new variables,\ngroup data based on variables,\ndummarise the data, and\njoin different data sets.\n\nWe will obviously use R to solve these problems, but we do have the choice between different coding styles to do it. One way is to only use functions that already ship with R, or we can use functions from additional packages to solve the same problems. We choose the latter, and not only that: We will thoughout this course use a specific set of packages, an ecosystem if you wish (or even a philosophy of data work), called the tidyverse.\nThe tidyverse (tidyverse.org) is a set of R packages that work very well together, follows a consistent logic, and that enables us to write extremely clean code. We have already touched upon the difference between data wrangling using base R and the tidyverse in Chapter 1.9.\nSee the video below for some more details regarding the tidyverse. Some formulations in the video gives the impression that this material should be consumed on a specific day, but that are just some residue from a time when this part of the course was given intensively. You can find the cheat sheet here. See also this webpage for further information about one of the most central packages in tidyverse, dplyr.\n\n\n\n\n\n\nFirst, we warm up with a first few functions from dplyr. Note how the syntax is similar to how we use the magrittr-pipe %>%. You can download the data set here: data-geilo.xlsx.\nExercise: Report the top of the dataset, sorted by\n\ncocoa (increasing)\nswix (decreasing)\norange (decreasing)\n\nYou should obtain a result equal to the data frame below. Note we only show the first six entries of the result.\n\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1    21       42      5     0     7\n2    19        7      4     0     2\n3    12       10      3     0     0\n4    20       NA      1     1     2\n5     5       NA      3     1     1\n6     9       NA      2     1     1\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nsales %>%\n  arrange(cocoa, desc(swix), desc(orange)) %>%\n  head\n\n# Alternatively: \n sales %>%\n   arrange(cocoa, -swix, -orange) %>%\n   head\n \n# Note however that (-) requires num. vectors. desc can take e.g. factors as well.)\n\n\n\n\n\n\n\n\n\n\n# Selecting some variables: \nhead(sales[,c(\"swix\", \"cocoa\")])\n\nsales %>%\n  select(cocoa, swix) %>%\n  head\n\n# create new variables: \nsales %>%\n  mutate(items = cocoa+swix+orange) %>%\n  head\n\nsales %>%\n  transmute(\n    items = cocoa+swix+orange,\n    trans = trans) %>%\n  head\n\n# Summarise data: \nsales %>%\n  summarise(sum_cocoa = sum(cocoa))\n\n# Assignment 1: how many items were sold in total?\n# Assignment 2: What was the max and min number of items bought by\n# the people that also bought cocoa?\n\nExercise:\n\nHow many items were sold in total?\nWhat is the min. and max. number of items purchased by customers that also bought at least one cocoa?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Exercise 1: \n\nsales %>%\n  transmute(items = cocoa + swix + orange) %>%\n  summarise(sum_items = sum(items))\n\n# Exercise 2: \nsales %>%\n  filter(cocoa>0) %>%\n  transmute(\n    items = cocoa + swix + orange) %>%\n  summarise(\n    max_items = max(items),\n    min_items = min(items)\n  )\n\n\n\n\n\n\n\n\n\n\n# Compare the output from this command: \nsales\n\n# ..to this one: \nsales %>% group_by(customer)\n\n# Note how we in the second command have added some meta-information on groups\n# to the data frame. Groups change the results we get when applying\n# functions to the data frame. See below: \n\nsales %>%\n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\nsales %>%\n  group_by(customer) %>%\n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\n# When we apply \"summarise\" to a data frame we are reducing it \n# to the summary statistics that we list in the call to the function. \n# In the command above, this is a sum and a count. There are many\n# such functions we can use - just keep in mind that the function\n# should return one item per group (or just one item if you don't\n# have groups). \n# \n# Make sure you are aware of the difference between mutate and \n# summarise: mutate *adds* a variable to the data frame, \n# summarise aggregates it. \n\nExercise: How many cocoas were bought in total by customers who also bought more than two oranges? (Hint: How does mutate work on a grouped data frame?)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nsales %>%\n  group_by(customer) %>%\n  mutate(sum.orange = sum(orange)) %>%\n  filter(sum.orange > 2) %>%\n  summarise(sum.cocoa = sum(cocoa))\n\n\n\n\n\n\n\n\n\nFinally, we will join data frames. See the data wrangling cheat sheet, under the header “Combine data sets”.\n\n# Read in both sheets\nsales <- read_excel(\"../datasett/Geilo.xlsx\", sheet=\"Sales\")\ncustomers <- read_excel(\"../datasett/Geilo.xlsx\", sheet=\"Customers\")\n\n# See that sales stores transactions, as well as a link to a \n# customer number: \nhead(sales)\n\n# In the \"customers\"-file we find info on each customer: \nhead(customers)\n\nExercise: Use the dplyr join verbs to create the following four different results:\n\nA dataframe with transactions and customer info filled in\nA dataframe with transactions for all registered customers\nA dataframe with transactions for customers that are not registered\nA dataframe that combines all the information in both data sets\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nsales %>%\n  left_join(customers, by=\"customer\") %>%\n  group_by(hotel) %>% \n  summarise(\n    sum_orange = sum(orange))\n\n  arrange(customer) %>%\n  head\n\n# 1: \nsales %>%\n  left_join(customers, by=\"customer\") %>%\n  arrange(customer) %>%\n  head\n\n# 2: \nsales %>%\n  semi_join(customers, by=\"customer\") %>%\n  arrange(customer) %>%\n  head\n\n# 3: (why does this give different res. than is.na()?)\nsales %>%\n  anti_join(customers, by=\"customer\") %>%\n  arrange(customer) %>%\n  head\n\n# 4:\nfull <- \n  sales %>%\n  full_join(customers, by = \"customer\")\n\n\n\n\n\n\n\nA word of caution on tidyverse: dplyr is somewhat contested among R-users. Some claim it is very easy to learn (although I’m not aware of any studies). Critics argue that it is slow compared to data.table. data.tablecan (sometimes) beat Python/Pandas in terms of spee not, but dplyr can not do that. Note also that the tidyverse ecosystem in general is continually being updated. This means that if you try to run the commands from this course on a fresh install of R and Tidyverse in a few years, it might not work unless you update the syntax.\nThink about your usage of programming: If it is occasional scripting, ad-hoc reports etc, then speed is often not important, and occasionally changing syntax might not be an issue.\nHowever, in my experience the tidyverse is significantly “faster” than alternatives for important use cases - which involve many exploratory analyses, data visualizations, and trying out many different ways of modelling a problem (i.e. usually what constrains your time may be how fast you can express what you want - not how long you need to wait for results).\nLet’s wrap up this chapter with a final exercise:\nExercise: Create a summary statistics with the following properties:\n\nCustomer on rows, with all customers as well as non-registered customers (non-registered can be in a single row)\nIn addition to customer numbers, four columns:\n\nNumber of transactions in total\nTotal sales of each type of item\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Why doesn't this work!?\n#(check e.g. customer nr 2!):\nfull %>% \n  arrange(customer) %>%\n  mutate(\n    customer=replace_na(customer, \"Unregistered\")\n  ) %>%\n  group_by(customer) %>%\n  summarise(count         = n(),\n            sale.orange   = sum(orange, na.rm=T),\n            sale.cocoa    = sum(cocoa   , na.rm=T),\n            sale.swix     = sum(swix    , na.rm=T))\n\n\n# A better way: \nsumstats <-\n  full %>%\n  arrange(customer) %>%\n  mutate(customer = replace_na(customer, \"Unregistered\")) %>%\n  group_by(customer) %>%\n  summarise(\n    count         = sum(!is.na(trans)),\n    sale.orange   = sum(orange, na.rm = T),\n    sale.cocoa    = sum(cocoa   , na.rm = T),\n    sale.swix     = sum(swix    , na.rm = T)\n  )"
  },
  {
    "objectID": "03-graphics.html",
    "href": "03-graphics.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will next explore the rich posibilities for making convincing visualizations in R; in particular using the ggplot2-package. We will start from the very basic syntax of making a plot, and then work our way towards making publication ready illustrations.\nThere are a lot of online resources for ggplot2, for example:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\nTwenty rules for good graphics, a useful blog post by statistician Rob Hyndman.\nThe books by Edward Tufte are classic works in the more general topic of data visualization.\n\n\n\n\n\nWe introduce the ggplot2-package and look at the basic syntax of that package and how it differs from the plotting engine that is built into the basic installation of R. Make sure that you have installed this package (using install.package(\"ggplot2\")) before proceeding to the rest of the lesson.\nWe can repeat the links that are mentioned at the end of the video:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\n\n\n\n\n\n\nNot only is ggplot2 picky in the sense that it only accepts data frames as the first argument to the ggplot()-function, we also need to be careful about the “shape” of the data frame. In this video we discuss the difference between storing data in the “wide” format versus the “long” format. We always prefer the latter when doing data science, in particular when using ggplot2 for creating visualizations.\nThe basic principle for the long format (which is discussed in length by Hadley Wickham, chief data scientist at RStudio and master mind of the tidyverse, in his article Tidy Data) is the following:\n\nOne observation is one row, one variable is one column.\n\nHere you can download the data set for this lesson: data-temps.csv.\n\nlibrary(ggplot2)      # For plotting\nlibrary(tidyr)        # For pivot_longer() and pivot_wider()\nlibrary(readr)        # For reading csv\nlibrary(dplyr)        # For pipe etc.\n\ntemps <- read_csv(\"data-temps.csv\")\n\nlong <-\n  temps %>% \n  pivot_longer(cols = -machine, \n               names_to = \"when\", \n               values_to = \"temperature\")\n\nlong %>% pivot_wider(id_cols = machine, \n                     names_from = when, \n                     values_from = temperature)\n\nggplot(long, aes(x = machine, y = temperature)) +\n  geom_point()\n\n# Color based on a variable (map a variable to color)\nggplot(long, aes(x = machine, y = temperature, colour = when)) +\n  geom_point()\n\n\n\n\n\n\nWe load the data that we are going to use in the lessons to follow. We look at hourly traffic volume over the Sotra bridge during 2017 and 2018. This bridge is particularly vulnerable to traffic jams during rush hour, which we will look at in more detail through visualizations using ggplot2.\nYou can download the data here: data-sotra.Rdata. Make sure that you have set the correct working directory and loaded the data set into R before proceeding to the next lesson.\n\nload(\"data-sotra.Rdata\")\nhead(traffic.df)\ntail(traffic.df)\n\n\n\n\n\n\n\n\n\nWe look at some basic plots for visualizing a single variable. First, we make a histogram and a density plot for the continuous variable hourly.volume, and then we look at two different ways to make bar plots for a categorical variable, using geom_bar() and geom_col(). In the first of these cases we provide ggplot() with the raw data set, so that the occurrences in the categorical variable has to be counted before being presented in the plot. In the second case we provide the counts directly, which is something that easily happens in practice.\nWe also consider the practical problem of sorting the bars in the plot according to their size. We did that by modifying the data directly using the fct_reorder()-function that can be found in the forcats package.\n\n# Histogram\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_histogram(bins = 100)\n\n# Density plot\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_density()\n\n# Bar plot for raw data, ggplot does the counting\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar()\n\n# Bar plot for finished counts. \n# (group_by() and summarise() are used to simulate that situation)\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\n# # Ordering the bars by size\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %>% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\nExercise:\nInvestigate the possibility of making the following modifications to the last bar plot that we made:\n\nFlip the coordinate system so that the bars become horizontal.\nReverse the order of the bars, so that the tallest (or now: longest) bar comes first.\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1. Just change the aesthetic mapping so that weekday.holiday now goes to the\n# y-axis, and the count goes to the x-axis:\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %>% \n  ggplot(aes(y = weekday.holiday, x = count)) +      # <- Change the aesthetic mapping\n  geom_col()\n\n\n\n# 2. A quick look at \"?fct_reorder()\" reveals that there is a \".desc\"-argument\n# in that function. Flipping that to TRUE does the trick.\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count, .desc = TRUE)) %>% \n  ggplot(aes(y = weekday.holiday, x = count)) +                # /\\\n  geom_col()                                                   # |  Add this argument\n\n\n\n\n\n\n\n\n\n\n\n\nWe move on to look at a couple of alternatives when visualizing two variables. The first one is the scatter plot (using geom_point()) with some useful options, and the second one is to add text labels to a plot.\nIn the latter example we see how we can add information from different data frames in the same plot, and also that we can update the aesthetic mapping on later layers if we need to do that.\nAgain, the cheat sheet reveals that we are barely scratching the surface, there are many more geom_*()-functions that we can use to create just about any plot that you can imagine!\n\n# The basic scatterplot with alpha and smoother\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# A data frame containing the information required to plot text labels on top of\n# the bars\ntext_labels <- \n  df.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(label = paste(count, \"days\"))\n\n# The final plot with text labels\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n\n\n\n\n\nCreating plots and graphs for various purposes takes more time than you think, not in the least because you may want to tweak all the small details so that it looks just perfect. The ggplot2-package (as well as various add-on packages) contain all the tools you will ever need for this (and many more). In this video we look at some ways to work with this. The most important thing in this lesson is perhaps not the functions that we use, but rather the process we follow when solving the problem using a combination of documentation, googling, trial and error.\n\ndf.traffic %>% \n  tail(n = 500) %>% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\nExercise: Filter out the last 24 hours instead, and make one tick mark for each hour showing the time on a date + 24 hour format (i.e 31.12.2018 12:00, 31.12.2018 13:00, etc). Make adjustments that make the labels readable. Also, add points for each observation\nHint 1: look at the options we deleted when adjusting the size of the labels.\nHint 2: Look at this blog post for more options when formatting date strings.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %>% \n  tail(n = 24) %>%                                    # Change the filtering\n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() + \n  geom_point() +                                      # Add points as well  \n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 hour\",                             # Change interval,\n                   labels = function(x) format(x, \"%d.%m%Y %H:%M\")) +  # format, and size \n  theme(axis.text.x = element_text(size = 7, angle = 90))              # of labels\n\n\n\n\n\n\n\n\n\n\n\n\nIt is important in many situations to be able to visualize group membership, and in this lesson we see two such princinples: using colour in a single plot by mapping the group membership variable to the colour dimension of the plot, and one where we use facet_wrap() in order to make one panel for each group.\n\n# Using colour\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# Using facet_wrap\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %>% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth() +\n  facet_wrap(~ weekday.holiday) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\nExercise:\nLet us make a different plot now. We want to make one line plot per day, and colour them by which day it is. Try to make the following plot.\n\nObviously we must use geom_line() instead of geom_point()\nOne problem is to get one line per day. In order to do that you need a variable that uniquely identifies the date of the day (which we have), and a way to map that variable to the group dimension of the plot. See if you find something useful under the headline Aesthetics in the help file for geom_line().\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday, group = date)) +\n  geom_line(alpha = .2) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\n\n\n\n\n\n\n\n\nWe look at some options for making final adjustments to your plot in order to make them publication ready, including the possibility of including several plots to the same canvas by means of the patchwork-package.\nYou can find the blog post about the mmtable2-package here.\n\n# Store the three plots under separate variable names\np1 <-\n  df.traffic %>% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\") +\n  labs(colour = \"Weekday\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np2 <- df.traffic %>% \n  tail(n = 500) %>% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\np3 <- ggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n# Assemble using the patchwork-package\nlibrary(patchwork)\n(p1 + p2)/p3"
  },
  {
    "objectID": "04-functions-and-loops.html",
    "href": "04-functions-and-loops.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will next introduce two vital programming techniques: functions and loops. With these techniques we will be able to leverage that we are working with a programming language, as opposed to manually moving around data in a spreadsheet. This will allow us to both do more calculations as well as writing more complex code.\nIn programming, functions and loops are essential building blocks that allow you to create efficient and reusable code. Functions allow you to encapsulate a piece of code into a named block, which you can then call from other parts of your program. Loops allow you to repeat a block of code a certain number of times, or until a certain condition is met.\nFunctions and loops are particularly important in data science, where you often need to perform the same operation on a large dataset. By encapsulating these operations into functions and using loops to apply them to the entire dataset, you can save yourself a lot of time and effort.\nIn this chapter, we will introduce the basics of functions and loops in R programming. We will introduce loops and functions in turn before we bring it all together in the end of the chapter.\n\n\n\n\n\n# We can write a first, simple loop. Note that\n# 1:  We iterate through all numbers 1:10, starting at the beginning. \n# 2: \"i\" is available as a variable when \"print(i)\" is executed\nfor(i in 1:10) {\n    print(i)\n}\n\n\n# We can loop through other collections as well: \nfor(animal in c(\"cats\", \"dogs\", \"hamsters\")){\n  print(paste0(\"I like \", animal, \"!\"))\n}\n\n\n# Let's say we have a vector x, and want to calculate the cumulative\n# sum of it. \nx <- seq(from = 1, to = 100, by = 2)\ny <- rep(NA, length(x))\n\n\n# We *could* write out the necessary calculations as below:\ny[1] <- sum(x[1:1])\ny[2] <- sum(x[1:2])\ny[3] <- sum(x[1:3])\n\n# No no no we write a loop instead\nfor(i in 1:length(x)) {\n    y[i] <- sum(x[1:i])\n}\n\n# For this particular example, we have a base R-function that \n# also does the job: \ncumsum(x)\n\nExercise:\nThe Fibonacci sequence 1,1,2,3,5,8,13,… is defined by F_n = F_{n-1}+F_{n-2} for n>1. This sequence possesses many mysterious qualities. Look at this remarkable picture for instance: https://en.wikipedia.org/wiki/Golden_ratio#/media/File:Golden_mean.png. It displays the ratio of subsequent Fibonacci numbers, i.e. F_{n}/F_{n-1}, that quickly converges to the golden ratio \\frac{1+\\sqrt{5}}{2}= 1.618.... Can you reproduce this figure in R?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nn <- 10\n\n# Create a data frame for storing results:\ndf <- \n  tibble(\n    number = 1:n,\n    F = NA_integer_)\n\n# The first two values are just 1: \ndf$F[1:2] <- 1\n\n# Calculate the rest of the sequence\nfor(i in 3:n) {\n  df$F[i] <- df$F[i-2] + df$F[i-1]\n}\n\n# We can calculate the subsequent ratios using a one-liner like this:\ndf$F[2:n]/df$F[1:(n-1)]\n\n# Alternatively, we can use the lag-function in dplyr. Note that\n# the first ratio is missing. \ndf <- \n  df %>% \n  mutate(ratio = F / lag(F, order_by = number))\n  \n# Define the golden ratio\nphi = (1 + sqrt(5))/2\n\n# We can plot this with ggplot. \ndf %>%\n  ggplot(aes(x = number, y = ratio)) +\n  geom_line(colour = \"red\") +\n  geom_point(colour = \"red\") +\n  geom_hline(yintercept = phi, colour = \"blue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet us look at the following problem. We have downloaded some stock price data files directly from Yahoo Finance (data-stockprices.zip). We are really only interested in the closing price for each stock, and we want to extract those columns and put them into a data frame. From what we know already, we can start imagining how we can do this - and remember that it is just an administrative job. We are not talking about doing any analyses, at least not yet. This is just a dirty job, but one that has to be done! It is obviously repetitive, so we’ll solve it using loops.\nWe have a folder with files. We want the “close” column in each of them in a data set because, eventually, I want to plot these time series in figures. The first obvious problem here is of course that the data is distributed across several files. Before we start loading them, however, we should recall our discussions on data structure in the previous chapter (Wide or long?).\nThe “Excel/Human”-way of storing data would probably be something like this: The first column contains the dates, and then we would have one column for each of the stock. This would make the data fit the shape of the screen alright, but the data would be wide and not suitable for further data analysis and plotting operations. Why? Because there would be several observations for each row; and furthermore: Are we certain that the dates for the different stocks match up exactly?\nSurely, it must be better to store this information in the long format; with one observation per row, and one column per variable. In this case, that would be three variables: The date, the stock, and the closing price for that stock at that date.\n\n# Let us play a bit with first one and see what we get:\n\n# Take the first one and select the date and the correct column\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(readr)\n\napple <- \n  read_csv(\"AAPL.csv\") %>% \n  select(Date, Close) %>% \n  mutate(Date = ymd(Date)) %>% \n  mutate(stock = \"AAPL\") %>% \n  rename(date = Date,\n         close = Close) \n\n# ... boring, repetitive, and also, what if we get some other files tomorrow?\nfiles <- dir(pattern = \"*.csv\")\n\n# How many files do we have?\nlength(files)\n\n# How many rows?\nnrow(apple)\n\n# Let us initialize an empty data frame where we fill inn the correct columns inside\n# a for loop\nstockprices <- \n  tibble(\n    date = ymd(),\n    close = numeric(),\n    stock = character())\n\n# Filling the rows\nfor(stockfile in files){\n  stockprices <- \n    read_csv(stockfile) %>% \n    select(Date, Close) %>% \n    mutate(Date = ymd(Date)) %>% \n    rename(date = Date,\n           close = Close) %>% \n    mutate(stock = tools::file_path_sans_ext(stockfile)) %>% \n    bind_rows(stockprices)\n}\n\n\n\n\n\n\nIn non-compiled languages such as R, loops tend to be slow, so experienced programmers often tries to avoid using them. Sometimes we can avoid using loops by finding a function that performs a particular task directly on a vector (or list) of elements, meaning that we do not have to explicitly write out the looping ourselves. This is called vectorizing our code.\n\n# For example, we calculated the cumulative sum of a vector of numbers by\n# creating the following loop:\n\npartial_sum <- function(x) {\n  ps <- rep(NA, length(x))\n  for(i in 1:length(x)) {\n    ps[i] <- sum(x[1:i])\n  }\n  return(ps)\n}\n\n# Incidentally, there is a function that does exactly the same operation in R\n# directly on the vector x:\ncumsum(x)\n\nlibrary(microbenchmark)\ntest <- microbenchmark(partial_sum(x), cumsum(x))\n\n# Why such a big difference? The built-in function is written in a much faster\n# language. Always vectorize your code if possible. This can make a huge\n# difference in bigger projects when we are dealing with hours and days instead\n# of nanoseconds.\n# \n\n# There are many ways to both vectorize and speed up our code \n# in R (particularly the \"apply\"-family of functions!). We will return\n# to this topic in BAN400. \n\n\n\n\n\n\nWriting your own functions is a great tool that can allow you to both do more complex calculations and simplify your code. Another benefit is that it allows you to free up memory in your own brain when developing code. Once you have figured out how to solve a specific problem, you can store your solution in a function. This way, you can re use your solution many times, without having to remember exactly how it was solved.\nThe book Clean Code presents principles and guidelines when writing functions. A few principles we should keep in mind when writing functions are:\n\nFunctions should be short\nA function should do one thing\nUse understandable names of functions and arguments\n\nNote that we use the terms “principles” here, and not rules. However, to motivate why these are good principles to strive for when writing code, consider what a function would look like if it does not adhere to the principles….it will be a complicated mess that is hard to understand, debug and use.\n\n# We use functions all the time\nplot(1:10, (1:10)^2, type = \"l\")\n\n# or we get out a number:\nmean(stockprices$close)\n\n# or perhaps a numerical summary of a data set\nsummary(stockprices)\n\n# Perhaps we want to make one of those ourselves. \nsubtract <- function(number1, number2) {\n    return(number1 - number2)\n}\n\nsubtract(10, 5)\nsubtract(5, 10)\n\nsubtract_sqrt <- function(number1, number2) {\n    sqrt(number1 - number2)\n}\n\nsubtract_sqrt(10, 5)\nsubtract_sqrt(5, 10)\n\nsubtract_sqrt <- function(number1, number2) {\n    if(number2 > number1) {\n        stop(\"Can't take the square root of a negative number, make sure that a >= b!\")\n    } else {\n        sqrt(number1 - number2)\n    }\n}\n\nsubtract_sqrt(5, 10)\nsubtract_sqrt(10, 5)\n\nExercise:\nMake a function that plots the stock value time series for a given stock.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nplotprice <- function(ticker, pricedata) {\n  \n  pricedata %>% \n    filter(stock == ticker) %>% \n    ggplot(aes(x = date, y = close)) +\n    geom_line()\n  \n}\n\nplotprice(\"AAPL\", stockprices)\n\n\n\n\n\n\n\n\n\nOkay, so let us try to make use of this if we imagine the following problem. We have collected the stock price data as before, and we have been tasked with presenting this data in a meeting. You really want to be able to create pretty plots on the fly with different stocks, perhaps in different formats, and you want the option to save the plot as well as a pdf file. Also, you want to be able to normalize the plots with a swith in the function so that they show percentage deviations from the level on the first day in the observation period rather than the raw price.\n\n# We build a function step by step. An we start with a simple version much like\n# the one we built in the last lesson:\n\nplotStocks <- function(data) {\n  data %>% \n    ggplot() +\n    geom_line(mapping = aes(x = date, y = close, colour = stock)) +\n    xlab(\"\") +\n    ylab(\"\") +\n    labs(linetype = \"Ticker\") +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    geom_text(mapping = aes(x = x,\n                            y = y,\n                            label = stock),\n              data =   \n                . %>% \n                group_by(stock) %>% \n                summarize(x = tail(date, n = 1),\n                          y = tail(close, n = 1)),\n              hjust = -.3) + \n    scale_x_date(expand = c(.14, 0))\n}\n\nnormaliseAndPlot <- \n  function(data,norm=FALSE){\n    if(norm){\n      data %>% \n        group_by(stock) %>% \n        arrange(stock,date) %>% \n        mutate(\n          firstclose = head(close,n=1),\n          close = close/firstclose\n        ) %>% \n        plotStocks() +\n        ggtitle(\"Normalized prices\")\n    }else{\n      data %>% \n        plotStocks() + \n        ggtitle(\"Prices\")\n    }\n  }\n\nstockprices %>% \n  filter(stock %in% c(\"AAPL\", \"MSFT\", \"SIRI\")) %>% \n  normaliseAndPlot(norm = TRUE) %>% \n  ggsave(filename = 'test.pdf')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Håkon Otneim: hakon.otneim@nhh.no\nOle-Petter Moe Hansen: ole-petter.hansen@nhh.no"
  },
  {
    "objectID": "assignment-01.html",
    "href": "assignment-01.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will look at the monthly returns on the NASDAQ composite stock index from August 2013 to June 2018, as well as the returns on 16 individual stocks listed on NASDAQ. The data is contained in the file data-nasdaq-returns.xls, and has been collected from the Yahoo Finance website.\nPut the data in an appropriate folder on your computer. Perform the following tasks:\n\nRead the data into R and take a first look at the data set. The main index is in the NASDAQ-column.\n\n\n\n\n\n\n\nClick here to see how the top of the data set should look like after you have loaded it into R.\n\n\n\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n# A tibble: 59 × 11\n   Date     NASDAQ    ADBE     AMZN     AAPL     BBBY     CSCO    CMCSA     COST\n   <chr>     <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 2013-0… -0.0101 -0.0329 -0.0696   0.0739  -3.64e-2 -0.0933  -0.0686  -0.0484 \n 2 2013-0…  0.0494  0.127   0.107   -0.0217   4.79e-2  0.00513  0.0695   0.0291 \n 3 2013-1…  0.0386  0.0430  0.152    0.0920  -5.17e-4 -0.0378   0.0535   0.0243 \n 4 2013-1…  0.0351  0.0461  0.0781   0.0619   9.14e-3 -0.0598   0.0466   0.0611 \n 5 2013-1…  0.0283  0.0532  0.0130   0.00886  2.87e-2  0.0540   0.0412  -0.0525 \n 6 2014-0… -0.0176 -0.0116 -0.106   -0.114   -2.29e-1 -0.0235   0.0466  -0.0576 \n 7 2014-0…  0.0486  0.148   0.00946  0.0500   6.03e-2 -0.00503 -0.0520   0.0388 \n 8 2014-0… -0.0257 -0.0430 -0.0737   0.0198   1.43e-2  0.0280  -0.0324  -0.0448 \n 9 2014-0… -0.0203 -0.0636 -0.101    0.0948  -1.02e-1  0.0303   0.0338   0.0352 \n10 2014-0…  0.0306  0.0452  0.0273   0.0702  -2.08e-2  0.0633   0.00846  0.00293\n# … with 49 more rows, and 2 more variables: DLTR <dbl>, EXPE <dbl>\n\n\n\n\n\n\nMake a new data frame containing only the date column and returns on the main index as well as one of the individual stocks of your choosing. Name the new data frame appropriately.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\nFor example, after picking ADBE as the stock:\n\n\n# A tibble: 59 × 3\n   Date        NASDAQ    ADBE\n   <chr>        <dbl>   <dbl>\n 1 2013-08-01 -0.0101 -0.0329\n 2 2013-09-01  0.0494  0.127 \n 3 2013-10-01  0.0386  0.0430\n 4 2013-11-01  0.0351  0.0461\n 5 2013-12-01  0.0283  0.0532\n 6 2014-01-01 -0.0176 -0.0116\n 7 2014-02-01  0.0486  0.148 \n 8 2014-03-01 -0.0257 -0.0430\n 9 2014-04-01 -0.0203 -0.0636\n10 2014-05-01  0.0306  0.0452\n# … with 49 more rows\n\n\n\n\n\n\nMake a scatterplot of the two variables in your newly created data frame.\n\n\n\n\n\n\n\nClick here to see the plot should look like.\n\n\n\n\n\nStill, using ADBE, will of course look a bit different if you have chosen a different stock:\n\n\n\n\n\n\n\n\n\nThe function sign(x) returns the sign of x, that is, it returns -1 if x is negative and 1 if x is positive. Make two new columns, named sign_NASDAQ and a corresponding name for the stock that you have chosen to include, that contains the sign of the return, indicating whether the index or stock went up or down that day.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 5\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign\n   <chr>        <dbl>   <dbl>       <dbl>     <dbl>\n 1 2013-08-01 -0.0101 -0.0329          -1        -1\n 2 2013-09-01  0.0494  0.127            1         1\n 3 2013-10-01  0.0386  0.0430           1         1\n 4 2013-11-01  0.0351  0.0461           1         1\n 5 2013-12-01  0.0283  0.0532           1         1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1\n 7 2014-02-01  0.0486  0.148            1         1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1\n10 2014-05-01  0.0306  0.0452           1         1\n# … with 49 more rows\n\n\n\n\n\n\nMake another column consisting of the sum of the two sign columns divided by two. The resulting value will then be -1 if both the index and the stock went down that day, 0 if they went in separate directions, and 1 if both went up.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 6\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign   sum\n   <chr>        <dbl>   <dbl>       <dbl>     <dbl> <dbl>\n 1 2013-08-01 -0.0101 -0.0329          -1        -1    -1\n 2 2013-09-01  0.0494  0.127            1         1     1\n 3 2013-10-01  0.0386  0.0430           1         1     1\n 4 2013-11-01  0.0351  0.0461           1         1     1\n 5 2013-12-01  0.0283  0.0532           1         1     1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1    -1\n 7 2014-02-01  0.0486  0.148            1         1     1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1    -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1    -1\n10 2014-05-01  0.0306  0.0452           1         1     1\n# … with 49 more rows\n\n\n\n\n\n\nWe would like to count the number of days for which the new sum-column is either -1, 0, or 1. Do that by applying the function table() to the sum-column. (Recall that we can pick out individual columns using the dollar-sign).\n\n\n\n\n\n\n\nClick here to see how the output should look like.\n\n\n\n\n\nFor the ADBE-stock:\n\n\n\n-1  0  1 \n15 10 34 \n\n\n\n\n\n\nIn the tasks above you may (or may not) have created several intermediate data frames under different names for each problem, or perhaps you have overwritten the data frame for each new task. Let us rather complete task 1, 2, 4 and 5 in one single operation, where you just append each task to the previous using the pipe-operator. That way you only need to come up with one name for the data set.\n\n\n\n\n\n\n\nClick here to see to see a hint if you need to.\n\n\n\n\n\nThe code may look something like this, replace the dots:\n\nstock_data <-\n  read_excel(...) %>%       \n  select(...) %>%                  \n  mutate(...) %>%          \n  mutate(...) %>%              \n  mutate(...)       \n\n\n\n\n\n\n\nThe .csv-file (comma separated values) is a common format for storing data in a plain text file. The file data-missile.csv contains data on North Korean missile launches from 1984 until 2017. Put the file in folder on your computer and inspect the contents by opening it in a text editor such as Notepad or Textedit.\nR ships with a function read.csv() that we can use to read csv-files in the same way as we use read_excel() to read excel-files. We will, however, use a function from the readr-package called read_csv() for this purpose that does almost the same thing as the default read.csv()-function. There are some subtle differences between these two functions that are not very important, but read_csv() works a little bit better together with many other functions and packages that we will use later.\nLoad readr using the library() function. If you get an error message telling you that there is no package called 'readr', then you need to install it first using the install.packages()-function.\nLoad the data into into R using the following command:\n\nmissile <- read_csv(\"data-missile.csv\")\n\nLook at the data. The variable «apogee» is the highest altitude reached by the missile in km. Calculate the following statistics for this variable:\n\nThe mean.\nThe median.\nThe standard deviation.\n\n\n\n\n\n\n\nMaybe you get some unexpected results here. You need to troubleshoot the problem in order to solve the issue. Click here if you need some hints on what to try.\n\n\n\n\n\nThe problem is that you get NA-values right? Why is this? Look at the data, and you will see that many values are missing, and they should be ignored when calculating the mean, median and standard deviation. Look at the help files for the functions in question (e.g. ?mean) to see if there is something you can to to fix the issue."
  },
  {
    "objectID": "assignment-02.html",
    "href": "assignment-02.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "Maybe first add some of the straightforward exercises from r4ds?\nFor this homework you will practice your data wrangling skills using a survey data set. The data comes from the European Social Survey, and is available at [europeansocialsurvey.org] in various formats. Further, the data from ESS contains many variables with abbreviated/encoded names. The file contains survey responses from Norway in 2016. To get you started with the assignment, you may use the commands below to read in the data set. You may need to install the package foreign first in order to use the read.dta function.\n\nlibrary(foreign)\n\ndf <- \n  read.dta(\"data-ESS8NO.dta\") %>%\n  transmute(\n    party = prtvtbno, \n    age = agea, \n    religiosity = rlgdgr, \n    income_decile = hinctnta)\n\nThe variables of interest prtvtbno, agea, rlgdgr and hinctnta are defined in the attached documentation, together with more information on the actual questions. To keep it simple, we rename them to party, age, religiosity and income_decile.\n\nProvide summary statistics of age of respondents split by the party the respondents voted for last election. Who has the oldest/youngest voters? Where is the standard deviation of voters age the largest? Do not report numbers for parties with less than 25 respondents.\nThe variables religiosity, income_decile and party are encoded as factors. Further, they contain some non-responses such as “Don’t know”, “Refusal” and “No answer”. Find a method for filtering out all the non-responses. How many respondents did not provide an eligible response to each of the questions? How many answered both the party and income question?\nFilter out all ineligible responses for both income and party. Calculate the average religiosity of each party. Provide your answer as a data frame with one row pr. party, sorted by average religiosity.\n(Slightly trickier!) For each party with more than 75 voters, calculate the ratio of the number of respondents in income deciles 9 and 10 over the number of respondents in income deciles 1 and 2. Which party has the highest high- to low-income voters?\n\nFor completeness: When working with survey data, we usually have to apply weights to ensure estimates are representative of the population. This is because a survey sample may be a non-random sample of the general population. The survey data set provides the weights. You don’t have to worry about weights in this assignment, but please store the link “survey data -> weights?!?” in your mind for future work."
  },
  {
    "objectID": "assignment-03.html",
    "href": "assignment-03.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "In this assignment we will visualize some of the data that was published here as part of the Tidy Tuesday project. There is one observation for each country of the world, and several interesting variables. One of them is the amount of mismanaged plastic waste in each country in a given year, and we will here build a plot using this data by adding more and more layers of complexity.\nDownload the data here, and load it into memory:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nplastic <- read_csv(\"data-plastic.csv\")\n\nThe warning message just says that the first column name is empty in the csv-file (confirm that by looking at the file if you wish), and that it has been given the generic name X1.\nWe would like to visualize the association between the GDP and the amount of mismanaged plastic waste in the countries of the world. Below you will find four figures of increasing complexity. Replicate them as best as you can, and feel free to spend a few minutes looking at and interpreting the graphs."
  },
  {
    "objectID": "assignment-04.html",
    "href": "assignment-04.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "Suggest adding some additional super basic drills as the first problem, where they basically do the same thing as below, but in a more trivial setting and much more guiding.\nIn the real world you will sometimes encounter data with strange distributions. One such example is insurance claims. For many insurance products, e.g. fire insurance, most customers will never have a claim. But occasionally a house may burn down, leading to a large claim because it must be rebuilt from the ground up. For the insurance industry it is vital that the expected claim cost is estimated correctly.\nTo see what claims data can look like, we can use the “tweedie”-package. We can interpret an observation as the total claim cost for one insurance held for a full year. The parameters phi and power control the shape of the distribution, and mu is the expected value.\nThe function rtweedie(n, mu, phi, power) generates n observations from the tweedie distribution with the specified parameters. You can try to generate 10 insurance claims from the distribution with mean mu = 10 and shape parameters phi = 10 000 and power = 1.9 using the following line:\n\nlibrary(tweedie)\n\nWarning: package 'tweedie' was built under R version 4.2.3\n\nrtweedie(n = 10, mu = 10000, phi = 1000, power = 1.9)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nMost likely you will observe 10 zeros, each of which belong to the majority of houses that do not burn down in a given year. If you do observe a non-zero, however, this would represent the total amount claimed by a single (unlucky) insurance holder that year.\nSo what is the expected value of the claim for each customer? The long-run average across all customers of the insurance company? The total amount of money that the company on average expects to pay out to their policy holders divided by the number of policy holders?\nWe know the anwer to that question because we generate the observations from a known probability distribution: The answer is mu = 10 000. Knowing this number is obviously very important to the company, because it dictates that the price of each of the insurance policies should be something like\n10 000 + \\text{Costs} + \\textrm{Safety margin} + \\textrm{Profits},\nthat is, the amount that the insurance company expects to pay back to the customer in damages, plus the costs of running the company, plus some safety margin to make sure that the natural fluctuations from year to year do not cause the company to lose money in bad years, plus some profits that make the whole endeaveour worthwile as a business.\nThe expected payout, however, is not known in practice, and must be estimated from historical data. We usually do this by taking the average, but we see immediately that it is not obvious that this will work in our situation above where we had just ten observations. If you sampled ten zeros, then the average is zero, and we have barely any information at all regarding the amount that the insurance companly must charge for these policies. In the event that you sampled one or more non-zeros, it is quite unlikely that the average over ten observations is anywhere near the true expected value of 10 000.\nWe can ramp this experiment up a notch, by rather sampling 100 000 observations from this distributions (or claimed amount from 100 000 customers over a given year if you wish) and then taking the average:\n\nx <- rtweedie(n = 100000, mu = 10000, phi = 1000, power = 1.9)\nmean(x)\n\nThis number is likely not that far from the true expectation. The story line here is as follows: How large must the sample be for the insurance company to make useful statistical inferences about their population of customers?\nOne classical method for making statements regarding the unknown population mean is the t-test. You may recall from your introduction to statistics that the t-statistic provides a measure of the distance from the observed sample mean to some hypothesized population mean that is approximately normally distributed. This is due to the Cental Limit Theorem, which postulates that the distribution of the sample mean (and essentially any other statistic that is based on a sum of random variables such as the t-statistic) converges towards the normal distribution… which is almost always true. One exception is if the underlying distribution of the observations is exceptionally weird. We have already seen that the claim-type data that we generate from the tweedie distribution are quite special, and that we need a fair amount of data in order to see the usual convergence of the sample mean towards the population mean.\nSo: How large must the sample size be in the insurance context? Let us explore.\nIn the code snippet below, we first define the variables N and true_mu and set their value to 100 and 10 000, respectively. We then sample n observations from the tweedie distribution having expected value equal to true_mu (and phi = 1000 and power = 1.9 as above).\nFinally, we use the sampled data to test the null hypothesis whether \\mu = 10 000, a hypothesis that we know is true. However, if the p-value of the test is smaller than 5%, then we reject the null hypothesis at the 5% level.\n\nN <- 100\ntrue_mu <- 10000\nsample <- rtweedie(N, mu = true_mu, phi = 1000, power = 1.9)\nt.test(sample, mu = true_mu)\n\nYou can check this several times and see what you get. If the t-test works as it should (i.e., if the sample is large enough for the Central Limit Theorem to provide a reasonable approximation for the distribution of the test statistic), then we should reject the null hypothesis 5% of the times.\nBut, why repeat such a trivial task manually? Let us put it in a loop, an perform a proper simulation experiment. The idea is to simulate M datasets, each with N observations, and run the t-test on each one of the M datasets. Again, with a 5% significance level we should expect to reject the null in about 5% of the times that we do this, if we have enough data.\n\nt.test(rtweedie(100, mu=10000, phi=100, power=1.9),mu=10000)$p.value\n\n\nAssignment 1: Create a function simTweedieTest that takes N as argument. The function should simulate a data set with a tweedie-distribution with parameters mu = 10000, phi = 100, power = 1.9, and run a t-test on the simulated data using the null hypothesis \\mu_0 = 10000. The function should return the p-value of the test.\nAssignment 2: Create a function MTweedieTests, that takes M, N and alpha as arguments. This function should call the simTweedieTests function M times with a sample size of N. The function MTweedieTests should then return percentage of tests where the p-value is lower than alpha (e.g. if we run tests on M = 10 datasets, and have p-values lower than \\alpha = .05 in two of the tests, the function should return 0.2). Hint: You may want to use the function replicate(M, fun(...)). `replicate() works well for random number generating functions.\nAssignment 3: Create a data frame tibble(N = c(10, 100, 1000, 5000), M = 100, share_reject = NA). Use a loop and the MTweedieTests-function to fill in the values in the share_reject-column. Create a figure with N on the X-axis and share_reject on the Y-axis. What does this tell you of the validity of the t-test in on this specific distribution? What does “large enough sample” mean for this?\nAssignment 4 (trickier): How general are the findings in assignment 3? And can we be sure we wrote the code correctly? If the data follows a normal distribution instead of a tweedie distribution we should expect that the t-test works better at lower sample sizes. Create a figure similar to assignment 3, but with two curves: one for tweedie-distributed data and one for normally distributed data. You will have to rewrite the functions from assignment 1-3. Think carefully on how you structure the functions: avoid duplicating code, use sensible names for arguments and functions, and ensure that the mean of the data and the test is consistent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "This is the companion website for the course BAN400 - R Programming for Data Science, given at The Norwegian School of Economics (NHH). The purpose of this website is to provide study material such as lecture videos, exercises and assignments for students taking the course.\nBAN400 has previously consisted of two separate modules; one intensive one-week introduction to R that could be taken separately as a 2.5 ECTS course as BAN420, as well as the main course itself (pun intended), which, together with BAN420, completed the 7.5 ECTS unit BAN400.\nFrom the fall semester of 2023, we do no longer offer the intensive option. BAN400 is now offered as one regular course. We will, however, still make an explicit transition from Part 1, where we introduce basic programming, to Part 2 where we will learn a number of useful techniques that are particularly useful when working with data.\nAll announcements and course administration such as homework delivery and feedback will be carried out through the course page at Canvas, which is the learning management system used by NHH. You will need to sign up to the course in order to gain access to the Canvas page."
  }
]