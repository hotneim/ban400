[
  {
    "objectID": "01-intro-to-r.html",
    "href": "01-intro-to-r.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "Welcome to the first taste of BAN400. We will start by downloading and installing the tools that we need to start coding, and then we will explore some of the most basic aspects of the R programming language. After most of the videos we have included a small problem that we encourage you to try before you move on to the next topic.\nBefore we start, you need to install two items on your computer. Please do the installations in the following order:\n\nThe R Programming Language: Navigate to cran.uib.no and download the version of R that corresponds to your operating system. Run the installation as you would for any other program that you install on your system.\nRStudio: Navigate to posit.co/download/rstudio-desktop/, and download the version of “RStudio Desktop” that corresponds to your operating system. Run the installation as you would for any other program that you install on your computer.\n\nBoth R and RStudio are free to download and free to use.\n\n\n\n\nIn this video we open up RStudio for the first time and take a small tour of the user interface.\n\n\n\n\n\nWe move on to write our first R commands. It is critical that you already now start to feel the programming, and you do that best by typing in the code lines just as in the video above (no copy/paste!), making sure that you get the same results.\n\n# We can use R as a calculator:\n2+2\n\n# Pretty simple! We must use paratheses if we have more complicated expressions:\n(2+8)/2\n2+8/2\n\n# Variables are important in R. We can save just about anything inside the\n# computer memory by giving them names:\na <- 5\na\na*4 \n\nb <- 3\n\n# R performs all operations on the right hand side before assigning the value to c:\nc <- a+b\nc\n\n# No errors, warnings or questions when overwriting!\nc <- 4\n\nc <- c + 2\nc\n\n# Let's make an error!\nd\n\n# We can name things more or less what we want. Not a non-trivial problem in\n# large projects!\nwhatever_we_want <- \"hello world\"\nwhatever_we_want\n\nExercise:\nPick your favorite three integers and store them in three different variables. Calculate yor magic number, which is the sum of these three integers. Store your magic number in a new variable. Give your new variable a name that clearly identifies what it is.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nnumber1 <- 1\nnumber2 <- 87\nnumber3 <- 101\n\nmagic_number <- number1 + number2 + number3\n\n\n\n\n\n\n\n\n\nVectors are very important in R. We remember perhaps from our math classes that vectors may represent points in space; in R it is a way to store more than one number (or string, or some other data type) under a single variable name. When doing statistics, this may for example be a set of observations.\nIn this video we first create a vector of numbers using the c()-function, and then we look at various ways to extract/pick out the elements: to subset. Python coders will notice two important distinctions from what they are used to:\n\nIn R we start counting on 1, and not 0!\nWhen trying to subset using an index that out of the range of the vector, we do not get an error message, we just get back the empty value NA.\n\nFurthermore, we use the Up-arrow to get back the last command that we have executed in the console. You can even tap the up-arrow again in order go further back in your command history (and of course use the down-arrow to navigate the other way).\n\n# We can make a vector in the following way:\nvector1 <- c(3, 5, 7.8, 10, 2, 0.16, -3)\n\n# Print out\nvector1\n\n# Subsetting (The first item has index 1!)\nvector1[1]        # Square brackets to subset\nvector1[10]       # Out-of-range error\nvector1[2:5]      # Subset a sequence\nvector1[c(1,3)]   # Subset using another vector!\n\n# The letter \"c\" stands for \"combine\". R makes it very easy to work with\n# vectors:\nvector1 - 1\nvector1*3\n\n# We can use *functions* to calculate various things:\nlength(vector1)\nmean(vector1)\nsum(vector1)\nsd(vector1)\n\n# We can make a vector of strings as well:\nvector2 <- c(\"hello\", \"world\")\n\n# A vector can only contain one data type!\n\n# Perhaps we need the standard deviation later?\nsd_vector1 <- sd(vector1)\nsd_vector1\n\nExercise:\nCalculate the maximum and minimum values of vector1, as well as the median. (Hint, and this will be the most important lesson you will learn in this course: If you do not know the name of the function, Google it!)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# Relevant Google searches: \"minimum value r\", \"maximum r\", \"median r\"\n\nmin(vector1)\nmax(vector1)\nmedian(vector1)\n\n\n\n\n\n\n\n\n\nIn this video we install our first package in R. There are two major takeaways from this:\n\nWe install the package on our computer using the install.packages()-function. We only have to do this once per computer.\nIf we are going to use some of the functions in a package we need to load it using the library()-command. You have to do that every time you restart R (and we will later see that we will typically load all the packages we need in the beginning of the scripts that we write).\n\n\n# In order to install the package readxl, we run the following command. \n# We run this command only once.\ninstall.packages(\"readxl\")\n\n# When we are going to use it, we load it using the \"library()\"-function, \n# and we need to repreat this every time we restart R.\nlibrary(readxl)\n\nExercise: Install the following packages. We will make use of them (and several others) later in the course: ggplot2, dplyr, tidyr and lubridate.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\n\n\nWe introduce the concept of a working directory, which is the folder where R looks for files that we are going to read into the memory, and where R puts the files that we create, for instance image files of plots.\nThere are two central functions:\n\ngetwd() prints out the current working directory.\nsetwd(\"C:/path/to/folder\") sets the working directory to the specified folder. We will as a general rule not use setwd() in our scripts (the reason for that will become clear later), but rather use RStudio’s menu system for changing the working directory (we will in practice not need to do that as a general rule as well, which will also become clear in a short while).\n\nWe may however have to deal with file paths in our code, and make the following technical notes:\n\nOn UNIX systems (such as Mac or Linux) the file paths look differently, they do not start with a drive letter such as C:\\.\nOn Windows, we always use the backslash / to separate between the folders in R code, and not the usual forward slash \\. This may be counter-intuitive to some, but in programming the forward slash usually has special meaning (the escape character) and must not be used for anything else. On UNIX systems we also use the backslash, but that is the system standard for writing file paths, so it does not require any special attention to users of those operating systems.\n\nExercise: Make sure that you have completed the following tasks before proceding to the next lesson:\n\nYou have created a dedicated folder on your computer where you will collected all material that we will use today.\nYou have downloaded the file testdata.xls and put in in your newly created folder.\nYou have changed your working directory to this folder.\nYou have positively confirmed that your working directory now is correctly set.\n\n\n\n\n\n\nWe read our first small data file into the memory of R and apply some simple operations to it. We will spend much more time working with data in R in later lessons.\n\n# The data is in the .xls-format, so we need the readxl-package in order to load \n# it into R.\nlibrary(readxl)\n\n# Inside this package, there is a function called read_excel:\nread_excel(\"testdata.xls\")\n\n# That's fine, but in order to use this data, we need to save it in a variable\ntestdata <- read_excel(\"testdata.xls\")\n\n# Print out the (top of the) data set.\ntestdata\n\n# Now we see the data in the environment. We can look at it by typing the name that we\n# gave it. We can also pick out individual columns using the $-sign:\ntestdata$X1\n\n# Calaculate the mean for X1 and X2:\nmean(testdata$X1)\nmean(testdata$X2)\n\n# How many rows/observations do we have?\nnrow(testdata)\n\nExercise:\n\nHow many columns does our data set have?\nCan you find a way to print out a vector that contains the sum of the X1 and X2 columns in testdata?\nWhat is the total sum of all the numbers in the X1 and X2 columns of testdata?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nncol(testdata)\n\n# 2 \ntestdata$X1 + testdata$X2\n\n# 3\nsum(testdata$X1 + testdata$X2)\n\n\n\n\n\n\n\n\n\nWe introduce the main package for the plotting engine that we will use in this course; ggplot2, and its basic syntax.\n\n# A simple scatterplot\nplot(testdata$X1, testdata$X2)\n\n# Making adjustments to the plot\nplot(testdata$X1, testdata$X2,\n     pch = 20,\n     bty = \"l\",\n     xlab = \"X1\",\n     ylab = \"X2\")\n\n# Load the ggplot2-package\nlibrary(ggplot2)\n\n# Here is the code for creating a simple scatterplot of the X1 and X2 columns in\n# our data set:\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\n\n# First make the plot, then save it to a file\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\")\n\n# A more flexible way to do it is to save the plot in a variable, and then\n# supply the name of the plot to the ggsave-finction:\np <- ggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\", p)\n\n# That way, we can save the plot p at any time, we do not have to do it directly\n# after the plotting commands.\n\nExercise: Can you figure out how to make the dots in the plot bigger and blue?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nggplot(testdata, aes(x = X1, y = X2)) + geom_point(colour = \"blue\", size = 5)\n\n\n\n\n\n\n\n\n\nIn this video we stop writing code directly in the console, and rather write our code in a script file, which is simply a pure text file containing commands. There are two important new concepts that we have to pay attention to when writing scripts:\n\nThe comment character #: Evertything after this character in an R-script is ignored when executing the script. We can use the comment character to add small comments to our code, briefly explaining what is going on. This is a great help for other people trying to understand what you have done, in particular, and perhaps most importantly: the future you returning to a project. The comment character # is the same as in Python.\nThe keybinding Ctrl - Enter (Cmd - Enter on a Mac) executes the line where your cursor is located in the script. You can also select several lines and execute all of them using this shortcut.\n\n\n# Introduction to R\n# -------------------\n\n# Load packages \nlibrary(readxl)\nlibrary(ggplot2)\n\n# Read our data set\ntestdata <- read_xls(\"testdata.xls\")\n\n# Make a scatterplot of the X1 and X2-variables\nplot <- ggplot(testdata, aes(x = X1, y = X2)) + \n    geom_point() + \n    ggtitle(\"Scatterplot of testdata\") +\n    theme_classic()\nggsave(\"testplot.pdf\", plot)\n\nExercise: Make sure to save your script as an .R-file in the folder that we created for this session. Close RStudio. Then navigate to the folder and double click on the script file. Hopefully RStudio opens (if not, right click, select “Open in” and then Rstudio, confirm if prompted to set RStudio as default program for opening .R-files).\nFind out what your working directory is now. What just happened? How is this useful?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nOpening RStudio by double clicking on the script file automatically sets the working directory to the location of the script file. Very useful when returning to a project.\n\n\n\n\n\n\nSince version 4.1.0, R has included the pipe operator |>. A pipe operator allows you to write code that is closer to how we read English. To see how it works, consider the code below, that applies three functions to a vector:\n\n# Create some data:\nx <- (-500):500\n\n# Want to calculate mean of sqrt of abs values of x...\n# one way: via temporary variables: \nabs.x <- abs(x)  \nsqrt.abs.x <- sqrt(abs.x)\nalt.1 <- mean(sqrt.abs.x)\n\nprint(alt.1)\n\n[1] 14.91415\n\n\nAssuming we only care about the final result alt.1, the script above creates two unnecessary variables in memory (abs.x and sqrt.abs.x), and clutters up the environment.\nWe could instead nest the three function calls:\n\nalt.2 <- mean(sqrt(abs(x)))\nprint(alt.2)\n\n[1] 14.91415\n\n\nNesting the function calls doesn’t store the intermediary calculations, so we don’t clutter up the environment. However, reading what is happening on this line is more challenging than it needs to be: you have to read from right to left. It the functions calls has more than one argument, you would also need to keep track of which of the right-parentheses belongs to which function.\nThis is when the pipe operator comes to the rescue. The pipe “passes” whatever is on the left hand side to the right hand side of the expression. As a simple example, the sqrt(2) can be written in a pipe as:\n\n2 |> sqrt()\n\n[1] 1.414214\n\n\nThe script above can be read as “The number 2, then the square root”. Pipes can also be used with multiple arguments. The two statements below gives the same answer:\n\nmean(c(1,2,NA), na.rm=T)\n\n[1] 1.5\n\nc(1,2,NA) |> mean(na.rm=T)\n\n[1] 1.5\n\n\nBy default, the pipe operator inserts the value of the left hand side as the first argument in the function call on the right hand side, so e.g. x |> mean(na.rm=TRUE) is equivalent to mean(x, na.rm=TRUE). If you want the value to be inserted as another argument, an underscore _ may be used as a placeholder for the left hand side value. However, then the argument names must be named:\n\natan2(x=1, y=2)\n\n[1] 1.107149\n\n2 |> atan2(x=1, y=_)\n\n[1] 1.107149\n\n\nThe Tidyverse-package magrittr also contains a pipe operator %>%. The Tidyverse-pipe behaves similarly to the |> in simple cases, however, it uses a period . as placeholder. The Tidyverse-pipe generally is more mature and has more features than the base-pipe |> (see here for a more thorough comparison of the two).\nIn this course we’ll generally use the Tidyverse-pipe. The shortcut for the pipe in RStudio is Ctrl - Shift - M on Windows, and Cmd - Shift - M on a Mac.\nWe will use this technique extensively throughout the course!\n\nlibrary(magrittr)\n2 %>% . ^ 2\n\n[1] 4\n\n2 %>% atan2(1, .)\n\n[1] 0.4636476\n\n\nExercise:\nUse magrittr to calculate the equivalent of alt.1 and alt.2\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nalt.3 <-\n  x %>% \n  abs %>%\n  sqrt %>%\n  mean\n\nalt.1\nalt.2\nalt.3\n\nAlternatively, with the base-pipe. Note |> is more picky than %>% with parentheses after function calls.\n\nx |>\n  abs() |>\n  sqrt() |>\n  mean() \n\n[1] 14.91415"
  },
  {
    "objectID": "02-data-wrangling.html",
    "href": "02-data-wrangling.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "In this section we will learn how to handle data in a very efficient way. We will learn to\n\nfilter a data set based on variable values,\nselect variables,\ncreate new variables,\ngroup data based on variables,\ndummarise the data, and\njoin different data sets.\n\nWe will obviously use R to solve these problems, but we do have the choice between different coding styles to do it. One way is to only use functions that already ship with R, or we can use functions from additional packages to solve the same problems. We choose the latter, and not only that: We will thoughout this course use a specific set of packages, an ecosystem if you wish (or even a philosophy of data work), called the tidyverse.\nThe tidyverse (tidyverse.org) is a set of R packages that work very well together, follows a consistent logic, and that enables us to write extremely clean code. We have already touched upon the difference between data wrangling using base R and the tidyverse in Chapter 1.9.\nSee the video below for some more details regarding the tidyverse. Some formulations in the video gives the impression that this material should be consumed on a specific day, but that are just some residue from a time when this part of the course was given intensively. You can find the cheat sheet here. See also this webpage for further information about one of the most central packages in tidyverse, dplyr.\n\n\n\n\n\n\nFirst, we warm up with a first few functions from dplyr. Note how the syntax is similar to how we use the pipe `|>. You can download the data set here: data-geilo.xlsx.\n\n# Load libraries\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(tidyr)\nlibrary(readxl)\n\n# Import data from Excel-file\nsales <- read_excel(\"data-geilo.xlsx\", sheet = \"Sales\")\n\n# Look at data: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# All sales where cocoa > 0: \nsales[sales$cocoa > 0,]\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   <dbl>    <dbl>  <dbl> <dbl> <dbl>\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Similarly with dplyr has a function for this: \nfilter(sales, cocoa > 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   <dbl>    <dbl>  <dbl> <dbl> <dbl>\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# or even better: combine dplyr and magrittr: \nsales %>% \n  filter(cocoa > 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   <dbl>    <dbl>  <dbl> <dbl> <dbl>\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Some governing principles in with dplyr/tidyverse functions: \n# \n#  - The first argument is a data frame.\n#  - The subsequent arguments describe what to do with the data frame. You \n#    can refer to columns in the data frame directly without using $.\n#  - The result is a new data frame\n\n# Sorting the data with dplyr: \nsales |> \n  arrange(cocoa) |> \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     5       NA      3     1     1\n6     7       NA      1     1     1\n\n# Sorting the data with several columns: \nsales |> \n  arrange(cocoa, orange, swix) |> \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     7       NA      1     1     1\n6    20       NA      1     1     2\n\n\nExercise: Report the top of the dataset, sorted by\n\ncocoa (increasing)\nswix (decreasing)\norange (decreasing)\n\nYou should obtain a result equal to the data frame below. Note we only show the first six entries of the result.\n\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1    21       42      5     0     7\n2    19        7      4     0     2\n3    12       10      3     0     0\n4    20       NA      1     1     2\n5     5       NA      3     1     1\n6     9       NA      2     1     1\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nsales |> \n  arrange(cocoa, desc(swix), desc(orange)) |> \n  head()\n\n# Alternatively: \n sales |>\n   arrange(cocoa, -swix, -orange) |>\n   head()\n \n# Note however that (-) requires num. vectors. desc can take e.g. factors as well.)\n\n\n\n\n\n\n\n\n\n\n# Selecting some variables: \nhead(sales[,c(\"swix\", \"cocoa\")])\n\nsales |> \n  select(cocoa, swix) |> \n  head()\n\n# create new variables: \nsales |> \n  mutate(items = cocoa+swix+orange) |> \n  head()\n\nsales |> \n  transmute(\n    items = cocoa+swix+orange,\n    trans = trans) |> \n  head()\n\n# Summarise data: \nsales |> \n  summarise(sum_cocoa = sum(cocoa))\n\n# Assignment 1: how many items were sold in total?\n# Assignment 2: What was the max and min number of items bought by\n# the people that also bought cocoa?\n\nExercise:\n\nHow many items were sold in total?\nWhat is the min. and max. number of items purchased by customers that also bought at least one cocoa?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Exercise 1: \n\nsales |> \n  transmute(items = cocoa + swix + orange) |> \n  summarise(sum_items = sum(items))\n\n# Exercise 2: \nsales |> \n  filter(cocoa>0) |> \n  transmute(\n    items = cocoa + swix + orange) |> \n  summarise(\n    max_items = max(items),\n    min_items = min(items)\n  )\n\n\n\n\n\n\n\n\n\n\n# Compare the output from this command: \nsales\n\n# ..to this one: \nsales  |> group_by(customer)\n\n# Note how we in the second command have added some meta-information on groups\n# to the data frame. Groups change the results we get when applying\n# functions to the data frame. See below: \n\nsales |> \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\nsales |> \n  group_by(customer) |> \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\n# When we apply \"summarise\" to a data frame we are reducing it \n# to the summary statistics that we list in the call to the function. \n# In the command above, this is a sum and a count. There are many\n# such functions we can use - just keep in mind that the function\n# should return one item per group (or just one item if you don't\n# have groups). \n# \n# Make sure you are aware of the difference between mutate and \n# summarise: mutate *adds* a variable to the data frame, \n# summarise aggregates it. \n\nExercise: How many cocoas were bought in total by customers who also bought more than two oranges? (Hint: How does mutate work on a grouped data frame?)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nsales |> \n  group_by(customer) |> \n  mutate(sum.orange = sum(orange)) |> \n  filter(sum.orange > 2) |> \n  summarise(sum.cocoa = sum(cocoa))\n\n\n\n\n\n\n\nApplying a group_by to a data frame keeps the data frame grouped until either you have summarized the data frame or you apply ungroup to the data frame. This is important to be aware of, many of the Tidyverse-functions behave differently on grouped and ungrouped data sets, and it can be easy to forget to ungroup-it after it is no longer needed.\nRecently however, dplyr-functions support a separate .by-argument (or in some cases, by), that allows you to group the data frame directly in a function call instead of grouping in a separate step before. The grouping will then only apply to the called function, and doesn’t have to be undone with a ungroup afterwards.\nAs an example - two ways below of summing the number of cocoas bought by customer with more than two transactions. Both give the same results, but maybe in this case the .by-argument makes the code both more readable and condensed.\n\nsales |>\n  group_by(customer) |>\n  filter(n() > 2) |>\n  ungroup() |>\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         <dbl>\n1           25\n\nsales |>\n  filter(n() > 2, .by = customer) |>\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         <dbl>\n1           25\n\n\n\n\n\n\n\nFinally, we will join data frames. See the R4DS-book. Joins are necessary when we want to analyse two or more data sets jointly. In our simple example, we have two data sets: one with transactions, and one with customers:\n\n# Read in both sheets\nsales <- read_excel(\"data-geilo.xlsx\", sheet=\"Sales\")\ncustomers <- read_excel(\"data-geilo.xlsx\", sheet=\"Customers\")\n\n# See that sales stores transactions, as well as a link to a \n# customer number: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# In the \"customers\"-file we find info on each customer: \nhead(customers)\n\n# A tibble: 6 × 3\n  customer hotel    room\n     <dbl> <chr>   <dbl>\n1        1 Vestlia   394\n2        2 Vestlia   489\n3        3 Holms     219\n4        4 Holms     155\n5        5 Holms     204\n6        6 Holms       6\n\n\ndplyr has six join functions, but of these left_join() and inner_join() are probably the most useful. Both functions take need two data frames as arguments, and joins them together into a single data frame. By default, the functions will match together rows based on column names that occur in both data sets.\n\nleft_join() returns all values from the first data frame, with all columns and values from the second data frame where there is a match between the two data sets. This means that we don’t keep any records from the second data set that are unmatched to the first data set.\ninner_join() keeps only records that are matched in both data sets.\n\nSee examples below, as well as the output produced.\n\nx <- tibble(id=c(1,2,3))\ny <- tibble(id=c(1,2,4), value=c(\"a\",\"b\",\"c\"))\n\nx |> left_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     3 <NA> \n\nx |> right_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     4 c    \n\nx |> full_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 2\n     id value\n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     3 <NA> \n4     4 c    \n\nx |> inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 2\n     id value\n  <dbl> <chr>\n1     1 a    \n2     2 b    \n\n# Filtering joins: \nx |> semi_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 1\n     id\n  <dbl>\n1     1\n2     2\n\nx |> anti_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 1 × 1\n     id\n  <dbl>\n1     3\n\n\nWhen joining, you should pay careful attention to the join conditions. In the joins above are equivalent to writing the join condition explicitly - which in the case below means that id from the first data set should be equal to id in the second data set to get a match. These join conditions may be exanded to join on e.g. inequalities and multiple columns with different names in the different data sets.\n\nx |> left_join(y, by=join_by(id==id))\n\n# A tibble: 3 × 2\n     id value\n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     3 <NA> \n\n\nExercise: Use the dplyr join verbs to create the following four different results:\n\nA dataframe with transactions and customer info filled in\nA dataframe with transactions for all registered customers\nA dataframe with transactions for customers that are not registered\nA dataframe that combines all the information in both data sets\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# 1: \nsales |> \n  left_join(customers, by=join_by(customer==customer)) |> \n  arrange(customer) |> \n  head()\n\n# A tibble: 6 × 7\n  trans customer orange cocoa  swix hotel    room\n  <dbl>    <dbl>  <dbl> <dbl> <dbl> <chr>   <dbl>\n1     6        1      1     3     3 Vestlia   394\n2    18        1      1     3     1 Vestlia   394\n3     2        3      1     5     0 Holms     219\n4    15        3      3     3     0 Holms     219\n5    16        3      3     1     0 Holms     219\n6    17        5      0     5     1 Holms     204\n\n# 2: \nsales |> \n  semi_join(customers, by=join_by(customer==customer)) |> \n  arrange(customer) |> \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1     6        1      1     3     3\n2    18        1      1     3     1\n3     2        3      1     5     0\n4    15        3      3     3     0\n5    16        3      3     1     0\n6    17        5      0     5     1\n\n# 3: (why does this give different res. than is.na()?)\nsales |> \n  anti_join(customers, by=join_by(customer==customer)) |> \n  arrange(customer) |> \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  <dbl>    <dbl>  <dbl> <dbl> <dbl>\n1    21       42      5     0     7\n2     4       NA      2     2     2\n3     5       NA      3     1     1\n4     7       NA      1     1     1\n5     8       NA      8     2     1\n6     9       NA      2     1     1\n\n# 4:\nfull <- \n  sales  |> \n  full_join(customers, by=join_by(customer==customer)) \n\n\n\n\n\n\n\nA word of caution on tidyverse: dplyr is somewhat contested among R-users. Some claim it is very easy to learn (although I’m not aware of any studies). Critics argue that it is slow compared to data.table. data.tablecan (sometimes) beat Python/Pandas in terms of speed not, but dplyr can not do that. Note also that the tidyverse ecosystem in general is continually being updated. This means that if you try to run the commands from this course on a fresh install of R and Tidyverse in a few years, it might not work unless you update the syntax.\nThink about your usage of programming: If it is occasional scripting, ad-hoc reports etc, then speed is often not important, and occasionally changing syntax might not be an issue.\nHowever, in my experience the tidyverse is significantly “faster” than alternatives for important use cases - which involve many exploratory analyses, data visualizations, and trying out many different ways of modelling a problem (i.e. usually what constrains your time may be how fast you can express what you want - not how long you need to wait for results).\nLet’s wrap up this chapter with a final exercise:\nExercise: Create a summary statistics with the following properties:\n\nCustomer on rows, with all customers as well as non-registered customers (non-registered can be in a single row)\nIn addition to customer numbers, four columns:\n\nNumber of transactions in total\nTotal sales of each type of item\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Why doesn't this work!?\n#(check e.g. customer nr 2!):\nfull |>  \n  arrange(customer) |> \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |> \n  group_by(customer) |> \n  summarise(count         = n(),\n            sale.orange   = sum(orange  , na.rm=T),\n            sale.cocoa    = sum(cocoa   , na.rm=T),\n            sale.swix     = sum(swix    , na.rm=T))\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   <chr>        <int>       <dbl>      <dbl>     <dbl>\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                1           0          0         0\n 4 3                3           7          9         0\n 5 4                1           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                1           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10\n\n# A better way: \nfull |> \n  arrange(customer) |> \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |> \n  group_by(customer) |> \n  summarise(\n    count         = sum(!is.na(trans)),\n    sale.orange   = sum(orange  , na.rm = T),\n    sale.cocoa    = sum(cocoa   , na.rm = T),\n    sale.swix     = sum(swix    , na.rm = T)\n  )\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   <chr>        <int>       <dbl>      <dbl>     <dbl>\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                0           0          0         0\n 4 3                3           7          9         0\n 5 4                0           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                0           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10"
  },
  {
    "objectID": "03-graphics.html",
    "href": "03-graphics.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will next explore the rich posibilities for making convincing visualizations in R; in particular using the ggplot2-package. We will start from the very basic syntax of making a plot, and then work our way towards making publication ready illustrations.\nThere are a lot of online resources for ggplot2, for example:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\nTwenty rules for good graphics, a useful blog post by statistician Rob Hyndman.\nThe books by Edward Tufte are classic works in the more general topic of data visualization.\n\n\n\n\n\nWe introduce the ggplot2-package and look at the basic syntax of that package and how it differs from the plotting engine that is built into the basic installation of R. Make sure that you have installed this package (using install.package(\"ggplot2\")) before proceeding to the rest of the lesson.\nWe can repeat the links that are mentioned at the end of the video:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\n\n\n\n\n\n\nNot only is ggplot2 picky in the sense that it only accepts data frames as the first argument to the ggplot()-function, we also need to be careful about the “shape” of the data frame. In this video we discuss the difference between storing data in the “wide” format versus the “long” format. We always prefer the latter when doing data science, in particular when using ggplot2 for creating visualizations.\nThe basic principle for the long format (which is discussed in length by Hadley Wickham, chief data scientist at RStudio and master mind of the tidyverse, in his article Tidy Data) is the following:\n\nOne observation is one row, one variable is one column.\n\nHere you can download the data set for this lesson: data-temps.csv.\n\nlibrary(ggplot2)      # For plotting\nlibrary(tidyr)        # For pivot_longer() and pivot_wider()\nlibrary(readr)        # For reading csv\nlibrary(dplyr)        # For pipe etc.\n\ntemps <- read_csv(\"data-temps.csv\")\n\nlong <-\n  temps %>% \n  pivot_longer(cols = -machine, \n               names_to = \"when\", \n               values_to = \"temperature\")\n\nlong %>% pivot_wider(id_cols = machine, \n                     names_from = when, \n                     values_from = temperature)\n\nggplot(long, aes(x = machine, y = temperature)) +\n  geom_point()\n\n# Color based on a variable (map a variable to color)\nggplot(long, aes(x = machine, y = temperature, colour = when)) +\n  geom_point()\n\n\n\n\n\n\nWe load the data that we are going to use in the lessons to follow. We look at hourly traffic volume over the Sotra bridge during 2017 and 2018. This bridge is particularly vulnerable to traffic jams during rush hour, which we will look at in more detail through visualizations using ggplot2.\nYou can download the data here: data-sotra.Rdata. Make sure that you have set the correct working directory and loaded the data set into R before proceeding to the next lesson.\n\nload(\"data-sotra.Rdata\")\nhead(traffic.df)\ntail(traffic.df)\n\n\n\n\n\n\n\n\n\nWe look at some basic plots for visualizing a single variable. First, we make a histogram and a density plot for the continuous variable hourly.volume, and then we look at two different ways to make bar plots for a categorical variable, using geom_bar() and geom_col(). In the first of these cases we provide ggplot() with the raw data set, so that the occurrences in the categorical variable has to be counted before being presented in the plot. In the second case we provide the counts directly, which is something that easily happens in practice.\nWe also consider the practical problem of sorting the bars in the plot according to their size. We did that by modifying the data directly using the fct_reorder()-function that can be found in the forcats package.\n\n# Histogram\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_histogram(bins = 100)\n\n# Density plot\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_density()\n\n# Bar plot for raw data, ggplot does the counting\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar()\n\n# Bar plot for finished counts. \n# (group_by() and summarise() are used to simulate that situation)\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\n# # Ordering the bars by size\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %>% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\nExercise:\nInvestigate the possibility of making the following modifications to the last bar plot that we made:\n\nFlip the coordinate system so that the bars become horizontal.\nReverse the order of the bars, so that the tallest (or now: longest) bar comes first.\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1. Just change the aesthetic mapping so that weekday.holiday now goes to the\n# y-axis, and the count goes to the x-axis:\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %>% \n  ggplot(aes(y = weekday.holiday, x = count)) +      # <- Change the aesthetic mapping\n  geom_col()\n\n\n\n# 2. A quick look at \"?fct_reorder()\" reveals that there is a \".desc\"-argument\n# in that function. Flipping that to TRUE does the trick.\ndf.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count, .desc = TRUE)) %>% \n  ggplot(aes(y = weekday.holiday, x = count)) +                # /\\\n  geom_col()                                                   # |  Add this argument\n\n\n\n\n\n\n\n\n\n\n\n\nWe move on to look at a couple of alternatives when visualizing two variables. The first one is the scatter plot (using geom_point()) with some useful options, and the second one is to add text labels to a plot.\nIn the latter example we see how we can add information from different data frames in the same plot, and also that we can update the aesthetic mapping on later layers if we need to do that.\nAgain, the cheat sheet reveals that we are barely scratching the surface, there are many more geom_*()-functions that we can use to create just about any plot that you can imagine!\n\n# The basic scatterplot with alpha and smoother\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# A data frame containing the information required to plot text labels on top of\n# the bars\ntext_labels <- \n  df.traffic %>% \n  group_by(weekday.holiday) %>% \n  summarize(count = n()) %>% \n  mutate(label = paste(count, \"days\"))\n\n# The final plot with text labels\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n\n\n\n\n\nCreating plots and graphs for various purposes takes more time than you think, not in the least because you may want to tweak all the small details so that it looks just perfect. The ggplot2-package (as well as various add-on packages) contain all the tools you will ever need for this (and many more). In this video we look at some ways to work with this. The most important thing in this lesson is perhaps not the functions that we use, but rather the process we follow when solving the problem using a combination of documentation, googling, trial and error.\n\ndf.traffic %>% \n  tail(n = 500) %>% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\nExercise: Filter out the last 24 hours instead, and make one tick mark for each hour showing the time on a date + 24 hour format (i.e 31.12.2018 12:00, 31.12.2018 13:00, etc). Make adjustments that make the labels readable. Also, add points for each observation\nHint 1: look at the options we deleted when adjusting the size of the labels.\nHint 2: Look at this blog post for more options when formatting date strings.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %>% \n  tail(n = 24) %>%                                    # Change the filtering\n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() + \n  geom_point() +                                      # Add points as well  \n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 hour\",                             # Change interval,\n                   labels = function(x) format(x, \"%d.%m%Y %H:%M\")) +  # format, and size \n  theme(axis.text.x = element_text(size = 7, angle = 90))              # of labels\n\n\n\n\n\n\n\n\n\n\n\n\nIt is important in many situations to be able to visualize group membership, and in this lesson we see two such princinples: using colour in a single plot by mapping the group membership variable to the colour dimension of the plot, and one where we use facet_wrap() in order to make one panel for each group.\n\n# Using colour\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# Using facet_wrap\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %>% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth() +\n  facet_wrap(~ weekday.holiday) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\nExercise:\nLet us make a different plot now. We want to make one line plot per day, and colour them by which day it is. Try to make the following plot.\n\nObviously we must use geom_line() instead of geom_point()\nOne problem is to get one line per day. In order to do that you need a variable that uniquely identifies the date of the day (which we have), and a way to map that variable to the group dimension of the plot. See if you find something useful under the headline Aesthetics in the help file for geom_line().\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday, group = date)) +\n  geom_line(alpha = .2) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\n\n\n\n\n\n\n\n\nWe look at some options for making final adjustments to your plot in order to make them publication ready, including the possibility of including several plots to the same canvas by means of the patchwork-package.\nYou can find the blog post about the mmtable2-package here.\n\n# Store the three plots under separate variable names\np1 <-\n  df.traffic %>% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %>% \n  filter(hourly.volume != 0) %>% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\") +\n  labs(colour = \"Weekday\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np2 <- df.traffic %>% \n  tail(n = 500) %>% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\np3 <- ggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n# Assemble using the patchwork-package\nlibrary(patchwork)\n(p1 + p2)/p3"
  },
  {
    "objectID": "04-functions-and-loops.html",
    "href": "04-functions-and-loops.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will next introduce two vital programming techniques: functions and loops. With these techniques we will be able to leverage that we are working with a programming language, as opposed to manually moving around data in a spreadsheet. This will allow us to both do more calculations as well as writing more complex code.\nIn programming, functions and loops are essential building blocks that allow you to create efficient and reusable code. Functions allow you to encapsulate a piece of code into a named block, which you can then call from other parts of your program. Loops allow you to repeat a block of code a certain number of times, or until a certain condition is met.\nFunctions and loops are particularly important in data science, where you often need to perform the same operation on a large dataset. By encapsulating these operations into functions and using loops to apply them to the entire dataset, you can save yourself a lot of time and effort.\nIn this chapter, we will introduce the basics of functions and loops in R programming. We will introduce loops and functions in turn before we bring it all together in the end of the chapter.\n\n\n\n\n\n# We can write a first, simple loop. Note that\n# 1:  We iterate through all numbers 1:10, starting at the beginning. \n# 2: \"i\" is available as a variable when \"print(i)\" is executed\nfor(i in 1:10) {\n    print(i)\n}\n\n\n# We can loop through other collections as well: \nfor(animal in c(\"cats\", \"dogs\", \"hamsters\")){\n  print(paste0(\"I like \", animal, \"!\"))\n}\n\n\n# Let's say we have a vector x, and want to calculate the cumulative\n# sum of it. \nx <- seq(from = 1, to = 100, by = 2)\ny <- rep(NA, length(x))\n\n\n# We *could* write out the necessary calculations as below:\ny[1] <- sum(x[1:1])\ny[2] <- sum(x[1:2])\ny[3] <- sum(x[1:3])\n\n# No no no we write a loop instead\nfor(i in 1:length(x)) {\n    y[i] <- sum(x[1:i])\n}\n\n# For this particular example, we have a base R-function that \n# also does the job: \ncumsum(x)\n\nExercise:\nThe Fibonacci sequence 1,1,2,3,5,8,13,… is defined by F_n = F_{n-1}+F_{n-2} for n>1. This sequence possesses many mysterious qualities. Look at this remarkable picture for instance: https://en.wikipedia.org/wiki/Golden_ratio#/media/File:Golden_mean.png. It displays the ratio of subsequent Fibonacci numbers, i.e. F_{n}/F_{n-1}, that quickly converges to the golden ratio \\frac{1+\\sqrt{5}}{2}= 1.618.... Can you reproduce this figure in R?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nn <- 10\n\n# Create a data frame for storing results:\ndf <- \n  tibble(\n    number = 1:n,\n    F = NA_integer_)\n\n# The first two values are just 1: \ndf$F[1:2] <- 1\n\n# Calculate the rest of the sequence\nfor(i in 3:n) {\n  df$F[i] <- df$F[i-2] + df$F[i-1]\n}\n\n# We can calculate the subsequent ratios using a one-liner like this:\ndf$F[2:n]/df$F[1:(n-1)]\n\n# Alternatively, we can use the lag-function in dplyr. Note that\n# the first ratio is missing. \ndf <- \n  df |> \n  mutate(ratio = F / lag(F, order_by = number))\n  \n# Define the golden ratio\nphi = (1 + sqrt(5))/2\n\n# We can plot this with ggplot. \ndf |> \n  ggplot(aes(x = number, y = ratio)) +\n  geom_line(colour = \"red\") +\n  geom_point(colour = \"red\") +\n  geom_hline(yintercept = phi, colour = \"blue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet us look at the following problem. We have downloaded some stock price data files directly from Yahoo Finance (data-stockprices.zip). We are really only interested in the closing price for each stock, and we want to extract those columns and put them into a data frame. From what we know already, we can start imagining how we can do this - and remember that it is just an administrative job. We are not talking about doing any analyses, at least not yet. This is just a dirty job, but one that has to be done! It is obviously repetitive, so we’ll solve it using loops.\nWe have a folder with files. We want the “close” column in each of them in a data set because, eventually, I want to plot these time series in figures. The first obvious problem here is of course that the data is distributed across several files. Before we start loading them, however, we should recall our discussions on data structure in the previous chapter (Wide or long?).\nThe “Excel/Human”-way of storing data would probably be something like this: The first column contains the dates, and then we would have one column for each of the stock. This would make the data fit the shape of the screen alright, but the data would be wide and not suitable for further data analysis and plotting operations. Why? Because there would be several observations for each row; and furthermore: Are we certain that the dates for the different stocks match up exactly?\nSurely, it must be better to store this information in the long format; with one observation per row, and one column per variable. In this case, that would be three variables: The date, the stock, and the closing price for that stock at that date.\n\n# Let us play a bit with first one and see what we get:\n\n# Take the first one and select the date and the correct column\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(readr)\n\napple <- \n  read_csv(\"AAPL.csv\") |>  \n  select(Date, Close) |>  \n  mutate(Date = ymd(Date)) |> \n  mutate(stock = \"AAPL\") |>  \n  rename(date = Date,\n         close = Close) \n\n# ... boring, repetitive, and also, what if we get some other files tomorrow?\nfiles <- dir(pattern = \"*.csv\")\n\n# How many files do we have?\nlength(files)\n\n# How many rows?\nnrow(apple)\n\n# Let us initialize an empty data frame where we fill inn the correct columns inside\n# a for loop\nstockprices <- \n  tibble(\n    date = ymd(),\n    close = numeric(),\n    stock = character())\n\n# Filling the rows\nfor(stockfile in files){\n  stockprices <- \n    read_csv(stockfile) |>  \n    select(Date, Close) |>  \n    mutate(Date = ymd(Date)) |> \n    rename(date = Date,\n           close = Close) |> \n    mutate(stock = tools::file_path_sans_ext(stockfile)) |> \n    bind_rows(stockprices)\n}\n\n\n\n\n\n\nIn non-compiled languages such as R, loops tend to be slow, so experienced programmers often tries to avoid using them. Sometimes we can avoid using loops by finding a function that performs a particular task directly on a vector (or list) of elements, meaning that we do not have to explicitly write out the looping ourselves. This is called vectorizing our code.\n\n# For example, we calculated the cumulative sum of a vector of numbers by\n# creating the following loop:\n\npartial_sum <- function(x) {\n  ps <- rep(NA, length(x))\n  for(i in 1:length(x)) {\n    ps[i] <- sum(x[1:i])\n  }\n  return(ps)\n}\n\n# Incidentally, there is a function that does exactly the same operation in R\n# directly on the vector x:\ncumsum(x)\n\nlibrary(microbenchmark)\ntest <- microbenchmark(partial_sum(x), cumsum(x))\n\n# Why such a big difference? The built-in function is written in a much faster\n# language. Always vectorize your code if possible. This can make a huge\n# difference in bigger projects when we are dealing with hours and days instead\n# of nanoseconds.\n# \n\n# There are many ways to both vectorize and speed up our code \n# in R (particularly the \"apply\"-family of functions!). We will return\n# to this topic in BAN400. \n\n\n\n\n\n\nWriting your own functions is a great tool that can allow you to both do more complex calculations and simplify your code. Another benefit is that it allows you to free up memory in your own brain when developing code. Once you have figured out how to solve a specific problem, you can store your solution in a function. This way, you can re use your solution many times, without having to remember exactly how it was solved.\nThe book Clean Code presents principles and guidelines when writing functions. A few principles we should keep in mind when writing functions are:\n\nFunctions should be short\nA function should do one thing\nUse understandable names of functions and arguments\n\nNote that we use the terms “principles” here, and not rules. However, to motivate why these are good principles to strive for when writing code, consider what a function would look like if it does not adhere to the principles….it will be a complicated mess that is hard to understand, debug and use.\n\n# We use functions all the time\nplot(1:10, (1:10)^2, type = \"l\")\n\n# or we get out a number:\nmean(stockprices$close)\n\n# or perhaps a numerical summary of a data set\nsummary(stockprices)\n\n# Perhaps we want to make one of those ourselves. \nsubtract <- function(number1, number2) {\n    return(number1 - number2)\n}\n\nsubtract(10, 5)\nsubtract(5, 10)\n\nsubtract_sqrt <- function(number1, number2) {\n    sqrt(number1 - number2)\n}\n\nsubtract_sqrt(10, 5)\nsubtract_sqrt(5, 10)\n\nsubtract_sqrt <- function(number1, number2) {\n    if(number2 > number1) {\n        stop(\"Can't take the square root of a negative number, make sure that a >= b!\")\n    } else {\n        sqrt(number1 - number2)\n    }\n}\n\nsubtract_sqrt(5, 10)\nsubtract_sqrt(10, 5)\n\nExercise:\nMake a function that plots the stock value time series for a given stock.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nplotprice <- function(ticker, pricedata) {\n  \n  pricedata |> \n    filter(stock == ticker) |> \n    ggplot(aes(x = date, y = close)) +\n    geom_line()\n  \n}\n\nplotprice(\"AAPL\", stockprices)\n\n\n\n\n\n\n\n\n\nOkay, so let us try to make use of this if we imagine the following problem. We have collected the stock price data as before, and we have been tasked with presenting this data in a meeting. You really want to be able to create pretty plots on the fly with different stocks, perhaps in different formats, and you want the option to save the plot as well as a pdf file. Also, you want to be able to normalize the plots with a swith in the function so that they show percentage deviations from the level on the first day in the observation period rather than the raw price.\n\n# We build a function step by step. An we start with a simple version much like\n# the one we built in the last lesson:\n\nplotStocks <- function(data) {\n  data |> \n    ggplot() +\n    geom_line(mapping = aes(x = date, y = close, colour = stock)) +\n    xlab(\"\") +\n    ylab(\"\") +\n    labs(linetype = \"Ticker\") +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    geom_text(mapping = aes(x = x,\n                            y = y,\n                            label = stock),\n              data =   \n                . |> \n                group_by(stock) |> \n                summarize(x = tail(date, n = 1),\n                          y = tail(close, n = 1)),\n              hjust = -.3) + \n    scale_x_date(expand = c(.14, 0))\n}\n\nnormaliseAndPlot <- \n  function(data,norm=FALSE){\n    if(norm){\n      data |>  \n        group_by(stock) |> \n        arrange(stock,date) |> \n        mutate(\n          firstclose = head(close,n=1),\n          close = close/firstclose\n        ) |> \n        plotStocks() +\n        ggtitle(\"Normalized prices\")\n    }else{\n      data |> \n        plotStocks() + \n        ggtitle(\"Prices\")\n    }\n  }\n\nstockprices |>  \n  filter(stock %in% c(\"AAPL\", \"MSFT\", \"SIRI\")) |> \n  normaliseAndPlot(norm = TRUE) |> \n  ggsave(filename = 'test.pdf')\n\n\n\n\nThere are occasions where you might need smaller function for one-time use, and you don’t want to add a function to your environment. In these cases it can make sense to create an anonymous function.\nIn the context of pipes, the syntax for defining an applying an anonymous function is:\n{\\(x) content of function}()\nSee below for an example of an anonymous function in the context of pipes:\n\n1:3 |> \n   {\\(x) mean(x ^ 3) - mean(x)}()\n\n[1] 10"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Please direct any feedback to the course instructors:\nHåkon Otneim: hakon.otneim@nhh.no\nOle-Petter Moe Hansen: ole-petter.hansen@nhh.no"
  },
  {
    "objectID": "assignment-01.html",
    "href": "assignment-01.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "We will look at the monthly returns on the NASDAQ composite stock index from August 2013 to June 2018, as well as the returns on 16 individual stocks listed on NASDAQ. The data is contained in the file data-nasdaq-returns.xls, and has been collected from the Yahoo Finance website.\nPut the data in an appropriate folder on your computer. Perform the following tasks:\n\nRead the data into R and take a first look at the data set. The main index is in the NASDAQ-column.\n\n\n\n\n\n\n\nClick here to see how the top of the data set should look like after you have loaded it into R.\n\n\n\n\n\n\n\n# A tibble: 59 × 11\n   Date     NASDAQ    ADBE     AMZN     AAPL     BBBY     CSCO    CMCSA     COST\n   <chr>     <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n 1 2013-0… -0.0101 -0.0329 -0.0696   0.0739  -3.64e-2 -0.0933  -0.0686  -0.0484 \n 2 2013-0…  0.0494  0.127   0.107   -0.0217   4.79e-2  0.00513  0.0695   0.0291 \n 3 2013-1…  0.0386  0.0430  0.152    0.0920  -5.17e-4 -0.0378   0.0535   0.0243 \n 4 2013-1…  0.0351  0.0461  0.0781   0.0619   9.14e-3 -0.0598   0.0466   0.0611 \n 5 2013-1…  0.0283  0.0532  0.0130   0.00886  2.87e-2  0.0540   0.0412  -0.0525 \n 6 2014-0… -0.0176 -0.0116 -0.106   -0.114   -2.29e-1 -0.0235   0.0466  -0.0576 \n 7 2014-0…  0.0486  0.148   0.00946  0.0500   6.03e-2 -0.00503 -0.0520   0.0388 \n 8 2014-0… -0.0257 -0.0430 -0.0737   0.0198   1.43e-2  0.0280  -0.0324  -0.0448 \n 9 2014-0… -0.0203 -0.0636 -0.101    0.0948  -1.02e-1  0.0303   0.0338   0.0352 \n10 2014-0…  0.0306  0.0452  0.0273   0.0702  -2.08e-2  0.0633   0.00846  0.00293\n# … with 49 more rows, and 2 more variables: DLTR <dbl>, EXPE <dbl>\n\n\n\n\n\n\nMake a new data frame containing only the date column and returns on the main index as well as one of the individual stocks of your choosing. Name the new data frame appropriately.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\nFor example, after picking ADBE as the stock:\n\n\n# A tibble: 59 × 3\n   Date        NASDAQ    ADBE\n   <chr>        <dbl>   <dbl>\n 1 2013-08-01 -0.0101 -0.0329\n 2 2013-09-01  0.0494  0.127 \n 3 2013-10-01  0.0386  0.0430\n 4 2013-11-01  0.0351  0.0461\n 5 2013-12-01  0.0283  0.0532\n 6 2014-01-01 -0.0176 -0.0116\n 7 2014-02-01  0.0486  0.148 \n 8 2014-03-01 -0.0257 -0.0430\n 9 2014-04-01 -0.0203 -0.0636\n10 2014-05-01  0.0306  0.0452\n# … with 49 more rows\n\n\n\n\n\n\nMake a scatterplot of the two variables in your newly created data frame.\n\n\n\n\n\n\n\nClick here to see the plot should look like.\n\n\n\n\n\nStill, using ADBE, will of course look a bit different if you have chosen a different stock:\n\n\n\n\n\n\n\n\n\nThe function sign(x) returns the sign of x, that is, it returns -1 if x is negative and 1 if x is positive. Make two new columns, named sign_NASDAQ and a corresponding name for the stock that you have chosen to include, that contains the sign of the return, indicating whether the index or stock went up or down that day.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 5\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign\n   <chr>        <dbl>   <dbl>       <dbl>     <dbl>\n 1 2013-08-01 -0.0101 -0.0329          -1        -1\n 2 2013-09-01  0.0494  0.127            1         1\n 3 2013-10-01  0.0386  0.0430           1         1\n 4 2013-11-01  0.0351  0.0461           1         1\n 5 2013-12-01  0.0283  0.0532           1         1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1\n 7 2014-02-01  0.0486  0.148            1         1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1\n10 2014-05-01  0.0306  0.0452           1         1\n# … with 49 more rows\n\n\n\n\n\n\nMake another column consisting of the sum of the two sign columns divided by two. The resulting value will then be -1 if both the index and the stock went down that day, 0 if they went in separate directions, and 1 if both went up.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 6\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign   sum\n   <chr>        <dbl>   <dbl>       <dbl>     <dbl> <dbl>\n 1 2013-08-01 -0.0101 -0.0329          -1        -1    -1\n 2 2013-09-01  0.0494  0.127            1         1     1\n 3 2013-10-01  0.0386  0.0430           1         1     1\n 4 2013-11-01  0.0351  0.0461           1         1     1\n 5 2013-12-01  0.0283  0.0532           1         1     1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1    -1\n 7 2014-02-01  0.0486  0.148            1         1     1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1    -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1    -1\n10 2014-05-01  0.0306  0.0452           1         1     1\n# … with 49 more rows\n\n\n\n\n\n\nWe would like to count the number of days for which the new sum-column is either -1, 0, or 1. Do that by applying the function table() to the sum-column. (Recall that we can pick out individual columns using the dollar-sign).\n\n\n\n\n\n\n\nClick here to see how the output should look like.\n\n\n\n\n\nFor the ADBE-stock:\n\n\n\n-1  0  1 \n15 10 34 \n\n\n\n\n\n\nIn the tasks above you may (or may not) have created several intermediate data frames under different names for each problem, or perhaps you have overwritten the data frame for each new task. Let us rather complete task 1, 2, 4 and 5 in one single operation, where you just append each task to the previous using the pipe-operator. That way you only need to come up with one name for the data set.\n\n\n\n\n\n\n\nClick here to see to see a hint if you need to.\n\n\n\n\n\nThe code may look something like this, replace the dots:\n\nstock_data <-\n  read_excel(...) %>%       \n  select(...) %>%                  \n  mutate(...) %>%          \n  mutate(...) %>%              \n  mutate(...)       \n\n\n\n\n\n\n\nThe .csv-file (comma separated values) is a common format for storing data in a plain text file. The file data-missile.csv contains data on North Korean missile launches from 1984 until 2017. Put the file in folder on your computer and inspect the contents by opening it in a text editor such as Notepad or Textedit.\nR ships with a function read.csv() that we can use to read csv-files in the same way as we use read_excel() to read excel-files. We will, however, use a function from the readr-package called read_csv() for this purpose that does almost the same thing as the default read.csv()-function. There are some subtle differences between these two functions that are not very important, but read_csv() works a little bit better together with many other functions and packages that we will use later.\nLoad readr using the library() function. If you get an error message telling you that there is no package called 'readr', then you need to install it first using the install.packages()-function.\nLoad the data into into R using the following command:\n\nmissile <- read_csv(\"data-missile.csv\")\n\nLook at the data. The variable «apogee» is the highest altitude reached by the missile in km. Calculate the following statistics for this variable:\n\nThe mean.\nThe median.\nThe standard deviation.\n\n\n\n\n\n\n\nMaybe you get some unexpected results here. You need to troubleshoot the problem in order to solve the issue. Click here if you need some hints on what to try.\n\n\n\n\n\nThe problem is that you get NA-values right? Why is this? Look at the data, and you will see that many values are missing, and they should be ignored when calculating the mean, median and standard deviation. Look at the help files for the functions in question (e.g. ?mean) to see if there is something you can to to fix the issue."
  },
  {
    "objectID": "assignment-02.html",
    "href": "assignment-02.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "The first part of the assignment is intended to give you to practice writing pipes using a Tidyverse data set where are are good online resources. You don’t have to hand in the first part. The second part uses a survey data set, and should be handed in.\n\n\nWe’ll use a data set from Tidyverse of all flights departing from New York City. The data set can be installed as a package install.packages(\"nycflights13\"). After calling library(nycflights13) you should have a data set flights available.\nYou may want to skim over R4DS on transformations. This chapter contains both a good introduction to the flights data as well as a recap on many of the concepts used in class on data wrangling.\n\nlibrary(nycflights13) \nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\nAssignment 1: Complete the first exercise of 4.2.5 in R4DS (repeated below):\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nSee also this solution manual for alternative ways of solving the assignments.\n\n# 1\nflights |> \n  filter(arr_delay >= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# 2\nflights |> \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# 3\nflights |> \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# 4\nflights |> \n  filter(month %in% c(7, 8, 9))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# 5\nflights |> \n  filter(dep_delay <=0 ) |> \n  filter(arr_delay >= 120)\n\n# A tibble: 29 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3    18     1844           1847        -3       39           2219\n 7  2013     4    17     1635           1640        -5     2049           1845\n 8  2013     4    18      558            600        -2     1149            850\n 9  2013     4    18      655            700        -5     1213            950\n10  2013     5    22     1827           1830        -3     2217           2010\n# ℹ 19 more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# 6\nflights |> \n  filter(dep_delay >= 60) |> \n  mutate(delay_flight_decrease = dep_delay - arr_delay) |> \n  filter(delay_flight_decrease > 30)\n\n# A tibble: 1,844 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 12 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, delay_flight_decrease <dbl>\n\n\n\n\n\nAssignment 2: Answer the questions below\n\nSpeed is distance divided by airtime. Use the planes dataframe (included in the nycflights13-package) to find the fastest and slowest aircraft model measured by Your answer should be a tibble containing the fastest and slowest plane\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nflights |> \n  inner_join(planes, by=join_by(tailnum)) |> \n  mutate(speed_kmh = 1.609344 * distance / (air_time/60)) |> \n  summarise(avg_speed_kmh = mean(speed_kmh, na.rm=TRUE), .by=model) |> \n  filter(avg_speed_kmh %in% c(max(avg_speed_kmh), min(avg_speed_kmh)))\n\n# A tibble: 2 × 2\n  model       avg_speed_kmh\n  <chr>               <dbl>\n1 CL-600-2B19          501.\n2 777-222              777.\n\n\n\n\n\n\n\n\nFor this homework you will practice your data wrangling skills using a survey data set. The data comes from the European Social Survey, and is available at [europeansocialsurvey.org] in various formats. Further, the data from ESS contains many variables with abbreviated/encoded names. The file contains survey responses from Norway in 2016. To get you started with the assignment, you may use the commands below to read in the data set. You may need to install the package foreign first in order to use the read.dta function.\n\nlibrary(foreign)\n\ndf <- \n  read.dta(\"data-ESS8NO.dta\") %>%\n  transmute(\n    party = prtvtbno, \n    age = agea, \n    religiosity = rlgdgr, \n    income_decile = hinctnta)\n\nThe variables of interest prtvtbno, agea, rlgdgr and hinctnta are defined in the attached documentation, together with more information on the actual questions. To keep it simple, we rename them to party, age, religiosity and income_decile.\n\nProvide summary statistics of age of respondents split by the party the respondents voted for last election. Who has the oldest/youngest voters? Where is the standard deviation of voters age the largest? Do not report numbers for parties with less than 25 respondents.\nThe variables religiosity, income_decile and party are encoded as factors (take a look at ?factor for en explanation). Further, they contain some non-responses such as “Don’t know”, “Refusal” and “No answer”. Find a method for filtering out all the non-responses. How many respondents did not provide an eligible response to each of the questions? How many answered both the party and income question?\nFilter out all ineligible responses for both income and party. Calculate the average religiosity of each party. Provide your answer as a data frame with one row pr. party, sorted by average religiosity.\n(Slightly trickier!) For each party with more than 75 voters, calculate the ratio of the number of respondents in income deciles 9 and 10 over the number of respondents in income deciles 1 and 2. Which party has the highest high- to low-income voters?\n\nFor completeness: When working with survey data, we usually have to apply weights to ensure estimates are representative of the population. This is because a survey sample may be a non-random sample of the general population. The survey data set provides the weights. You don’t have to worry about weights in this assignment, but please store the link “survey data -> weights?!?” in your mind for future work."
  },
  {
    "objectID": "assignment-03.html",
    "href": "assignment-03.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "Assignment 3\nIn this assignment we will visualize some of the data that was published here as part of the Tidy Tuesday project. There is one observation for each country of the world, and several interesting variables. One of them is the amount of mismanaged plastic waste in each country in a given year, and we will here build a plot using this data by adding more and more layers of complexity.\nDownload the data here, and load it into memory:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nplastic <- read_csv(\"data-plastic.csv\")\n\nWarning: Missing column names filled in: 'X1' [1]\n\n\nThe warning message just says that the first column name is empty in the csv-file (confirm that by looking at the file if you wish), and that it has been given the generic name X1.\nWe would like to visualize the association between the GDP and the amount of mismanaged plastic waste in the countries of the world. Below you will find four figures of increasing complexity. Replicate them as best as you can, and feel free to spend a few minutes looking at and interpreting the graphs.\n\nStep 1: A basic scatterplot of the log-transformed variables\n\n\n\n\n\n\n\nStep 2: Make the dots reflect country size and continent\n\n\n\n\n\n\n\nStep 3: Fix labels, add theme (you can choose whatever you want), change color palette\n\n\n\n\n\n\n\nStep 4: Add text labels for the countries and a smoother\n\n\n\n\n\n\n\nStep 5 (if you have time): Be creative. Add more layers to the plot above, or learn something else from the data set using ggplot2"
  },
  {
    "objectID": "assignment-04.html",
    "href": "assignment-04.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "In this assignment we will use what we have learned about functions and loops to explore some basic concepts that we may (or may not) remember from our basic statistics courses.\n\n\nAssume that we observe n independent realizations X_1,\\ldots,X_n of the random variable X having mean \\textrm{E}(X) = \\mu and variance \\textrm{Var}(X) = \\sigma^2. We know that the sample mean \\overline X is unbiased:\n\\textrm{E}(\\overline X) = \\mu. Furthermore, we know that the variance of the sample mean is given by\n\\textrm{Var}(\\overline X) = \\textrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2}\\cdot n \\cdot \\textrm{Var}(X) = \\frac{\\sigma^2}{n}, and hence that the standard deviation of the sample mean is given by \\textrm{SD}(X) = \\sigma/\\sqrt{n}. All this makes very good sense: As the sample size increases, the precision of the sample mean as an estimator for the population mean increases as well, because the standard deviation decreases towards zero with the speed of 1/\\sqrt{n}. Let us set up a simulation experiment to see how this works in practice.\n\nWrite a function with arguments N , mu and sigma that simulates N observations from a normal distribution with mean mu and standard deviation sigma, and returns the mean of the sample. The default value of mu should be 0 and the default value of sigma should be 1. Give the function a suitable name.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nThe function for generating observations from the normal distribution is rnorm(), see the help file to see what the arguments are called, and pay particular attention to make sure that you do not mix up variance and standard deviation. Repeated applications of the function should look something like this:\n\n\n\n\nsim_norm(N = 10)\n\n[1] 0.1488255\n\nsim_norm(N = 10)\n\n[1] 0.01521948\n\nsim_norm(N = 10)\n\n[1] -0.3332179\n\n\n… or with a different value of the population mean:\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.34236\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.54248\n\nsim_norm(N = 10, mu = 15)\n\n[1] 14.70494\n\n\n\n\n\n\nDefine a variable M and assign to it an integer, say M <- 20. Create an empty vector of length M, and use a for-loop to fill the vector with sample means from repeated applications of the function you made in question 1. Use whatever sample size that you wish.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nReplace the dots (...) in the code chunk below.\n\nM <- ...\nsample_means <- rep(NA, M)\n\nfor(i in ...) {\n  sample_means[...] <- sim_norm(...)\n}\n\nsample_means\n\nUsing N = 10 and M = 20 should give something like this (however not with identical results because you will sample different numbers).\n\n\n [1] -0.21315623 -0.58240490 -0.56568540  0.07856251 -0.20501248  0.37413594\n [7] -0.26513100 -0.18754990 -0.43141663 -0.01795517 -0.23648566 -0.05719243\n[13] -0.20066529 -0.25307109  0.22985957  0.47238978 -0.08321535 -0.27527197\n[19] -0.24231053  0.35854555\n\n\n\n\n\n\nCalculate the standard deviation of all the sample means that you just calculated above. Compare it with the true value.\n\n\n\n\n\n\n\nExpand for a little hint.\n\n\n\n\n\nThe theoretical value is \\sigma/\\sqrt{n} as we saw in the introduction, or just 1/\\sqrt{n} if you used the default value of sigma = 1.\n\n\n\nNow, we want to do a systematic experiment where we calculate the sample variance for a relatively large number of sample means (i.e. M relatively large) for different values of N. We would like to do that by creating a table with the following structure:\n\n\n\nN\nst_dev\nsigma\ntheoretical\n\n\n\n\n10\n\n1\n1.3162278\n\n\n\\ldots\n\n\\ldots\n\\ldots\n\n\n200\n\n1\n0.07071068\n\n\n\nThe first column contains the sample size that we use for calculating the sample means, the second column is empty for now, but will eventually contain the empirical standard deviation of M means for that sample size, the third column contains the value of sigma that we have chosen for the experiment, and the final column contains the theoretical value for the standard deviation of the means (\\sigma/\\sqrt{n}).\n\nCreate this table as a data frame (or a tibble) in R, with a suitable sequence of sample sizes\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou can use the seq() function to create a sequence of numbers, by for example specifying the minimum and maximum value and the interval between each entry (see ?seq for details). You can make a tibble-type data frame (a tibble is just a data frame that has a few additional features that are particularly useful in the tidyverse) using the following structure:\n\nlibrary(dplyr)    # Contains the tibble-function\nsimexp <- tibble(N = ...the vector of N-values...,\n                 st_dev = NA,\n                 sigma = ...)\n\nAnd then you can make the theoretical-column using for instance a pipe and a mutate().\n\n\n\n\nFill in the value of the st_dev-column using a for-loop and the function you made in question 1.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou must loop over all the rows of the data frame that you created in the last question, and fill in the standard deviations such as the one we calculated in question 3. That is: we can solve this by putting a for loop inside a for loop. In that case we must be careful to not mix up the two counting indices.\nYour code might look something like this, assuming that you have defined the number M and that you have initialized the data frame in the previous question:\n\nfor(i in 1:nrow(simexp)) {  # \"Outer\" for loop, using \"i\" as counting index\n  \n  # Input the code from question 3 to create a vector of sample means.\n  # You must, however change the counting index to something else (\"j\" for example)\n  # Within this \"inner\" loop, you must also pick out the correct N-value from the \n  # data frame with \"simexp$N[i]\".\n  \n  # Insert the standard deviation of the sample means into the data frame.\n  # Can be something like this:\n  simexp$st_dev[i] <- sd(sample_means)\n  \n}\n\nNB! It is good if some alarm bells go off here. Nesting for loops inside of each other is not something that we like, because they are so inefficient. A natural next step here is to see if we can get rid of one of them by vectorizing or some other technique (that might be slightly too advanced for us at this point).\n\n\n\n\nMake a graph where you compare the observed standard deviations with the theoretical value.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nHere is a simple plot using M = 200; observed as solid line, theoretical as dashed line. Looks good!\n\n\n\n\n\n\n\n\n\n\n\nIn the real world you will sometimes encounter data with strange distributions. One such example is insurance claims. For many insurance products, e.g. fire insurance, most customers will never have a claim. But occasionally a house may burn down, leading to a large claim because it must be rebuilt from the ground up. For the insurance industry it is vital that the expected claim cost is estimated correctly.\nTo see what claims data can look like, we can use the “tweedie”-package. We can interpret an observation as the total claim cost for one insurance held for a full year. The parameters phi and power control the shape of the distribution, and mu is the expected value.\nThe function rtweedie(n, mu, phi, power) generates n observations from the tweedie distribution with the specified parameters. You can try to generate 10 insurance claims from the distribution with mean mu = 10 and shape parameters phi = 10 000 and power = 1.9 using the following line:\n\nlibrary(tweedie)\nrtweedie(n = 10, mu = 10000, phi = 1000, power = 1.9)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nMost likely you will observe 10 zeros, each of which belong to the majority of houses that do not burn down in a given year. If you do observe a non-zero, however, this would represent the total amount claimed by a single (unlucky) insurance holder that year.\nSo what is the expected value of the claim for each customer? The long-run average across all customers of the insurance company, i.e. the total amount of money that the company on average pays out to their policy holders divided by the number of policy holders and the number of years of data?\nWe know the answer to this question in our particular case because we generate the observations from a known probability distribution: The answer is mu = 10 000. Knowing this number is obviously very important to the company, because it dictates that the price of each of the insurance policies should be something like\n10 000 + \\text{Costs} + \\textrm{Safety margin} + \\textrm{Profits}, that is, the amount that the insurance company expects to pay back to the customer in damages, plus the costs of running the company, plus some safety margin to make sure that the natural fluctuations from year to year do not cause the company to lose money in bad years, plus some profits that make the whole endeavor worthwhile as a business.\nThe expected payout, however, is not known in practice, and must be estimated from historical data. We usually do this by taking the average, but we see immediately that it is not obvious that this will work in our situation above where we had just ten observations. If you sampled ten zeros, then the average is zero, and we have barely any information at all regarding the amount that the insurance companly must charge for these policies. In the event that you sampled one or more non-zeros, it is quite unlikely that the average over ten observations is anywhere near the true expected value of 10 000.\nWe can ramp this experiment up a notch, by rather sampling 100 000 observations from this distributions (or claimed amount from 100 000 customers over a given year if you wish) and then taking the average:\n\nx <- rtweedie(n = 100000, mu = 10000, phi = 1000, power = 1.9)\nmean(x)\n\nThis number is likely not that far from the true expectation. The story line here is as follows: How large must the sample be for the insurance company to make useful statistical inferences about their population of customers?\nOne classical method for making statements regarding the unknown population mean is the t-test. You may recall from your introduction to statistics that the t-statistic provides a measure of the distance from the observed sample mean to some hypothesized population mean that is approximately normally distributed. This is due to the Cental Limit Theorem, which postulates that the distribution of the sample mean (and essentially any other statistic that is based on a sum of random variables such as the t-statistic) converges towards the normal distribution… which is almost always true. One exception is if the underlying distribution of the observations is exceptionally weird. We have already seen that the claim-type data that we generate from the tweedie distribution are quite special, and that we need a fair amount of data in order to see the usual convergence of the sample mean towards the population mean.\nSo: How large must the sample size be in the insurance context? Let us explore.\nIn the code snippet below, we first define the variables N and true_mu and set their value to 100 and 10 000, respectively. We then sample n observations from the tweedie distribution having expected value equal to true_mu (and phi = 1000 and power = 1.9 as above).\nFinally, we use the sampled data to test the null hypothesis whether \\mu = 10 000, a hypothesis that we know is true. However, if the p-value of the test is smaller than 5%, then we reject the null hypothesis at the 5% level.\n\nN <- 100\ntrue_mu <- 10000\nsample <- rtweedie(N, mu = true_mu, phi = 1000, power = 1.9)\nt.test(sample, mu = true_mu)\n\nYou can check this several times and see what you get. If the t-test works as it should (i.e., if the sample is large enough for the Central Limit Theorem to provide a reasonable approximation for the distribution of the test statistic), then we should reject the null hypothesis 5% of the times.\nBut, why repeat such a trivial task manually? Let us put it in a loop, an perform a proper simulation experiment. The idea is to simulate M datasets, each with N observations, and run the t-test on each one of the M datasets. Again, with a 5% significance level we should expect to reject the null in about 5% of the times that we do this, if we have enough data.\nPerform the following tasks:\n\nCreate a function simTweedieTest() that takes N as argument. The function should simulate a data set with a tweedie-distribution with parameters mu = 10000, phi = 100, power = 1.9, and run a t-test on the simulated data using the null hypothesis \\mu_0 = 10000. The function should return the p-value of the test.\n\n\n\n\n\n\n\nYou must figure out how to extract the p-value from a t-test. Google it first. Click here to see a line of code that you can adapt to use in your function.\n\n\n\n\n\n\nt.test(rtweedie(N = ..., mu = ..., phi = ..., power = ...), mu = ...)$p.value\n\n\n\n\n\nCreate a function MTweedieTests(), that takes M, N and alpha as arguments. This function should call the simTweedieTests() function M times with a sample size of N. The function MTweedieTests() should then return percentage of tests where the p-value is lower than alpha (e.g. if we run tests on M = 10 datasets, and have p-values lower than \\alpha = .05 in two of the tests, the function should return 0.2).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis can be solved by making an empty vector of size M, which we then fill with p-values using a for-loop. If you want to try to make the code briefer, without using the for-loop explicitly, you can check out the function replicate(M, fun(...)). `replicate() works well for random number generating functions.\n\n\n\n\nCreate a data frame tibble(N = c(10, 100, 1000, 5000), M = 100, share_reject = NA). Use a loop and the MTweedieTests-function to fill in the values in the share_reject-column. Create a figure with N on the X-axis and share_reject on the Y-axis. What does this tell you of the validity of the t-test in on this specific distribution? What does “large enough sample” mean for this?\n(Trickier) How general are the findings in the previous question? And can we be sure we wrote the code correctly? If the data follows a normal distribution instead of a tweedie distribution we should expect that the t-test works better at lower sample sizes. Create a figure similar to question 3, but with two curves: one for tweedie-distributed data and one for normally distributed data. You will have to rewrite the functions from questions 1-3. Think carefully on how you structure the functions: avoid duplicating code, use sensible names for arguments and functions, and ensure that the mean of the data and the test is consistent."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "This is the companion website for the course BAN400 - R Programming for Data Science, given at The Norwegian School of Economics (NHH). The purpose of this website is to provide study material such as lecture videos, exercises and assignments for students taking the course.\nBAN400 has previously consisted of two separate modules; one intensive one-week introduction to R that could be taken separately as a 2.5 ECTS course as BAN420, as well as the main course itself (pun intended), which, together with BAN420, completed the 7.5 ECTS unit BAN400.\nFrom the fall semester of 2023, we do no longer offer the intensive option. BAN400 is now offered as one regular course. We will, however, still make an explicit transition from Part 1, where we introduce basic programming, to Part 2 where we will learn a number of useful techniques that are particularly useful when working with data.\nAll announcements and course administration such as homework delivery and feedback will be carried out through the course page at Canvas, which is the learning management system used by NHH. You will need to sign up to the course in order to gain access to the Canvas page.\n\n\n\n\n\n\n\nWeek number\nTopic\nLecturer\n\n\n\n\n34\nIntroduction to R\nHO\n\n\n35\nTidy data\nOPMH\n\n\n36\nPlotting\nHO\n\n\n37\nFunctions and loops\nOPMH\n\n\n38\nManaging an R-project\nHO\n\n\n\n\n\n\n\n\n\nWeek number\nTopic\nLecturer\n\n\n\n\n39\nGit\nHO\n\n\n40\nGuest lecture\nPWC\n\n\n41\nIterations\nOPMH\n\n\n42\nMachine Learning\nHO\n\n\n43\nParallel computing\nOPMH\n\n\n44\nMany models\nHO\n\n\n45\nDocumentation and deployment\nOPMH"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "06-iterations.html",
    "href": "06-iterations.html",
    "title": "BAN400 - R Programming for Data Science",
    "section": "",
    "text": "library(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(DescTools)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rlang)\nlibrary(lubridate)\nlibrary(anytime)\n\n# Today we are going to work with some more advanced topics in \n# terms of data handling and processing. We will play with an API\n# from Vegvesenet. Vegvesenet has an API we can query for data\n# on traffic volumes at many sensor stations in Norway. \n# \n# The API uses graphQL for requests. This is a relatively\n# new language we might see more of in the future?\n# \n# Let's define a function where we can submit queries to an external API. \nGQL <- function(query,\n                ...,\n                .token = NULL,\n                .variables = NULL,\n                .operationName = NULL,\n                .url = url) {\n  pbody <-\n    list(query = query,\n         variables = .variables,\n         operationName = .operationName)\n  if (is.null(.token)) {\n    res <- POST(.url, body = pbody, encode = \"json\", ...)\n  } else {\n    auth_header <- paste(\"bearer\", .token)\n    res <-\n      POST(\n        .url,\n        body = pbody,\n        encode = \"json\",\n        add_headers(Authorization = auth_header),\n        ...\n      )\n  }\n  res <- content(res, as = \"parsed\", encoding = \"UTF-8\")\n  if (!is.null(res$errors)) {\n    warning(toJSON(res$errors))\n  }\n  res$data\n}\n\n\n# The URL we will use is stored below: \nurl <- \"https://www.vegvesen.no/trafikkdata/api/\"\n\n\n# Let's figure out which sensor stations that are operable. \n# The query below extracts all the stations, with a date for \n# when the station was in operation as well as a long/latitude. \nqry <-\n  '\n{\n    trafficRegistrationPoints {\n        id\n        name\n        latestData {\n            volumeByDay\n        }\n        location {\n            coordinates {\n                latLon {\n                    lat\n                    lon\n                }\n            }\n        }\n    }\n}\n'\n\n# Allright - let's try submitting the query: \nstations <-GQL(qry) \n\n\n# We now have the a long list in memory - 11mb! - with just \n# a little information on each station. We can note that this \n# is a list, not a dataframe. For our purposes, it would be better if\n# the list was instead a data frame, with one row pr. sensor station. \n\n\n# Note that the list only has one entry..   \nlength(stations)\n\n[1] 1\n\n# However, the list contains another list, called trafficRegistrationPoints. \n# This list has almost 5000 entries. We can select this sublist using \n# either $ or [[1]]. Note that when we subset a list, using [[i]] selects\n# the contents of the item [[i]]. \nlength(stations$trafficRegistrationPoints)\n\n[1] 6210\n\nlength(stations[[1]])\n\n[1] 6210\n\n# Let's look at the first entry of this long list. We can see there is\n# a station ID, station name, date time of latest recording from the station\n# and coordinates. This looks like something that could fit well within \n# a data frame, with columns id, name, latestdata, lat, and lon. The \n# question is how!\nstations[[1]][[1]]\n\n$id\n[1] \"91860V444292\"\n\n$name\n[1] \"Haugland\"\n\n$latestData\n$latestData$volumeByDay\n[1] \"2023-06-14T00:00:00+02:00\"\n\n\n$location\n$location$coordinates\n$location$coordinates$latLon\n$location$coordinates$latLon$lat\n[1] 59.798\n\n$location$coordinates$latLon$lon\n[1] 11.44989\n\n# We could perhaps hope that we can force this list into a data frame. For\n# this we will use as_tibble: \nstations[[1]][[1]] %>% \n  as_tibble()\n\n# A tibble: 1 x 4\n  id           name     latestData   location        \n  <chr>        <chr>    <named list> <named list>    \n1 91860V444292 Haugland <chr [1]>    <named list [1]>\n\n# We now want to apply this as_tibble transformation to each of the stations, \n# and combine them in a single data frame. We could do this with lapply, \n# and then bind toghether the rows: \nlapply(stations[[1]], . %>% as_tibble) %>% \n  bind_rows\n\n# A tibble: 6,210 x 4\n   id           name                      latestData   location        \n   <chr>        <chr>                     <named list> <named list>    \n 1 91860V444292 \"Haugland\"                <chr [1]>    <named list [1]>\n 2 01099V444361 \"Langset\"                 <chr [1]>    <named list [1]>\n 3 31744V443465 \"Grepperud\"               <chr [1]>    <named list [1]>\n 4 05768V444258 \"Torp\"                    <chr [1]>    <named list [1]>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" <chr [1]>    <named list [1]>\n 6 90227V578453 \"Finnvolldalen\"           <chr [1]>    <named list [1]>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   <chr [1]>    <named list [1]>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" <chr [1]>    <named list [1]>\n 9 42390V367287 \"Kipervegen\"              <chr [1]>    <named list [1]>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" <chr [1]>    <named list [1]>\n# i 6,200 more rows\n\n\nExercise:\nTransform the list into a data frame, with at id and name as columns, and one row per station. We can fix the date time and locations columns later, but use one of the map-functions from purrr.\n\n\nSolution\n\n\n# Using the map_df-function we traverse all the entries in the stations list, \n# and transform these lists to data frames. \n# \n# There is still some work left to do with the date time and location \n# columns. As you can see below, they are still in a list format. \nstations[[1]] %>% \n  map_df(~as_tibble(.))\n\n# A tibble: 6,210 x 4\n   id           name                      latestData   location        \n   <chr>        <chr>                     <named list> <named list>    \n 1 91860V444292 \"Haugland\"                <chr [1]>    <named list [1]>\n 2 01099V444361 \"Langset\"                 <chr [1]>    <named list [1]>\n 3 31744V443465 \"Grepperud\"               <chr [1]>    <named list [1]>\n 4 05768V444258 \"Torp\"                    <chr [1]>    <named list [1]>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" <chr [1]>    <named list [1]>\n 6 90227V578453 \"Finnvolldalen\"           <chr [1]>    <named list [1]>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   <chr [1]>    <named list [1]>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" <chr [1]>    <named list [1]>\n 9 42390V367287 \"Kipervegen\"              <chr [1]>    <named list [1]>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" <chr [1]>    <named list [1]>\n# i 6,200 more rows\n\n# We can try to pull out the insides of the contents of the latestData-\n# column. It is formatted as a list, but actually only contains one \n# date time entry. \nstations[[1]] %>% \n  map_df(~as_tibble(.)) %>% \n  head(1) %>% \n  select(latestData) %>% \n  pull\n\n$volumeByDay\n[1] \"2023-06-14T00:00:00+02:00\"\n\n\n\nExercise:\nMutate the contents of the latestData-columns, such that it is in a character format. You don’t have to format it to a proper date time (yet..)\n\n\nSolution\n\n\n# There are two complications to this one.. \n# 1:  Similarly to the previous task, we want to apply a transformation\n#     to all entries of a list.. \n# 2:  However at least one of the entries does not contain the list item\n#     \"latestData\". \n#     \n# As you can see from the documentation, the map-functions are \n# very flexible, and we can e.g. use them to extract named items\n# from a list. Below, we are asking map_chr to return the first item\n# of each sub list in latestData. However, this will fail if it \n# meets an entry that does not have aything stored under latestdata!\n# stations[[1]] %>% \n#   map_df(~as_tibble(.)) %>% \n#   mutate(latestData = map_chr(latestData, 1))\n\n\n# We could write a custom \"unlisting\"-function.\n# The function below unlists the elements of latestData - if there are \n# any elements there. If it the content is null, the function just\n# returns an empty character string. \nunlist_safe <- \n  function(x){\n    x <- unlist(x)\n    if(is.null(x)){\n      return(NA_character_)\n  }else{\n    return(x)\n  }\n}\n\nstations[[1]] %>% \n  map_df(~as_tibble(.)) %>% \n  mutate(latestData = map_chr(latestData, unlist_safe))\n\n# A tibble: 6,210 x 4\n   id           name                      latestData                location    \n   <chr>        <chr>                     <chr>                     <named list>\n 1 91860V444292 \"Haugland\"                2023-06-14T00:00:00+02:00 <named list>\n 2 01099V444361 \"Langset\"                 2023-06-06T00:00:00+02:00 <named list>\n 3 31744V443465 \"Grepperud\"               2023-06-14T00:00:00+02:00 <named list>\n 4 05768V444258 \"Torp\"                    2023-06-14T00:00:00+02:00 <named list>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23T00:00:00+02:00 <named list>\n 6 90227V578453 \"Finnvolldalen\"           2023-06-22T00:00:00+02:00 <named list>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31T00:00:00+02:00 <named list>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31T00:00:00+02:00 <named list>\n 9 42390V367287 \"Kipervegen\"              2023-06-30T00:00:00+02:00 <named list>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17T00:00:00+02:00 <named list>\n# i 6,200 more rows\n\n# Alternatively, we can use the defaults in map_chr. It will \n# now have a safe fallback value it can use if it doesn't \n# find the element we are looking for in latestData. \n# A simple solution is to use the .default-argument, and set this to missing: \nstations[[1]] %>% \n    map_df(~as_tibble(.)) %>% \n    mutate(latestData = map_chr(latestData,1, .default=NA_character_)) \n\n# A tibble: 6,210 x 4\n   id           name                      latestData                location    \n   <chr>        <chr>                     <chr>                     <named list>\n 1 91860V444292 \"Haugland\"                2023-06-14T00:00:00+02:00 <named list>\n 2 01099V444361 \"Langset\"                 2023-06-06T00:00:00+02:00 <named list>\n 3 31744V443465 \"Grepperud\"               2023-06-14T00:00:00+02:00 <named list>\n 4 05768V444258 \"Torp\"                    2023-06-14T00:00:00+02:00 <named list>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23T00:00:00+02:00 <named list>\n 6 90227V578453 \"Finnvolldalen\"           2023-06-22T00:00:00+02:00 <named list>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31T00:00:00+02:00 <named list>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31T00:00:00+02:00 <named list>\n 9 42390V367287 \"Kipervegen\"              2023-06-30T00:00:00+02:00 <named list>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17T00:00:00+02:00 <named list>\n# i 6,200 more rows\n\n\n\n\n\n\n\n# Next, let's format the date format. Date formats can be tricky, but is\n# an obstacle you just have to learn to work with. We can reformat the \n# latestData column into a date by simply using as.Date - however - \n# we now have lost information on the time of day. Let's see if we \n# can retain all the information in the column. \nstations[[1]] %>% \n  map_df(~as_tibble(.)) %>% \n  mutate(latestData = map_chr(latestData, 1, .default=NA_character_)) %>% \n  mutate(latestData = as.Date(latestData))\n\n# A tibble: 6,210 x 4\n   id           name                      latestData location        \n   <chr>        <chr>                     <date>     <named list>    \n 1 91860V444292 \"Haugland\"                2023-06-14 <named list [1]>\n 2 01099V444361 \"Langset\"                 2023-06-06 <named list [1]>\n 3 31744V443465 \"Grepperud\"               2023-06-14 <named list [1]>\n 4 05768V444258 \"Torp\"                    2023-06-14 <named list [1]>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23 <named list [1]>\n 6 90227V578453 \"Finnvolldalen\"           2023-06-22 <named list [1]>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31 <named list [1]>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31 <named list [1]>\n 9 42390V367287 \"Kipervegen\"              2023-06-30 <named list [1]>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17 <named list [1]>\n# i 6,200 more rows\n\n# There are several functions we can use to transform the string into\n# a date time variable. as_datetime in lubridate works in this case. \n# Note that the interpretation of dates may be dependent on the time zone\n# settings on your laptop. Here, we are explicitly stating that we want the\n# a Europe/Berlin tz on the variable: \nstations[[1]] %>%\n  map_df( ~ as_tibble(.)) %>%\n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) %>%\n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) \n\n# A tibble: 6,210 x 4\n   id           name                      latestData          location        \n   <chr>        <chr>                     <dttm>              <named list>    \n 1 91860V444292 \"Haugland\"                2023-06-14 00:00:00 <named list [1]>\n 2 01099V444361 \"Langset\"                 2023-06-06 00:00:00 <named list [1]>\n 3 31744V443465 \"Grepperud\"               2023-06-14 00:00:00 <named list [1]>\n 4 05768V444258 \"Torp\"                    2023-06-14 00:00:00 <named list [1]>\n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23 00:00:00 <named list [1]>\n 6 90227V578453 \"Finnvolldalen\"           2023-06-22 00:00:00 <named list [1]>\n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31 00:00:00 <named list [1]>\n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31 00:00:00 <named list [1]>\n 9 42390V367287 \"Kipervegen\"              2023-06-30 00:00:00 <named list [1]>\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17 00:00:00 <named list [1]>\n# i 6,200 more rows\n\n\n\nExercise: Finalizing the transformation\nLet’s take on the final location variable. Complete the operation by unpacking the location column into two columns: lat and lon. You may use the functions you have already seen, or see of you can find mode specialized functions.\nNote: This a nested list i.e. the contents of a cell in “location” is a list with one entry. This list contains two other lists..\nThe script should return a data frame similar to the one below (only the first few entries shown).\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nlatestData\nlat\nlon\n\n\n\n\n91860V444292\nHaugland\n2023-06-14\n59.79800\n11.44989\n\n\n01099V444361\nLangset\n2023-06-06\n60.41106\n11.24313\n\n\n31744V443465\nGrepperud\n2023-06-14\n59.70574\n11.40330\n\n\n05768V444258\nTorp\n2023-06-14\n59.72265\n11.42352\n\n\n99306V704790\nFv1702 Helg<U+00F8>yvegen\n2023-07-23\n60.75069\n10.96026\n\n\n90227V578453\nFinnvolldalen\n2023-06-22\n64.88226\n13.13444\n\n\n\n\n\n# A tibble: 6,210 x 6\n   id           name                latestData          location       lat   lon\n   <chr>        <chr>               <dttm>              <named list> <dbl> <dbl>\n 1 91860V444292 \"Haugland\"          2023-06-14 00:00:00 <named list>  59.8 11.4 \n 2 01099V444361 \"Langset\"           2023-06-06 00:00:00 <named list>  60.4 11.2 \n 3 31744V443465 \"Grepperud\"         2023-06-14 00:00:00 <named list>  59.7 11.4 \n 4 05768V444258 \"Torp\"              2023-06-14 00:00:00 <named list>  59.7 11.4 \n 5 99306V704790 \"Fv1702 Helg\\u00f8~ 2023-07-23 00:00:00 <named list>  60.8 11.0 \n 6 90227V578453 \"Finnvolldalen\"     2023-06-22 00:00:00 <named list>  64.9 13.1 \n 7 64472V705298 \"Fv1812 Stensbakkv~ 2023-07-31 00:00:00 <named list>  60.8 11.2 \n 8 85054V704808 \"Fv186 K\\u00e5rtor~ 2023-07-31 00:00:00 <named list>  60.8 11.1 \n 9 42390V367287 \"Kipervegen\"        2023-06-30 00:00:00 <named list>  61.9  6.71\n10 24310V443852 \"Nadderudv ved Dyr~ 2023-05-17 00:00:00 <named list>  59.9 10.6 \n# i 6,200 more rows\n\n\n# A tibble: 6,210 x 5\n   id           name                      latestData            lat   lon\n   <chr>        <chr>                     <dttm>              <dbl> <dbl>\n 1 91860V444292 \"Haugland\"                2023-06-14 00:00:00  59.8 11.4 \n 2 01099V444361 \"Langset\"                 2023-06-06 00:00:00  60.4 11.2 \n 3 31744V443465 \"Grepperud\"               2023-06-14 00:00:00  59.7 11.4 \n 4 05768V444258 \"Torp\"                    2023-06-14 00:00:00  59.7 11.4 \n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23 00:00:00  60.8 11.0 \n 6 90227V578453 \"Finnvolldalen\"           2023-06-22 00:00:00  64.9 13.1 \n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31 00:00:00  60.8 11.2 \n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31 00:00:00  60.8 11.1 \n 9 42390V367287 \"Kipervegen\"              2023-06-30 00:00:00  61.9  6.71\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17 00:00:00  59.9 10.6 \n# i 6,200 more rows\n\n\n\n\n\nSolution\n\n\n## We can use a similar solution we used before. First we use\n## unlist-safe to remove one level from the list, and then extract \n## the contents using map_dbl - remember these are numbers, not text. \n\nstations[[1]] %>% \n  map_dfr(as_tibble) %>% \n  mutate(latestData = map_chr(latestData, 1, .default = \"\")) %>% \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) %>% \n  mutate(location = map(location, unlist)) %>% \n  mutate(\n    lat = map_dbl(location, \"latLon.lat\"),\n    lon = map_dbl(location, \"latLon.lon\")\n  ) %>% \n  select(-location)\n\n# A tibble: 6,210 x 5\n   id           name                      latestData            lat   lon\n   <chr>        <chr>                     <dttm>              <dbl> <dbl>\n 1 91860V444292 \"Haugland\"                2023-06-14 00:00:00  59.8 11.4 \n 2 01099V444361 \"Langset\"                 2023-06-06 00:00:00  60.4 11.2 \n 3 31744V443465 \"Grepperud\"               2023-06-14 00:00:00  59.7 11.4 \n 4 05768V444258 \"Torp\"                    2023-06-14 00:00:00  59.7 11.4 \n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23 00:00:00  60.8 11.0 \n 6 90227V578453 \"Finnvolldalen\"           2023-06-22 00:00:00  64.9 13.1 \n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31 00:00:00  60.8 11.2 \n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31 00:00:00  60.8 11.1 \n 9 42390V367287 \"Kipervegen\"              2023-06-30 00:00:00  61.9  6.71\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17 00:00:00  59.9 10.6 \n# i 6,200 more rows\n\n## Alternatively, we can use unnest_wider twice. This one does some work\n## for us, and gives the same result: \n\nstations[[1]] %>%\n  map_df( ~ as_tibble(.)) %>%\n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) %>%\n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) %>% \n  unnest_wider(location) %>%\n  unnest_wider(latLon)\n\n# A tibble: 6,210 x 5\n   id           name                      latestData            lat   lon\n   <chr>        <chr>                     <dttm>              <dbl> <dbl>\n 1 91860V444292 \"Haugland\"                2023-06-14 00:00:00  59.8 11.4 \n 2 01099V444361 \"Langset\"                 2023-06-06 00:00:00  60.4 11.2 \n 3 31744V443465 \"Grepperud\"               2023-06-14 00:00:00  59.7 11.4 \n 4 05768V444258 \"Torp\"                    2023-06-14 00:00:00  59.7 11.4 \n 5 99306V704790 \"Fv1702 Helg\\u00f8yvegen\" 2023-07-23 00:00:00  60.8 11.0 \n 6 90227V578453 \"Finnvolldalen\"           2023-06-22 00:00:00  64.9 13.1 \n 7 64472V705298 \"Fv1812 Stensbakkvegen\"   2023-07-31 00:00:00  60.8 11.2 \n 8 85054V704808 \"Fv186 K\\u00e5rtorpvegen\" 2023-07-31 00:00:00  60.8 11.1 \n 9 42390V367287 \"Kipervegen\"              2023-06-30 00:00:00  61.9  6.71\n10 24310V443852 \"Nadderudv ved Dyrefaret\" 2023-05-17 00:00:00  59.9 10.6 \n# i 6,200 more rows"
  }
]