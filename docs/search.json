[
  {
    "objectID": "07-iterations-wip.html",
    "href": "07-iterations-wip.html",
    "title": "7 purrr on Traffic data",
    "section": "",
    "text": "library(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(DescTools)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rlang)\nlibrary(lubridate)\nlibrary(anytime)\n\n# Today we are going to work with some more advanced topics in \n# terms of data handling and processing. We will play with an API\n# from Vegvesenet. Vegvesenet has an API we can query for data\n# on traffic volumes at many sensor stations in Norway. \n# \n# The API uses graphQL for requests. This is a relatively\n# new language we might see more of in the future?\n# \n# Let's define a function where we can submit queries to an external API. \nGQL &lt;- function(query,\n                ...,\n                .token = NULL,\n                .variables = NULL,\n                .operationName = NULL,\n                .url = url) {\n  pbody &lt;-\n    list(query = query,\n         variables = .variables,\n         operationName = .operationName)\n  if (is.null(.token)) {\n    res &lt;- POST(.url, body = pbody, encode = \"json\", ...)\n  } else {\n    auth_header &lt;- paste(\"bearer\", .token)\n    res &lt;-\n      POST(\n        .url,\n        body = pbody,\n        encode = \"json\",\n        add_headers(Authorization = auth_header),\n        ...\n      )\n  }\n  res &lt;- content(res, as = \"parsed\", encoding = \"UTF-8\")\n  if (!is.null(res$errors)) {\n    warning(toJSON(res$errors))\n  }\n  res$data\n}\n\n\n# The URL we will use is stored below: \nurl &lt;- \"https://www.vegvesen.no/trafikkdata/api/\"\n\n\n# Let's figure out which sensor stations that are operable. \n# The query below extracts all the stations, with a date for \n# when the station was in operation as well as a long/latitude. \nqry &lt;-\n  '\n{\n    trafficRegistrationPoints {\n        id\n        name\n        latestData {\n            volumeByDay\n        }\n        location {\n            coordinates {\n                latLon {\n                    lat\n                    lon\n                }\n            }\n        }\n    }\n}\n'\n\n# Allright - let's try submitting the query: \nstations &lt;-GQL(qry) \n\n\n# We now have the a long list in memory - 11mb! - with just \n# a little information on each station. We can note that this \n# is a list, not a dataframe. For our purposes, it would be better if\n# the list was instead a data frame, with one row pr. sensor station. \n\n\n# Note that the list only has one entry..   \nlength(stations)\n\n[1] 1\n\n# However, the list contains another list, called trafficRegistrationPoints. \n# This list has almost 5000 entries. We can select this sublist using \n# either $ or [[1]]. Note that when we subset a list, using [[i]] selects\n# the contents of the item [[i]]. \nlength(stations$trafficRegistrationPoints)\n\n[1] 7895\n\nlength(stations[[1]])\n\n[1] 7895\n\n# Let's look at the first entry of this long list. We can see there is\n# a station ID, station name, date time of latest recording from the station\n# and coordinates. This looks like something that could fit well within \n# a data frame, with columns id, name, latestdata, lat, and lon. The \n# question is how!\nstations[[1]][[1]]\n\n$id\n[1] \"52742V2282262\"\n\n$name\n[1] \"ØRBEKK EV6\"\n\n$latestData\n$latestData$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n$location\n$location$coordinates\n$location$coordinates$latLon\n$location$coordinates$latLon$lat\n[1] 60.41426\n\n$location$coordinates$latLon$lon\n[1] 11.24117\n\n# We could perhaps hope that we can force this list into a data frame. For\n# this we will use as_tibble: \nstations[[1]][[1]] |&gt;  \n  as_tibble()\n\n# A tibble: 1 × 4\n  id            name       latestData   location        \n  &lt;chr&gt;         &lt;chr&gt;      &lt;named list&gt; &lt;named list&gt;    \n1 52742V2282262 ØRBEKK EV6 &lt;chr [1]&gt;    &lt;named list [1]&gt;\n\n# We now want to apply this as_tibble transformation to each of the stations, \n# and combine them in a single data frame. We could do this with lapply, \n# and then bind toghether the rows: \nlapply(stations[[1]], . %&gt;% as_tibble) |&gt; \n  bind_rows()\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nExercise:\nTransform the list into a data frame, with at id and name as columns, and one row per station. We can fix the date time and locations columns later, but use one of the map-functions from purrr.\n\n\nSolution\n\n\n# Using the map_df-function we traverse all the entries in the stations list, \n# and transform these lists to data frames. \n# \n# There is still some work left to do with the date time and location \n# columns. As you can see below, they are still in a list format. \nstations[[1]] |&gt; \n  map_df(~as_tibble(.))\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# We can try to pull out the insides of the contents of the latestData-\n# column. It is formatted as a list, but actually only contains one \n# date time entry. \nstations[[1]] |&gt;  \n  map_df(~as_tibble(.)) |&gt; \n  head(1) |&gt;  \n  select(latestData) |&gt;  \n  pull()\n\n$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n\nExercise:\nMutate the contents of the latestData-columns, such that it is in a character format. You don’t have to format it to a proper date time (yet..)\n\n\nSolution\n\n\n# There are two complications to this one.. \n# 1:  Similarly to the previous task, we want to apply a transformation\n#     to all entries of a list.. \n# 2:  However at least one of the entries does not contain the list item\n#     \"latestData\". \n#     \n# As you can see from the documentation, the map-functions are \n# very flexible, and we can e.g. use them to extract named items\n# from a list. Below, we are asking map_chr to return the first item\n# of each sub list in latestData. However, this will fail if it \n# meets an entry that does not have aything stored under latestdata!\n# stations[[1]] |&gt; \n#   map_df(~as_tibble(.)) |&gt; \n#   mutate(latestData = map_chr(latestData, 1))\n\n\n# We could write a custom \"unlisting\"-function.\n# The function below unlists the elements of latestData - if there are \n# any elements there. If it the content is null, the function just\n# returns an empty character string. \nunlist_safe &lt;- \n  function(x){\n    x &lt;- unlist(x)\n    if(is.null(x)){\n      return(NA_character_)\n  }else{\n    return(x)\n  }\n}\n\nstations[[1]] |&gt; \n  map_df(~as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, unlist_safe))\n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# Alternatively, we can use the defaults in map_chr. It will \n# now have a safe fallback value it can use if it doesn't \n# find the element we are looking for in latestData. \n# A simple solution is to use the .default-argument, and set this to missing: \nstations[[1]] |&gt; \n    map_df(~as_tibble(.)) |&gt; \n    mutate(latestData = map_chr(latestData,1, .default=NA_character_)) \n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\n\n\n\n\n\n# Next, let's format the date format. Date formats can be tricky, but is\n# an obstacle you just have to learn to work with. We can reformat the \n# latestData column into a date by simply using as.Date - however - \n# we now have lost information on the time of day. Let's see if we \n# can retain all the information in the column. \nstations[[1]] |&gt; \n  map_df(~as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default=NA_character_)) |&gt; \n  mutate(latestData = as.Date(latestData))\n\n# A tibble: 7,895 × 4\n   id            name            latestData location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;date&gt;     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# There are several functions we can use to transform the string into\n# a date time variable. as_datetime in lubridate works in this case. \n# Note that the interpretation of dates may be dependent on the time zone\n# settings on your laptop. Here, we are explicitly stating that we want the\n# a Europe/Berlin tz on the variable: \nstations[[1]] |&gt; \n  map_df( ~ as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) \n\n# A tibble: 7,895 × 4\n   id            name            latestData          location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\n\nExercise: Finalizing the transformation\nLet’s take on the final location variable. Complete the operation by unpacking the location column into two columns: lat and lon. You may use the functions you have already seen, or see of you can find mode specialized functions.\nNote: This a nested list i.e. the contents of a cell in “location” is a list with one entry. This list contains two other lists..\nThe script should return a data frame similar to the one below (only the first few entries shown).\n\n\n\n\n\nid\nname\nlatestData\nlat\nlon\n\n\n\n\n52742V2282262\nØRBEKK EV6\n2024-08-23\n60.41426\n11.241171\n\n\n41517V704478\nBASTERUD\n2017-11-14\n60.76916\n11.171156\n\n\n01050V1126115\nTrollvika\n2024-08-23\n69.23921\n17.981692\n\n\n24748V22148\nPinesund\n2024-08-23\n58.79845\n9.054728\n\n\n11446V1175840\nMelsomvik\n2024-08-23\n59.22947\n10.338938\n\n\n99015V249528\nHjelset øst\n2023-05-09\n62.78733\n7.521145\n\n\n\n\n\n# A tibble: 7,895 × 6\n   id            name            latestData          location       lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list&gt;  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list&gt;  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list&gt;  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list&gt;  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list&gt;  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list&gt;  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list&gt;  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list&gt;  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list&gt;  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list&gt;  59.6 10.7 \n# ℹ 7,885 more rows\n\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n\n\n\n\nSolution\n\n\n## We can use a similar solution we used before. First we use\n## unlist-safe to remove one level from the list, and then extract \n## the contents using map_dbl - remember these are numbers, not text. \n\nstations[[1]] |&gt; \n  map_dfr(as_tibble) |&gt;  \n  mutate(latestData = map_chr(latestData, 1, .default = \"\"))  |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  mutate(location = map(location, unlist)) |&gt;  \n  mutate(\n    lat = map_dbl(location, \"latLon.lat\"),\n    lon = map_dbl(location, \"latLon.lon\")\n  ) %&gt;% \n  select(-location)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n## Alternatively, we can use unnest_wider twice. This one does some work\n## for us, and gives the same result: \n\nstations[[1]] |&gt; \n  map_df( ~ as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  unnest_wider(location) |&gt; \n  unnest_wider(latLon)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows"
  },
  {
    "objectID": "07-iterations-wip.html#getting-some-data",
    "href": "07-iterations-wip.html#getting-some-data",
    "title": "7 purrr on Traffic data",
    "section": "",
    "text": "library(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(DescTools)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rlang)\nlibrary(lubridate)\nlibrary(anytime)\n\n# Today we are going to work with some more advanced topics in \n# terms of data handling and processing. We will play with an API\n# from Vegvesenet. Vegvesenet has an API we can query for data\n# on traffic volumes at many sensor stations in Norway. \n# \n# The API uses graphQL for requests. This is a relatively\n# new language we might see more of in the future?\n# \n# Let's define a function where we can submit queries to an external API. \nGQL &lt;- function(query,\n                ...,\n                .token = NULL,\n                .variables = NULL,\n                .operationName = NULL,\n                .url = url) {\n  pbody &lt;-\n    list(query = query,\n         variables = .variables,\n         operationName = .operationName)\n  if (is.null(.token)) {\n    res &lt;- POST(.url, body = pbody, encode = \"json\", ...)\n  } else {\n    auth_header &lt;- paste(\"bearer\", .token)\n    res &lt;-\n      POST(\n        .url,\n        body = pbody,\n        encode = \"json\",\n        add_headers(Authorization = auth_header),\n        ...\n      )\n  }\n  res &lt;- content(res, as = \"parsed\", encoding = \"UTF-8\")\n  if (!is.null(res$errors)) {\n    warning(toJSON(res$errors))\n  }\n  res$data\n}\n\n\n# The URL we will use is stored below: \nurl &lt;- \"https://www.vegvesen.no/trafikkdata/api/\"\n\n\n# Let's figure out which sensor stations that are operable. \n# The query below extracts all the stations, with a date for \n# when the station was in operation as well as a long/latitude. \nqry &lt;-\n  '\n{\n    trafficRegistrationPoints {\n        id\n        name\n        latestData {\n            volumeByDay\n        }\n        location {\n            coordinates {\n                latLon {\n                    lat\n                    lon\n                }\n            }\n        }\n    }\n}\n'\n\n# Allright - let's try submitting the query: \nstations &lt;-GQL(qry) \n\n\n# We now have the a long list in memory - 11mb! - with just \n# a little information on each station. We can note that this \n# is a list, not a dataframe. For our purposes, it would be better if\n# the list was instead a data frame, with one row pr. sensor station. \n\n\n# Note that the list only has one entry..   \nlength(stations)\n\n[1] 1\n\n# However, the list contains another list, called trafficRegistrationPoints. \n# This list has almost 5000 entries. We can select this sublist using \n# either $ or [[1]]. Note that when we subset a list, using [[i]] selects\n# the contents of the item [[i]]. \nlength(stations$trafficRegistrationPoints)\n\n[1] 7895\n\nlength(stations[[1]])\n\n[1] 7895\n\n# Let's look at the first entry of this long list. We can see there is\n# a station ID, station name, date time of latest recording from the station\n# and coordinates. This looks like something that could fit well within \n# a data frame, with columns id, name, latestdata, lat, and lon. The \n# question is how!\nstations[[1]][[1]]\n\n$id\n[1] \"52742V2282262\"\n\n$name\n[1] \"ØRBEKK EV6\"\n\n$latestData\n$latestData$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n$location\n$location$coordinates\n$location$coordinates$latLon\n$location$coordinates$latLon$lat\n[1] 60.41426\n\n$location$coordinates$latLon$lon\n[1] 11.24117\n\n# We could perhaps hope that we can force this list into a data frame. For\n# this we will use as_tibble: \nstations[[1]][[1]] |&gt;  \n  as_tibble()\n\n# A tibble: 1 × 4\n  id            name       latestData   location        \n  &lt;chr&gt;         &lt;chr&gt;      &lt;named list&gt; &lt;named list&gt;    \n1 52742V2282262 ØRBEKK EV6 &lt;chr [1]&gt;    &lt;named list [1]&gt;\n\n# We now want to apply this as_tibble transformation to each of the stations, \n# and combine them in a single data frame. We could do this with lapply, \n# and then bind toghether the rows: \nlapply(stations[[1]], . %&gt;% as_tibble) |&gt; \n  bind_rows()\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nExercise:\nTransform the list into a data frame, with at id and name as columns, and one row per station. We can fix the date time and locations columns later, but use one of the map-functions from purrr.\n\n\nSolution\n\n\n# Using the map_df-function we traverse all the entries in the stations list, \n# and transform these lists to data frames. \n# \n# There is still some work left to do with the date time and location \n# columns. As you can see below, they are still in a list format. \nstations[[1]] |&gt; \n  map_df(~as_tibble(.))\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# We can try to pull out the insides of the contents of the latestData-\n# column. It is formatted as a list, but actually only contains one \n# date time entry. \nstations[[1]] |&gt;  \n  map_df(~as_tibble(.)) |&gt; \n  head(1) |&gt;  \n  select(latestData) |&gt;  \n  pull()\n\n$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n\nExercise:\nMutate the contents of the latestData-columns, such that it is in a character format. You don’t have to format it to a proper date time (yet..)\n\n\nSolution\n\n\n# There are two complications to this one.. \n# 1:  Similarly to the previous task, we want to apply a transformation\n#     to all entries of a list.. \n# 2:  However at least one of the entries does not contain the list item\n#     \"latestData\". \n#     \n# As you can see from the documentation, the map-functions are \n# very flexible, and we can e.g. use them to extract named items\n# from a list. Below, we are asking map_chr to return the first item\n# of each sub list in latestData. However, this will fail if it \n# meets an entry that does not have aything stored under latestdata!\n# stations[[1]] |&gt; \n#   map_df(~as_tibble(.)) |&gt; \n#   mutate(latestData = map_chr(latestData, 1))\n\n\n# We could write a custom \"unlisting\"-function.\n# The function below unlists the elements of latestData - if there are \n# any elements there. If it the content is null, the function just\n# returns an empty character string. \nunlist_safe &lt;- \n  function(x){\n    x &lt;- unlist(x)\n    if(is.null(x)){\n      return(NA_character_)\n  }else{\n    return(x)\n  }\n}\n\nstations[[1]] |&gt; \n  map_df(~as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, unlist_safe))\n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# Alternatively, we can use the defaults in map_chr. It will \n# now have a safe fallback value it can use if it doesn't \n# find the element we are looking for in latestData. \n# A simple solution is to use the .default-argument, and set this to missing: \nstations[[1]] |&gt; \n    map_df(~as_tibble(.)) |&gt; \n    mutate(latestData = map_chr(latestData,1, .default=NA_character_)) \n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows"
  },
  {
    "objectID": "07-iterations-wip.html#transforming-time",
    "href": "07-iterations-wip.html#transforming-time",
    "title": "7 purrr on Traffic data",
    "section": "",
    "text": "# Next, let's format the date format. Date formats can be tricky, but is\n# an obstacle you just have to learn to work with. We can reformat the \n# latestData column into a date by simply using as.Date - however - \n# we now have lost information on the time of day. Let's see if we \n# can retain all the information in the column. \nstations[[1]] |&gt; \n  map_df(~as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default=NA_character_)) |&gt; \n  mutate(latestData = as.Date(latestData))\n\n# A tibble: 7,895 × 4\n   id            name            latestData location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;date&gt;     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n# There are several functions we can use to transform the string into\n# a date time variable. as_datetime in lubridate works in this case. \n# Note that the interpretation of dates may be dependent on the time zone\n# settings on your laptop. Here, we are explicitly stating that we want the\n# a Europe/Berlin tz on the variable: \nstations[[1]] |&gt; \n  map_df( ~ as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) \n\n# A tibble: 7,895 × 4\n   id            name            latestData          location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\n\nExercise: Finalizing the transformation\nLet’s take on the final location variable. Complete the operation by unpacking the location column into two columns: lat and lon. You may use the functions you have already seen, or see of you can find mode specialized functions.\nNote: This a nested list i.e. the contents of a cell in “location” is a list with one entry. This list contains two other lists..\nThe script should return a data frame similar to the one below (only the first few entries shown).\n\n\n\n\n\nid\nname\nlatestData\nlat\nlon\n\n\n\n\n52742V2282262\nØRBEKK EV6\n2024-08-23\n60.41426\n11.241171\n\n\n41517V704478\nBASTERUD\n2017-11-14\n60.76916\n11.171156\n\n\n01050V1126115\nTrollvika\n2024-08-23\n69.23921\n17.981692\n\n\n24748V22148\nPinesund\n2024-08-23\n58.79845\n9.054728\n\n\n11446V1175840\nMelsomvik\n2024-08-23\n59.22947\n10.338938\n\n\n99015V249528\nHjelset øst\n2023-05-09\n62.78733\n7.521145\n\n\n\n\n\n# A tibble: 7,895 × 6\n   id            name            latestData          location       lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list&gt;  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list&gt;  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list&gt;  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list&gt;  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list&gt;  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list&gt;  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list&gt;  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list&gt;  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list&gt;  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list&gt;  59.6 10.7 \n# ℹ 7,885 more rows\n\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n\n\n\n\nSolution\n\n\n## We can use a similar solution we used before. First we use\n## unlist-safe to remove one level from the list, and then extract \n## the contents using map_dbl - remember these are numbers, not text. \n\nstations[[1]] |&gt; \n  map_dfr(as_tibble) |&gt;  \n  mutate(latestData = map_chr(latestData, 1, .default = \"\"))  |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  mutate(location = map(location, unlist)) |&gt;  \n  mutate(\n    lat = map_dbl(location, \"latLon.lat\"),\n    lon = map_dbl(location, \"latLon.lon\")\n  ) %&gt;% \n  select(-location)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n## Alternatively, we can use unnest_wider twice. This one does some work\n## for us, and gives the same result: \n\nstations[[1]] |&gt; \n  map_df( ~ as_tibble(.)) |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  unnest_wider(location) |&gt; \n  unnest_wider(latLon)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows"
  },
  {
    "objectID": "06-git.html",
    "href": "06-git.html",
    "title": "6 Git and Github",
    "section": "",
    "text": "We mentioned the need for version control in our previous module on project management. This is a central topic for any kind of software development. In this lesson we will learn the most popular system for version control, git, and see how we can use it for managing the progress of a project. This simplest way to think of this workflow is that we introduce a kind of “undo”-button for the entire project, by saving the state of our various files onto a timeline that we can navigate as we wish. But version control is so much more: we can use git to work on different branches of ideas, that we can merge into the project in a controlled way, we can formalize collaboration via online repositories, and we will see how open source development works via forks and pull requests.\nYou will use git and GitHub to complete the assignments in the course, so it is important that you learn this material now!\nPlease note that we will just briefly review the contents of the video lessons below in the beginning of our lecture. This means that you should work with the video lessons before we start the session at 10:15. We will use the remaining time for further discussion of central topics, including Github Classroom, which is the system that we will use to deliver the assignments for the rest of the semester. Maybe we will have time for a secret, but very fun activity in the end…\nHere are some slides that we will use for discussion in class: ban400-git-github.pdf.\nWe will follow this schedule in our lecture slot:\n\n\n\n\n\n\n\n08:15 - 10:00\nComplete watching the videos if you have not finished by this date. The instructor is available in the auditorium for discussion and support if you have any questions or errors that you would like to resolve.\n\n\n10:15 - 11:00\nBrief review of the material. Discussion of workflow in data science projects.\n\n\n11:15 - 12:00\nWe introduce two important concepts for code contribution to open source projects (the fork and the pull request). We will also introduce Github Classroom which we will use to administer the assignments in the course.\n\n\n\nOur brief treatment of Git and Github is by no means complete. Below follow a few extra sources for further reading:\n\nPro Git is a standard reference on the subject.\nHappy Git with R gives an R-specific introduction to Git, with special consideration to the possibility of integrating the basic features if Git directly into RStudio.\nGithub Ultimate is a (paid) course at Udemy.com by Jason Taylor, that has served as inspiration for several of the videos for this seminar.\nThis lecture at MIT provides a basic introduction to the inner workings of Git as well as many of the commands that we cover in our seminar.\n\n\n\nIn the video player below we set up git and learn the basic commands. Note that we have simplified the setup process from earlier years (specifically, we no longer use a third party tool for conflict resolution), which means that some of these videos are somewhat amputated.\n\n\n\n\n\n\n\nWe introduce some more advanced concepts for version control on your local computer:\n\n\n\n\n\n\n\nFinally, we move our repository online using GitHub and see how we can use online version control to complement our workflow with tools for collaboration and project management.",
    "crumbs": [
      "Home",
      "PART 2",
      "6. Git and Github"
    ]
  },
  {
    "objectID": "06-git.html#setup-and-basic-commands",
    "href": "06-git.html#setup-and-basic-commands",
    "title": "6 Git and Github",
    "section": "",
    "text": "In the video player below we set up git and learn the basic commands. Note that we have simplified the setup process from earlier years (specifically, we no longer use a third party tool for conflict resolution), which means that some of these videos are somewhat amputated.",
    "crumbs": [
      "Home",
      "PART 2",
      "6. Git and Github"
    ]
  },
  {
    "objectID": "06-git.html#branching-and-conflict-resolution",
    "href": "06-git.html#branching-and-conflict-resolution",
    "title": "6 Git and Github",
    "section": "",
    "text": "We introduce some more advanced concepts for version control on your local computer:",
    "crumbs": [
      "Home",
      "PART 2",
      "6. Git and Github"
    ]
  },
  {
    "objectID": "06-git.html#online-repositories-on-github",
    "href": "06-git.html#online-repositories-on-github",
    "title": "6 Git and Github",
    "section": "",
    "text": "Finally, we move our repository online using GitHub and see how we can use online version control to complement our workflow with tools for collaboration and project management.",
    "crumbs": [
      "Home",
      "PART 2",
      "6. Git and Github"
    ]
  },
  {
    "objectID": "01-intro-to-r.html",
    "href": "01-intro-to-r.html",
    "title": "1 Introduction to R",
    "section": "",
    "text": "Welcome to the first taste of BAN400. We will start by downloading and installing the tools that we need to start coding, and then we will explore some of the most basic aspects of the R programming language. After most of the videos we have included a small problem that we encourage you to try before you move on to the next topic.\nBefore we start, you need to install two items on your computer. Please do the installations in the following order:\n\nThe R Programming Language: Navigate to cran.uib.no and download the version of R that corresponds to your operating system. Run the installation as you would for any other program that you install on your system.\nRStudio: Navigate to posit.co/download/rstudio-desktop/, and download the version of “RStudio Desktop” that corresponds to your operating system. Run the installation as you would for any other program that you install on your computer.\n\nBoth R and RStudio are free to download and free to use.\n\n\n\n\nIn this video we open up RStudio for the first time and take a small tour of the user interface.\n\n\n\n\n\nWe move on to write our first R commands. It is critical that you already now start to feel the programming, and you do that best by typing in the code lines just as in the video above (no copy/paste!), making sure that you get the same results.\n\n# We can use R as a calculator:\n2+2\n\n# Pretty simple! We must use paratheses if we have more complicated expressions:\n(2+8)/2\n2+8/2\n\n# Variables are important in R. We can save just about anything inside the\n# computer memory by giving them names:\na &lt;- 5\na\na*4 \n\nb &lt;- 3\n\n# R performs all operations on the right hand side before assigning the value to c:\nc &lt;- a+b\nc\n\n# No errors, warnings or questions when overwriting!\nc &lt;- 4\n\nc &lt;- c + 2\nc\n\n# Let's make an error!\nd\n\n# We can name things more or less what we want. Not a non-trivial problem in\n# large projects!\nwhatever_we_want &lt;- \"hello world\"\nwhatever_we_want\n\nExercise:\nPick your favorite three integers and store them in three different variables. Calculate yor magic number, which is the sum of these three integers. Store your magic number in a new variable. Give your new variable a name that clearly identifies what it is.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nnumber1 &lt;- 1\nnumber2 &lt;- 87\nnumber3 &lt;- 101\n\nmagic_number &lt;- number1 + number2 + number3\n\n\n\n\n\n\n\n\n\nVectors are very important in R. We remember perhaps from our math classes that vectors may represent points in space; in R it is a way to store more than one number (or string, or some other data type) under a single variable name. When doing statistics, this may for example be a set of observations.\nIn this video we first create a vector of numbers using the c()-function, and then we look at various ways to extract/pick out the elements: to subset. Python coders will notice two important distinctions from what they are used to:\n\nIn R we start counting on 1, and not 0!\nWhen trying to subset using an index that out of the range of the vector, we do not get an error message, we just get back the empty value NA.\n\nFurthermore, we use the Up-arrow to get back the last command that we have executed in the console. You can even tap the up-arrow again in order go further back in your command history (and of course use the down-arrow to navigate the other way).\n\n# We can make a vector in the following way:\nvector1 &lt;- c(3, 5, 7.8, 10, 2, 0.16, -3)\n\n# Print out\nvector1\n\n# Subsetting (The first item has index 1!)\nvector1[1]        # Square brackets to subset\nvector1[10]       # Out-of-range error\nvector1[2:5]      # Subset a sequence\nvector1[c(1,3)]   # Subset using another vector!\n\n# The letter \"c\" stands for \"combine\". R makes it very easy to work with\n# vectors:\nvector1 - 1\nvector1*3\n\n# We can use *functions* to calculate various things:\nlength(vector1)\nmean(vector1)\nsum(vector1)\nsd(vector1)\n\n# We can make a vector of strings as well:\nvector2 &lt;- c(\"hello\", \"world\")\n\n# A vector can only contain one data type!\n\n# Perhaps we need the standard deviation later?\nsd_vector1 &lt;- sd(vector1)\nsd_vector1\n\nExercise:\nCalculate the maximum and minimum values of vector1, as well as the median. (Hint, and this will be the most important lesson you will learn in this course: If you do not know the name of the function, Google it!)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# Relevant Google searches: \"minimum value r\", \"maximum r\", \"median r\"\n\nmin(vector1)\nmax(vector1)\nmedian(vector1)\n\n\n\n\n\n\n\n\n\nIn this video we install our first package in R. There are two major takeaways from this:\n\nWe install the package on our computer using the install.packages()-function. We only have to do this once per computer.\nIf we are going to use some of the functions in a package we need to load it using the library()-command. You have to do that every time you restart R (and we will later see that we will typically load all the packages we need in the beginning of the scripts that we write).\n\n\n# In order to install the package readxl, we run the following command. \n# We run this command only once.\ninstall.packages(\"readxl\")\n\n# When we are going to use it, we load it using the \"library()\"-function, \n# and we need to repreat this every time we restart R.\nlibrary(readxl)\n\nExercise: Install the following packages. We will make use of them (and several others) later in the course: ggplot2, dplyr, tidyr and lubridate.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")\n\n\n\n\n\n\n\n\n\nWe introduce the concept of a working directory, which is the folder where R looks for files that we are going to read into the memory, and where R puts the files that we create, for instance image files of plots.\nThere are two central functions:\n\ngetwd() prints out the current working directory.\nsetwd(\"C:/path/to/folder\") sets the working directory to the specified folder. We will as a general rule not use setwd() in our scripts (the reason for that will become clear later), but rather use RStudio’s menu system for changing the working directory (we will in practice not need to do that as a general rule as well, which will also become clear in a short while).\n\nWe may however have to deal with file paths in our code, and make the following technical notes:\n\nOn UNIX systems (such as Mac or Linux) the file paths look differently, they do not start with a drive letter such as C:\\.\nOn Windows, we always use the backslash / to separate between the folders in R code, and not the usual forward slash \\. This may be counter-intuitive to some, but in programming the forward slash usually has special meaning (the escape character) and must not be used for anything else. On UNIX systems we also use the backslash, but that is the system standard for writing file paths, so it does not require any special attention to users of those operating systems.\n\nExercise: Make sure that you have completed the following tasks before proceding to the next lesson:\n\nYou have created a dedicated folder on your computer where you will collected all material that we will use today.\nYou have downloaded the file testdata.xls and put in in your newly created folder.\nYou have changed your working directory to this folder.\nYou have positively confirmed that your working directory now is correctly set.\n\n\n\n\n\n\nWe read our first small data file into the memory of R and apply some simple operations to it. We will spend much more time working with data in R in later lessons.\n\n# The data is in the .xls-format, so we need the readxl-package in order to load \n# it into R.\nlibrary(readxl)\n\n# Inside this package, there is a function called read_excel:\nread_excel(\"testdata.xls\")\n\n# That's fine, but in order to use this data, we need to save it in a variable\ntestdata &lt;- read_excel(\"testdata.xls\")\n\n# Print out the (top of the) data set.\ntestdata\n\n# Now we see the data in the environment. We can look at it by typing the name that we\n# gave it. We can also pick out individual columns using the $-sign:\ntestdata$X1\n\n# Calaculate the mean for X1 and X2:\nmean(testdata$X1)\nmean(testdata$X2)\n\n# How many rows/observations do we have?\nnrow(testdata)\n\nExercise:\n\nHow many columns does our data set have?\nCan you find a way to print out a vector that contains the sum of the X1 and X2 columns in testdata?\nWhat is the total sum of all the numbers in the X1 and X2 columns of testdata?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nncol(testdata)\n\n# 2 \ntestdata$X1 + testdata$X2\n\n# 3\nsum(testdata$X1 + testdata$X2)\n\n\n\n\n\n\n\n\n\nWe introduce the main package for the plotting engine that we will use in this course; ggplot2, and its basic syntax.\n\n# A simple scatterplot\nplot(testdata$X1, testdata$X2)\n\n# Making adjustments to the plot\nplot(testdata$X1, testdata$X2,\n     pch = 20,\n     bty = \"l\",\n     xlab = \"X1\",\n     ylab = \"X2\")\n\n# Load the ggplot2-package\nlibrary(ggplot2)\n\n# Here is the code for creating a simple scatterplot of the X1 and X2 columns in\n# our data set:\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\n\n# First make the plot, then save it to a file\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\")\n\n# A more flexible way to do it is to save the plot in a variable, and then\n# supply the name of the plot to the ggsave-finction:\np &lt;- ggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\", p)\n\n# That way, we can save the plot p at any time, we do not have to do it directly\n# after the plotting commands.\n\nExercise: Can you figure out how to make the dots in the plot bigger and blue?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nggplot(testdata, aes(x = X1, y = X2)) + geom_point(colour = \"blue\", size = 5)\n\n\n\n\n\n\n\n\n\nIn this video we stop writing code directly in the console, and rather write our code in a script file, which is simply a pure text file containing commands. There are two important new concepts that we have to pay attention to when writing scripts:\n\nThe comment character #: Evertything after this character in an R-script is ignored when executing the script. We can use the comment character to add small comments to our code, briefly explaining what is going on. This is a great help for other people trying to understand what you have done, in particular, and perhaps most importantly: the future you returning to a project. The comment character # is the same as in Python.\nThe keybinding Ctrl - Enter (Cmd - Enter on a Mac) executes the line where your cursor is located in the script. You can also select several lines and execute all of them using this shortcut.\n\n\n# Introduction to R\n# -------------------\n\n# Load packages \nlibrary(readxl)\nlibrary(ggplot2)\n\n# Read our data set\ntestdata &lt;- read_xls(\"testdata.xls\")\n\n# Make a scatterplot of the X1 and X2-variables\nplot &lt;- ggplot(testdata, aes(x = X1, y = X2)) + \n    geom_point() + \n    ggtitle(\"Scatterplot of testdata\") +\n    theme_classic()\nggsave(\"testplot.pdf\", plot)\n\nExercise: Make sure to save your script as an .R-file in the folder that we created for this session. Close RStudio. Then navigate to the folder and double click on the script file. Hopefully RStudio opens (if not, right click, select “Open in” and then Rstudio, confirm if prompted to set RStudio as default program for opening .R-files).\nFind out what your working directory is now. What just happened? How is this useful?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nOpening RStudio by double clicking on the script file automatically sets the working directory to the location of the script file. Very useful when returning to a project.\n\n\n\n\n\n\nSince version 4.1.0, R has included the pipe operator |&gt;. A pipe operator allows you to write code that is closer to how we read English. To see how it works, consider the code below, that applies three functions to a vector:\n\n# Create some data:\nx &lt;- (-500):500\n\n# Want to calculate mean of sqrt of abs values of x...\n# one way: via temporary variables: \nabs.x &lt;- abs(x)  \nsqrt.abs.x &lt;- sqrt(abs.x)\nalt.1 &lt;- mean(sqrt.abs.x)\n\nprint(alt.1)\n\n[1] 14.91415\n\n\nAssuming we only care about the final result alt.1, the script above creates two unnecessary variables in memory (abs.x and sqrt.abs.x), and clutters up the environment.\nWe could instead nest the three function calls:\n\nalt.2 &lt;- mean(sqrt(abs(x)))\nprint(alt.2)\n\n[1] 14.91415\n\n\nNesting the function calls doesn’t store the intermediary calculations, so we don’t clutter up the environment. However, reading what is happening on this line is more challenging than it needs to be: you have to read from right to left. It the functions calls has more than one argument, you would also need to keep track of which of the right-parentheses belongs to which function.\nThis is when the pipe operator comes to the rescue. The pipe “passes” whatever is on the left hand side to the right hand side of the expression. As a simple example, the sqrt(2) can be written in a pipe as:\n\n2 |&gt; sqrt()\n\n[1] 1.414214\n\n\nThe script above can be read as “The number 2, then the square root”. Pipes can also be used with multiple arguments. The two statements below gives the same answer:\n\nmean(c(1,2,NA), na.rm=T)\n\n[1] 1.5\n\nc(1,2,NA) |&gt; mean(na.rm=T)\n\n[1] 1.5\n\n\nBy default, the pipe operator inserts the value of the left hand side as the first argument in the function call on the right hand side, so e.g. x |&gt; mean(na.rm=TRUE) is equivalent to mean(x, na.rm=TRUE). If you want the value to be inserted as another argument, an underscore _ may be used as a placeholder for the left hand side value. However, then the argument names must be named:\n\natan2(x=1, y=2)\n\n[1] 1.107149\n\n2 |&gt; atan2(x=1, y=_)\n\n[1] 1.107149\n\n\nThe Tidyverse-package magrittr also contains a pipe operator %&gt;%. The Tidyverse-pipe behaves similarly to the |&gt; in simple cases, however, it uses a period . as placeholder. The Tidyverse-pipe generally is more mature and has more features than the base-pipe |&gt; (see here for a more thorough comparison of the two).\nIn this course we’ll generally use the Tidyverse-pipe. The shortcut for the pipe in RStudio is Ctrl - Shift - M on Windows, and Cmd - Shift - M on a Mac.\nWe will use this technique extensively throughout the course!\n\nlibrary(magrittr)\n2 %&gt;% . ^ 2\n\n[1] 4\n\n2 %&gt;% atan2(1, .)\n\n[1] 0.4636476\n\n\nExercise:\nUse magrittr to calculate the equivalent of alt.1 and alt.2\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nalt.3 &lt;-\n  x %&gt;% \n  abs %&gt;%\n  sqrt %&gt;%\n  mean\n\nalt.1\nalt.2\nalt.3\n\nAlternatively, with the base-pipe. Note |&gt; is more picky than %&gt;% with parentheses after function calls.\n\nx |&gt;\n  abs() |&gt;\n  sqrt() |&gt;\n  mean() \n\n[1] 14.91415",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#tour-of-rstudio",
    "href": "01-intro-to-r.html#tour-of-rstudio",
    "title": "1 Introduction to R",
    "section": "",
    "text": "In this video we open up RStudio for the first time and take a small tour of the user interface.",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#calculations-and-variables",
    "href": "01-intro-to-r.html#calculations-and-variables",
    "title": "1 Introduction to R",
    "section": "",
    "text": "We move on to write our first R commands. It is critical that you already now start to feel the programming, and you do that best by typing in the code lines just as in the video above (no copy/paste!), making sure that you get the same results.\n\n# We can use R as a calculator:\n2+2\n\n# Pretty simple! We must use paratheses if we have more complicated expressions:\n(2+8)/2\n2+8/2\n\n# Variables are important in R. We can save just about anything inside the\n# computer memory by giving them names:\na &lt;- 5\na\na*4 \n\nb &lt;- 3\n\n# R performs all operations on the right hand side before assigning the value to c:\nc &lt;- a+b\nc\n\n# No errors, warnings or questions when overwriting!\nc &lt;- 4\n\nc &lt;- c + 2\nc\n\n# Let's make an error!\nd\n\n# We can name things more or less what we want. Not a non-trivial problem in\n# large projects!\nwhatever_we_want &lt;- \"hello world\"\nwhatever_we_want\n\nExercise:\nPick your favorite three integers and store them in three different variables. Calculate yor magic number, which is the sum of these three integers. Store your magic number in a new variable. Give your new variable a name that clearly identifies what it is.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nnumber1 &lt;- 1\nnumber2 &lt;- 87\nnumber3 &lt;- 101\n\nmagic_number &lt;- number1 + number2 + number3",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#vectors",
    "href": "01-intro-to-r.html#vectors",
    "title": "1 Introduction to R",
    "section": "",
    "text": "Vectors are very important in R. We remember perhaps from our math classes that vectors may represent points in space; in R it is a way to store more than one number (or string, or some other data type) under a single variable name. When doing statistics, this may for example be a set of observations.\nIn this video we first create a vector of numbers using the c()-function, and then we look at various ways to extract/pick out the elements: to subset. Python coders will notice two important distinctions from what they are used to:\n\nIn R we start counting on 1, and not 0!\nWhen trying to subset using an index that out of the range of the vector, we do not get an error message, we just get back the empty value NA.\n\nFurthermore, we use the Up-arrow to get back the last command that we have executed in the console. You can even tap the up-arrow again in order go further back in your command history (and of course use the down-arrow to navigate the other way).\n\n# We can make a vector in the following way:\nvector1 &lt;- c(3, 5, 7.8, 10, 2, 0.16, -3)\n\n# Print out\nvector1\n\n# Subsetting (The first item has index 1!)\nvector1[1]        # Square brackets to subset\nvector1[10]       # Out-of-range error\nvector1[2:5]      # Subset a sequence\nvector1[c(1,3)]   # Subset using another vector!\n\n# The letter \"c\" stands for \"combine\". R makes it very easy to work with\n# vectors:\nvector1 - 1\nvector1*3\n\n# We can use *functions* to calculate various things:\nlength(vector1)\nmean(vector1)\nsum(vector1)\nsd(vector1)\n\n# We can make a vector of strings as well:\nvector2 &lt;- c(\"hello\", \"world\")\n\n# A vector can only contain one data type!\n\n# Perhaps we need the standard deviation later?\nsd_vector1 &lt;- sd(vector1)\nsd_vector1\n\nExercise:\nCalculate the maximum and minimum values of vector1, as well as the median. (Hint, and this will be the most important lesson you will learn in this course: If you do not know the name of the function, Google it!)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# Relevant Google searches: \"minimum value r\", \"maximum r\", \"median r\"\n\nmin(vector1)\nmax(vector1)\nmedian(vector1)",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#packages",
    "href": "01-intro-to-r.html#packages",
    "title": "1 Introduction to R",
    "section": "",
    "text": "In this video we install our first package in R. There are two major takeaways from this:\n\nWe install the package on our computer using the install.packages()-function. We only have to do this once per computer.\nIf we are going to use some of the functions in a package we need to load it using the library()-command. You have to do that every time you restart R (and we will later see that we will typically load all the packages we need in the beginning of the scripts that we write).\n\n\n# In order to install the package readxl, we run the following command. \n# We run this command only once.\ninstall.packages(\"readxl\")\n\n# When we are going to use it, we load it using the \"library()\"-function, \n# and we need to repreat this every time we restart R.\nlibrary(readxl)\n\nExercise: Install the following packages. We will make use of them (and several others) later in the course: ggplot2, dplyr, tidyr and lubridate.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ninstall.packages(\"ggplot2\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"lubridate\")",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#working-directory",
    "href": "01-intro-to-r.html#working-directory",
    "title": "1 Introduction to R",
    "section": "",
    "text": "We introduce the concept of a working directory, which is the folder where R looks for files that we are going to read into the memory, and where R puts the files that we create, for instance image files of plots.\nThere are two central functions:\n\ngetwd() prints out the current working directory.\nsetwd(\"C:/path/to/folder\") sets the working directory to the specified folder. We will as a general rule not use setwd() in our scripts (the reason for that will become clear later), but rather use RStudio’s menu system for changing the working directory (we will in practice not need to do that as a general rule as well, which will also become clear in a short while).\n\nWe may however have to deal with file paths in our code, and make the following technical notes:\n\nOn UNIX systems (such as Mac or Linux) the file paths look differently, they do not start with a drive letter such as C:\\.\nOn Windows, we always use the backslash / to separate between the folders in R code, and not the usual forward slash \\. This may be counter-intuitive to some, but in programming the forward slash usually has special meaning (the escape character) and must not be used for anything else. On UNIX systems we also use the backslash, but that is the system standard for writing file paths, so it does not require any special attention to users of those operating systems.\n\nExercise: Make sure that you have completed the following tasks before proceding to the next lesson:\n\nYou have created a dedicated folder on your computer where you will collected all material that we will use today.\nYou have downloaded the file testdata.xls and put in in your newly created folder.\nYou have changed your working directory to this folder.\nYou have positively confirmed that your working directory now is correctly set.",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#reading-data",
    "href": "01-intro-to-r.html#reading-data",
    "title": "1 Introduction to R",
    "section": "",
    "text": "We read our first small data file into the memory of R and apply some simple operations to it. We will spend much more time working with data in R in later lessons.\n\n# The data is in the .xls-format, so we need the readxl-package in order to load \n# it into R.\nlibrary(readxl)\n\n# Inside this package, there is a function called read_excel:\nread_excel(\"testdata.xls\")\n\n# That's fine, but in order to use this data, we need to save it in a variable\ntestdata &lt;- read_excel(\"testdata.xls\")\n\n# Print out the (top of the) data set.\ntestdata\n\n# Now we see the data in the environment. We can look at it by typing the name that we\n# gave it. We can also pick out individual columns using the $-sign:\ntestdata$X1\n\n# Calaculate the mean for X1 and X2:\nmean(testdata$X1)\nmean(testdata$X2)\n\n# How many rows/observations do we have?\nnrow(testdata)\n\nExercise:\n\nHow many columns does our data set have?\nCan you find a way to print out a vector that contains the sum of the X1 and X2 columns in testdata?\nWhat is the total sum of all the numbers in the X1 and X2 columns of testdata?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nncol(testdata)\n\n# 2 \ntestdata$X1 + testdata$X2\n\n# 3\nsum(testdata$X1 + testdata$X2)",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#plotting",
    "href": "01-intro-to-r.html#plotting",
    "title": "1 Introduction to R",
    "section": "",
    "text": "We introduce the main package for the plotting engine that we will use in this course; ggplot2, and its basic syntax.\n\n# A simple scatterplot\nplot(testdata$X1, testdata$X2)\n\n# Making adjustments to the plot\nplot(testdata$X1, testdata$X2,\n     pch = 20,\n     bty = \"l\",\n     xlab = \"X1\",\n     ylab = \"X2\")\n\n# Load the ggplot2-package\nlibrary(ggplot2)\n\n# Here is the code for creating a simple scatterplot of the X1 and X2 columns in\n# our data set:\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\n\n# First make the plot, then save it to a file\nggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\")\n\n# A more flexible way to do it is to save the plot in a variable, and then\n# supply the name of the plot to the ggsave-finction:\np &lt;- ggplot(testdata, aes(x = X1, y = X2)) + geom_point()\nggsave(\"testplot.pdf\", p)\n\n# That way, we can save the plot p at any time, we do not have to do it directly\n# after the plotting commands.\n\nExercise: Can you figure out how to make the dots in the plot bigger and blue?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nggplot(testdata, aes(x = X1, y = X2)) + geom_point(colour = \"blue\", size = 5)",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#writing-scripts",
    "href": "01-intro-to-r.html#writing-scripts",
    "title": "1 Introduction to R",
    "section": "",
    "text": "In this video we stop writing code directly in the console, and rather write our code in a script file, which is simply a pure text file containing commands. There are two important new concepts that we have to pay attention to when writing scripts:\n\nThe comment character #: Evertything after this character in an R-script is ignored when executing the script. We can use the comment character to add small comments to our code, briefly explaining what is going on. This is a great help for other people trying to understand what you have done, in particular, and perhaps most importantly: the future you returning to a project. The comment character # is the same as in Python.\nThe keybinding Ctrl - Enter (Cmd - Enter on a Mac) executes the line where your cursor is located in the script. You can also select several lines and execute all of them using this shortcut.\n\n\n# Introduction to R\n# -------------------\n\n# Load packages \nlibrary(readxl)\nlibrary(ggplot2)\n\n# Read our data set\ntestdata &lt;- read_xls(\"testdata.xls\")\n\n# Make a scatterplot of the X1 and X2-variables\nplot &lt;- ggplot(testdata, aes(x = X1, y = X2)) + \n    geom_point() + \n    ggtitle(\"Scatterplot of testdata\") +\n    theme_classic()\nggsave(\"testplot.pdf\", plot)\n\nExercise: Make sure to save your script as an .R-file in the folder that we created for this session. Close RStudio. Then navigate to the folder and double click on the script file. Hopefully RStudio opens (if not, right click, select “Open in” and then Rstudio, confirm if prompted to set RStudio as default program for opening .R-files).\nFind out what your working directory is now. What just happened? How is this useful?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nOpening RStudio by double clicking on the script file automatically sets the working directory to the location of the script file. Very useful when returning to a project.",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "01-intro-to-r.html#pipe",
    "href": "01-intro-to-r.html#pipe",
    "title": "1 Introduction to R",
    "section": "",
    "text": "Since version 4.1.0, R has included the pipe operator |&gt;. A pipe operator allows you to write code that is closer to how we read English. To see how it works, consider the code below, that applies three functions to a vector:\n\n# Create some data:\nx &lt;- (-500):500\n\n# Want to calculate mean of sqrt of abs values of x...\n# one way: via temporary variables: \nabs.x &lt;- abs(x)  \nsqrt.abs.x &lt;- sqrt(abs.x)\nalt.1 &lt;- mean(sqrt.abs.x)\n\nprint(alt.1)\n\n[1] 14.91415\n\n\nAssuming we only care about the final result alt.1, the script above creates two unnecessary variables in memory (abs.x and sqrt.abs.x), and clutters up the environment.\nWe could instead nest the three function calls:\n\nalt.2 &lt;- mean(sqrt(abs(x)))\nprint(alt.2)\n\n[1] 14.91415\n\n\nNesting the function calls doesn’t store the intermediary calculations, so we don’t clutter up the environment. However, reading what is happening on this line is more challenging than it needs to be: you have to read from right to left. It the functions calls has more than one argument, you would also need to keep track of which of the right-parentheses belongs to which function.\nThis is when the pipe operator comes to the rescue. The pipe “passes” whatever is on the left hand side to the right hand side of the expression. As a simple example, the sqrt(2) can be written in a pipe as:\n\n2 |&gt; sqrt()\n\n[1] 1.414214\n\n\nThe script above can be read as “The number 2, then the square root”. Pipes can also be used with multiple arguments. The two statements below gives the same answer:\n\nmean(c(1,2,NA), na.rm=T)\n\n[1] 1.5\n\nc(1,2,NA) |&gt; mean(na.rm=T)\n\n[1] 1.5\n\n\nBy default, the pipe operator inserts the value of the left hand side as the first argument in the function call on the right hand side, so e.g. x |&gt; mean(na.rm=TRUE) is equivalent to mean(x, na.rm=TRUE). If you want the value to be inserted as another argument, an underscore _ may be used as a placeholder for the left hand side value. However, then the argument names must be named:\n\natan2(x=1, y=2)\n\n[1] 1.107149\n\n2 |&gt; atan2(x=1, y=_)\n\n[1] 1.107149\n\n\nThe Tidyverse-package magrittr also contains a pipe operator %&gt;%. The Tidyverse-pipe behaves similarly to the |&gt; in simple cases, however, it uses a period . as placeholder. The Tidyverse-pipe generally is more mature and has more features than the base-pipe |&gt; (see here for a more thorough comparison of the two).\nIn this course we’ll generally use the Tidyverse-pipe. The shortcut for the pipe in RStudio is Ctrl - Shift - M on Windows, and Cmd - Shift - M on a Mac.\nWe will use this technique extensively throughout the course!\n\nlibrary(magrittr)\n2 %&gt;% . ^ 2\n\n[1] 4\n\n2 %&gt;% atan2(1, .)\n\n[1] 0.4636476\n\n\nExercise:\nUse magrittr to calculate the equivalent of alt.1 and alt.2\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nalt.3 &lt;-\n  x %&gt;% \n  abs %&gt;%\n  sqrt %&gt;%\n  mean\n\nalt.1\nalt.2\nalt.3\n\nAlternatively, with the base-pipe. Note |&gt; is more picky than %&gt;% with parentheses after function calls.\n\nx |&gt;\n  abs() |&gt;\n  sqrt() |&gt;\n  mean() \n\n[1] 14.91415",
    "crumbs": [
      "Home",
      "PART 1",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "submit_assignments.html",
    "href": "submit_assignments.html",
    "title": "A guide to submitting assignments with Github Classroom",
    "section": "",
    "text": "A guide to submitting assignments with Github Classroom\nIn the second half of the course, we will work with assignments as git repositories, and we will submit our work via Github Classroom. In the following guide we will see the steps needed to access, work with, and hand in the assignments.\n\nStep 1: Go through the basics of Git and Github by watching and following the video lessons.\nStep 2: Sign up to Github. As a student you are eligible for a free Github Pro account. This means, for instance, that you can keep your repositories private. Apply here.\nStep 3: Click on the assignment link when you will start working on it, for example:\n\n\n\n\nClick on the assignment link\n\n\n\nStep 4: Log into your Github account and choose your student number from the list in order to associate your account. The list of student numbers has been generated from the student roster of the course. If you do not find your number on the list, send an e-mail to hakon.otneim@nhh.no in order to fix it!\nStep 5: Press “Accept this assignment”.\nStep 6: After a few moments (you might have to refresh the page), you will see the page below, indicating that a repository has been made for you. This repository will be “owned” by the organization set up by us (“nhh-ban”), and the name of the repository will be {the name of the assignment}-{your user name}.\n\n\n\n\nConfirmation page, click on the link to go to the repository of your assignment\n\n\n\nStep 7: Click on the repository link to navigate to it. It will contain all you need to complete the assignment. The instructions are contained in the file README.md, which is typically displayed on the repository page. Clone the repository to your own machine by using the ssh link under “Code” in the upper right corner.\n\n\n\n\nThe repository and the git clone in a terminal window\n\n\n\nStep 8: You can now work on the repository on your own machine. You can start by updating the readme file with your personal information, commit the changes and push them online. Verify that the online repository has been updated.\n\n\n\n\nWorkflow\n\n\n\nStep 9: Make commits and push to the online repository as you see fit yourself as you are working. When you are done, you simply make a final commit, and a push.\nStep 10: Note that a pull request has been opened in your repository. Do not close the pull request. This will be used by the TAs in order to give feedback to your solution.",
    "crumbs": [
      "Home",
      "Other material",
      "Submit assignments with Github Classroom"
    ]
  },
  {
    "objectID": "05-project-organization.html",
    "href": "05-project-organization.html",
    "title": "5 Project organization",
    "section": "",
    "text": "In this module, we will not do so much R-coding, but rather reflect on how we should organize a coding project. If we spend some time now on identifying efficient, consistent and robust practices that work all the time, we do not have to use our creative energy later on topics such as folder structure and coding style. But this is not just a matter of practical convenience. It is critically important that our work is reproducible. Baumer (2018) writes:\n\nA reproducible workflow is one in which each step of the analytical process is clearly documented in such a way that someone – and here it is better to imagine that person is not you – can retrace your steps and verify the exact results that you presented. Since your workflow necessarily involves computing, that means that your computing workflow needs to be reproducible, and this immediately necessitates scriptable programs, as opposed to point-and-click, menu-driven software. There are many reasons not to use spreadsheet software (e.g. Microsoft Excel [… and here there is a reference to Broman and Woo, 2017, “Data organization in spreadsheets”, for using this kind of software responsibly]), but chief among them is the fact that spreadsheet operations cannot be scripted. This means that it is generally impossible to produce truly reproducible work in that environment. A fundamental problem with spreadsheets is that they fail to distinguish between data and the presentation of those data. In a spreadsheet, everything is fair game: data can be overwritten or reformatted in a way that destroys the original precision, or simply garbled by automatic type conversion tools. Limitations imposed on the number of “Undo” commands further restrict one’s ability to retrace steps.\nIf a program is scriptable, then the precise sequence of commands that load and transform the data, perform the analysis, and produce the plots can be recorded. This need not be a history or transcript of the entire session, but rather should be the minimal set of commands needed to reproduce your analysis. For even the most thoughtful programmer, a complete history will contain many irrelevant or incorrect commands that are not necessarily recorded in a sensible order. What reproducibility demands is a carefully edited recipe.\n\n\n\nHere are the data files that we will look at in this module: project-organization-raw-data.zip. Some of the most central topics in this module are also discussed in Chapters 3, 5, 7 and 9 in R4DS.\n\n\n\nWe discuss the paper A Quick Guide to Organizing Computational Biology Projects (Noble 2009) in order to adopt a consistent folder structure that we can use for our projects.\n\n\n\n\n\nNext, we choose, and stick to, an accepted coding style. There are various options out there (and you can of course make your own informed choice in this matter). In the video, we consider some aspects of the Tidyverse style guide. You can download the R-file that we work with here: process_data.R.\n\n\n\n\n\nRStudio lets us create a handy little “project file” that we can put in the top level folder in our project. At the end of the video, we make some remarks about a packace called here, that we need to walk back on in the next video in order to solve a practical problem there.\n\n\n\n\n\nFinally, we introduce a convenient system for writing technical reports: Quarto. The great advantage of writing with Quarto is that we no longer have to separate the tasks of writing and running code for producing some kind of result, and putting those results into a document. All of this can now be handled in one operation. Quarto is the successor of RMarkdown, which in turn is based on the well known Markdown format for writing. You can download Quarto here: Quarto.org, and you will find the reference guide on the same page.\nAgain, you will notice that these videos were not planned out entirely before recording them, because we see the immediate use of the here -package for placing the document in its correct place in our folder structure. You can download the finished state of the project, including the quarto-file and the updated script, here: newspaper-analysis.zip.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "05-project-organization.html#introduction",
    "href": "05-project-organization.html#introduction",
    "title": "5 Project organization",
    "section": "",
    "text": "In this module, we will not do so much R-coding, but rather reflect on how we should organize a coding project. If we spend some time now on identifying efficient, consistent and robust practices that work all the time, we do not have to use our creative energy later on topics such as folder structure and coding style. But this is not just a matter of practical convenience. It is critically important that our work is reproducible. Baumer (2018) writes:\n\nA reproducible workflow is one in which each step of the analytical process is clearly documented in such a way that someone – and here it is better to imagine that person is not you – can retrace your steps and verify the exact results that you presented. Since your workflow necessarily involves computing, that means that your computing workflow needs to be reproducible, and this immediately necessitates scriptable programs, as opposed to point-and-click, menu-driven software. There are many reasons not to use spreadsheet software (e.g. Microsoft Excel [… and here there is a reference to Broman and Woo, 2017, “Data organization in spreadsheets”, for using this kind of software responsibly]), but chief among them is the fact that spreadsheet operations cannot be scripted. This means that it is generally impossible to produce truly reproducible work in that environment. A fundamental problem with spreadsheets is that they fail to distinguish between data and the presentation of those data. In a spreadsheet, everything is fair game: data can be overwritten or reformatted in a way that destroys the original precision, or simply garbled by automatic type conversion tools. Limitations imposed on the number of “Undo” commands further restrict one’s ability to retrace steps.\nIf a program is scriptable, then the precise sequence of commands that load and transform the data, perform the analysis, and produce the plots can be recorded. This need not be a history or transcript of the entire session, but rather should be the minimal set of commands needed to reproduce your analysis. For even the most thoughtful programmer, a complete history will contain many irrelevant or incorrect commands that are not necessarily recorded in a sensible order. What reproducibility demands is a carefully edited recipe.\n\n\n\nHere are the data files that we will look at in this module: project-organization-raw-data.zip. Some of the most central topics in this module are also discussed in Chapters 3, 5, 7 and 9 in R4DS.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "05-project-organization.html#folder-structure",
    "href": "05-project-organization.html#folder-structure",
    "title": "5 Project organization",
    "section": "",
    "text": "We discuss the paper A Quick Guide to Organizing Computational Biology Projects (Noble 2009) in order to adopt a consistent folder structure that we can use for our projects.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "05-project-organization.html#coding-practice",
    "href": "05-project-organization.html#coding-practice",
    "title": "5 Project organization",
    "section": "",
    "text": "Next, we choose, and stick to, an accepted coding style. There are various options out there (and you can of course make your own informed choice in this matter). In the video, we consider some aspects of the Tidyverse style guide. You can download the R-file that we work with here: process_data.R.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "05-project-organization.html#rstudio-projects",
    "href": "05-project-organization.html#rstudio-projects",
    "title": "5 Project organization",
    "section": "",
    "text": "RStudio lets us create a handy little “project file” that we can put in the top level folder in our project. At the end of the video, we make some remarks about a packace called here, that we need to walk back on in the next video in order to solve a practical problem there.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "05-project-organization.html#writing-with-quarto",
    "href": "05-project-organization.html#writing-with-quarto",
    "title": "5 Project organization",
    "section": "",
    "text": "Finally, we introduce a convenient system for writing technical reports: Quarto. The great advantage of writing with Quarto is that we no longer have to separate the tasks of writing and running code for producing some kind of result, and putting those results into a document. All of this can now be handled in one operation. Quarto is the successor of RMarkdown, which in turn is based on the well known Markdown format for writing. You can download Quarto here: Quarto.org, and you will find the reference guide on the same page.\nAgain, you will notice that these videos were not planned out entirely before recording them, because we see the immediate use of the here -package for placing the document in its correct place in our folder structure. You can download the finished state of the project, including the quarto-file and the updated script, here: newspaper-analysis.zip.",
    "crumbs": [
      "Home",
      "PART 1",
      "5. Project organization"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This web page is written using Quarto and published via Github. The source code is loated here: github.com/hotneim/ban400. Please direct any feedback to the course instructors:\nHåkon Otneim: hakon.otneim@nhh.no\nOle-Petter Moe Hansen: ole-petter.hansen@nhh.no\nThanks to Sondre Hølleland for some wonderful CSS-files. Check out his course at holleland.github.io/BAN430."
  },
  {
    "objectID": "09-parallel-computing.html",
    "href": "09-parallel-computing.html",
    "title": "9 Parallel computing",
    "section": "",
    "text": "Sometimes we come across problems that are computationally hard. The point of this lecture is to provide you with some techniques for speeding up your scripts. This will however just be an cursory introduction to speeding up programs. The focus here is fairly simple techniques that might be enough for many, but certainly not all, use cases.\n\n\nTo showcase the need for speed we need a time consuming script. We’ll use assignment 3 from the home exam in Ban400 Fall 2021. In this case, we studied a fictitious Tesla car dealership, where customers could pay for cars using Bitcoin (BTC). Customers would lock in the price (in BTC) when signing a contract, but would only pay when the car arrived a few days later. The dealership pays a fixed amount in Norwegian Kroner for each car. Given that the BTC-exchange rate is volatile, this scheme exposes the card dealership to currency risk, where the risk is increasing with a longer delay between contract agreements and payments (lead_time).\nAssuming the car dealership:\n\nHas an initial equity of 10\nSells one car every day\nRetains all accumulated profits\n\n…will the dealership go bankrupt?\nThe question should be answered separately for different lead times, and with different potential start-dates for the dealership.\nYou can find the data set with NOK/BTC exhange rates here: parallel_data.Rdata\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"parallel_data.Rdata\")\n\nsales_price_mnok &lt;- 1\ncar_cost_mnok &lt;- .95\n\ndf %&gt;%\n  head() %&gt;% \n  knitr::kable()\n\n\n\n\ndate\nNOKBTC\n\n\n\n\n2017-09-15\n22407.12\n\n\n2017-09-16\n25244.89\n\n\n2017-09-17\n25244.89\n\n\n2017-09-18\n25244.89\n\n\n2017-09-19\n24413.31\n\n\n2017-09-20\n24138.77\n\n\n\n\n\nThe solution proposal presented two functions to help solve the problem. calcProfits calculates the daily profits in the data frame, given a data frame, sales price, car cost and lead days. test_neg_equity tests if a specific combination of lead_days and startdates would ever lead to a negative equity.\n\ncalcProfits &lt;-\n  function(df,\n           sales_price_mnok,\n           car_cost_mnok,\n           lead_days) {\n    df %&gt;%\n      mutate(\n        sales_price_BTC = sales_price_mnok / NOKBTC,\n        mNOK_val_sales =\n          lead(NOKBTC, lead_days, order_by = date)\n        * sales_price_BTC,\n        profit_mnok = mNOK_val_sales - car_cost_mnok\n      )\n  }\n\n\ninitial_equity &lt;- 10\n\ntest_neg_equity &lt;-\n  function(df, startdate, lead_days) {\n    tmpdf &lt;-\n      df %&gt;%\n      filter(date &gt;= startdate) %&gt;%\n      calcProfits(sales_price_mnok, car_cost_mnok, lead_days) %&gt;%\n      filter(complete.cases(.))\n    \n    if (nrow(tmpdf) &gt; 0) {\n      tmpdf %&gt;%\n        mutate(cumulative_profits_mnok = cumsum(profit_mnok)) %&gt;%\n        summarise(negative_equity =\n                    1 * (min(\n                      cumulative_profits_mnok + initial_equity\n                    ) &lt; 0)) %&gt;%\n        pull %&gt;%\n        return\n    } else{\n      return(NA_real_)\n    }\n  }\n\nBefore we run the function, lets use some functionality for storing how long time it takes to complete the calculation. Let’s use the tictoc-library:\n\nlibrary(tictoc)\n\n# We can use tictoc to time a function..:\ntic()\nSys.sleep(1)\ntoc()\n\n1.008 sec elapsed\n\n\nTictoc can also store to logs, so we can make comparisons across experiments. However, the log is stored in an awkward format, so let’s make a function for printing out results in a data frame:\n\nprintTicTocLog &lt;-\n  function() {\n    tic.log() %&gt;%\n      unlist %&gt;%\n      tibble(logvals = .) %&gt;%\n      separate(logvals,\n               sep = \":\",\n               into = c(\"Function type\", \"log\")) %&gt;%\n      mutate(log = str_trim(log)) %&gt;%\n      separate(log,\n               sep = \" \",\n               into = c(\"Seconds\"),\n               extra = \"drop\")\n  }\n\ntic.clearlog()\n\ntic(\"Test\")\nSys.sleep(1)\ntoc(log = TRUE)\n\nTest: 1.008 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nTest\n1.008\n\n\n\n\n\nFinally, we make a result data frame with three columns: date, lead_days and neg_eq, where we want to fill in the values of neg_eq. With the combinations below we need to apply the test_neg_equity-function 7305 times. On my machine this takes around a minute. If we wanted to test all the lead days from 1 to 60 we would get almost 90000 function calls, and I would expect the calculations would take 12 times longer.\n\ndf_res &lt;-\n  expand.grid(date = df$date,\n              lead_days = c(1, 5, 10, 30, 60)) %&gt;%\n  mutate(neg_eq = NA) %&gt;%\n  as_tibble()\n\n\ntic.clearlog()\ntic(\"Regular loop\")\n\nfor (i in 1:nrow(df_res)) {\n  df_res$neg_eq[i] &lt;-\n    test_neg_equity(df,\n                    startdate = df_res$date[i],\n                    lead_days = df_res$lead_days[i])\n}\n\ntoc(log = TRUE)\n\nRegular loop: 10.978 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\n\n\n\nWaiting a minute is not very long, but is tedious if we need to do this computation often. And sometimes there are problems that simply takes a long time to compute.\n\n\n\nThere are several methods we can apply to speed up a program.\n\nWe can find better ways of solving the problem. Sometimes we can reformulate the problem, use analytical results such that we can get the same answer with less effort.\nUse a profiler to understand where the code is spending time, and figure out which parts we should try to make faster. See e.g. https://rstudio.github.io/profvis/\n…or: maybe we can use more hardware to solve the problem faster.\n\nIn the following, we will try to speed up the program by using more hardware.\n\n\n\nYour computer has a central processing unit - also known as a CPU or a simply a processor. This is the unit that does the calculations on your laptop. Modern CPU’s consist of several cores, where each of the cores can do computations in parallell with the others. Below is a picture of the activity on my CPU. Not much happening, but 12 so called “logical cores”. In reality, this CPU only has six cores, but through a technology called hyperthreading each core can process two sets of instructions at the same time. This doubles the amount to six. If you have a Windows machine, you can see your processor activity by pressing CTRL+SHIFT+ESC (Mac users see here). Your computer may have a different number of cores. \n\n\n\nProcessor use example\n\n\n\n\n\nSometimes, a problem might have to be solve in sequence. Logically, that is how our scripts work: the program starts at the beginning and work its’ way to the end. However, sometimes parts of our code may be such that we can break it up into pieces that can be solved independently of each other. An example of this is the problem below. Here we’re looping through the rows of df_res, and the calculation done at one step doesn’t have any dependence on any of the other steps. If we can break the program up in such parts, we call the problem embarrasingly parallel.\nAn example of a calculation that is at least not immediately embarrassingly parallel is a cumulative sum. Here, the ith value of the cumulative sum depends on the i-1 value.\n\nfor(i in 1:nrow(df_res)) {\n  df_res$neg_eq[i] &lt;-\n    test_neg_equity(df,\n                    startdate = df_res$date[i],\n                    lead_days = df_res$lead_days[i])\n}\n\nIf we have multiple cores available, we might get a speedup if we can put all the cores to work at the same time.\n\n\n\nThere are many libraries available to write code in parallel. One good example is the doParallel-package.\n\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# The function detectCores finds the number of cores\n# available on the machine. We update the \"Cores\"-value\n# to the minimum of the chosen cores and the available cores.\nmaxcores &lt;- 8\nCores &lt;- min(parallel::detectCores(), maxcores)\n\n# Instantiate the cores:\ncl &lt;- makeCluster(Cores)\n\n# Next we register the cluster..\nregisterDoParallel(cl)\n\n# Take the time as before:\ntic(paste0(\"Parallel loop, \", Cores, \" cores\"))\nres &lt;-\n  foreach(\n    i = 1:nrow(df_res),\n    .combine = 'rbind',\n    .packages = c('magrittr', 'dplyr')\n  ) %dopar%\n  tibble(\n    date = df_res$date[i],\n    lead_days = df_res$lead_days[i],\n    neg_eq =\n      test_neg_equity(\n        df,\n        startdate = df_res$date[i],\n        lead_days = df_res$lead_days[i]\n      )\n  )\n\n# Now that we're done, we close off the clusters\nstopCluster(cl)\n\ntoc(log = TRUE)\n\nParallel loop, 8 cores: 2.961 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\n\n\n\nFrom the times above, you can note that we got a nice speedup from a parallel run, and that the compute time was cut to 75%. That isn’t so bad, for the price of only a few more lines of code - but depends of course also on if you have other tasks that might slow down as you run this script.\n\n\n\nIn a previous lecture, we discussed using purrr. Below, it is shown how we can execute the loop using purrr - here with an iteration over the date and lead dates as lists.\n\nlibrary(purrr)\n\ntic(\"purrr\")\ndf_res$neg_eq &lt;-\n  df %&gt;%\n  map2_dbl(as.list(df_res$date),\n           as.list(df_res$lead_days),\n           test_neg_equity,\n           df = .)\n\ntoc(log = TRUE)\n\npurrr: 10.747 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\npurrr\n10.747\n\n\n\n\n\n\n\n\nfurrr is a library that has parallell-versions of purrr-functions. This packages is not very developed, but it allows you to simply switch from map_* to future_map_* (in addition to adding the plan(...-line), and thereby executing your program in parallel:\n\nlibrary(furrr)\n\nLoading required package: future\n\nplan(multisession, workers = Cores)\n\ntic(paste0(\"furrr, \", Cores, \" cores\"))\n\ndf_res$neg_eq &lt;-\n  df %&gt;%\n  future_map2_dbl(as.list(df_res$date),\n                  as.list(df_res$lead_days),\n                  test_neg_equity,\n                  df = .)\n\ntoc(log = TRUE)\n\nfurrr, 8 cores: 2.714 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\npurrr\n10.747\n\n\nfurrr, 8 cores\n2.714\n\n\n\n\n\n\n\n\nGiven that my stationary computer has 12 logical cores, we might wonder if we can make this go even faster. We can experiment with that, and try the furrr-functionality with different number of cores. You may again need to adjust the number of max-cores to suit your machine:\n\nmaxcores &lt;- 12\n\nfor (Cores in 1:maxcores) {\n  plan(multisession, workers = Cores)\n  \n  tic(paste0(\"furrr, \", Cores, \" cores\"))\n  \n  df_res$neg_eq &lt;-\n    df %&gt;%\n    future_map2_dbl(as.list(df_res$date),\n                    as.list(df_res$lead_days),\n                    test_neg_equity,\n                    df = .)\n  \n  toc(log = TRUE)\n}\n\nfurrr, 1 cores: 10.855 sec elapsed\nfurrr, 2 cores: 5.913 sec elapsed\nfurrr, 3 cores: 4.354 sec elapsed\nfurrr, 4 cores: 3.521 sec elapsed\nfurrr, 5 cores: 3.182 sec elapsed\nfurrr, 6 cores: 2.838 sec elapsed\nfurrr, 7 cores: 2.763 sec elapsed\nfurrr, 8 cores: 2.622 sec elapsed\nfurrr, 9 cores: 2.689 sec elapsed\nfurrr, 10 cores: 2.687 sec elapsed\nfurrr, 11 cores: 2.769 sec elapsed\nfurrr, 12 cores: 2.901 sec elapsed\n\nprintTicTocLog() %&gt;%\n  tail(maxcores) %&gt;%\n  separate(\n    `Function type`,\n    sep = \" \",\n    into = c(\"Function type\", \"nCores\"),\n    extra = \"drop\"\n  ) %&gt;%\n  mutate(\n    Seconds = as.numeric(Seconds),\n    nCores = as.numeric(nCores),\n    lowered_compute_time = Seconds / lag(Seconds, order_by = nCores) - 1,\n    theoretical_max = lag(nCores) / nCores - 1\n  ) %&gt;%\n  ggplot(aes(x = nCores)) +\n  geom_line(aes(y = lowered_compute_time, col = \"Realized performance gain\")) +\n  geom_line(aes(y = theoretical_max, col = \"Theoretical performance gain\")) +\n  theme_classic() +\n  xlab(\"Number of cores\") +\n  ylab(\"Lowered compute time by additional core\") +\n  theme(legend.title = element_blank(),\n        legend.position = 'bottom')\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nIf we have N cores to do an amount of work, we should expect that doing the same amount of work with N+1 cores could reduce the compute time by up to N/(N+1) (e.g. if you have one core, and try out two cores instead, the compute time should be cut in half (e.g. 1/2)). As we see from the case, the actual performance gain starts out close to the theoretical limit, but as we add cores the performance gain is smaller than the N/(N+1). Reasons for decreasing performance gain are:\n\nThere is some overhead involved with dealing with multiple workers.\nYour computer needs to do other things than running your R-scripts (such as keeping the operating system up and running!). This means that if you get close to using 100% of your computer’s CPU-power, performance will decrease.\nThere might be other resource constraints, such as how much memory your computer has, and how much memory each worker demands.\n\nIn short, making things go fast is a very interesting rabbit hole you may (or may not) want to dig into. However - CPU-parallelization is a very useful tool.\n\n\n\nThere are other topics that we cannot cover in Ban400, however, they may be of interest to some students:\n\n\nSome calculations, particularly operations on matrices, can be done on a graphical processing unit (GPU). If you have a good GPU - and not all laptops do - there can be tremendous speed gains by moving some calculations to a GPU.\nIn addition to higher hardware requirements, there is often also more work that needs to be done to install software and ensure drivers and packages work together. I would generally not recommend to start testing your GPU unless you also use Docker. We will not cover how to set this up in Ban400.\nThere are packages in R that allow you to run your own mathematical operations on GPU’s. Also, many of the libraries for machine learning models (e.g. TensorFlow and Keras) let you train the models on your GPU. Some large machine learning models (e.g. large language models) may just be unfeasible to train unless you use one or more GPU.\nIf you want to work with models that require GPU’s, you could consider investing in a stationary computer with the appropriate hardware. Another option is to use cloud services.\n\n\n\nOne way of making computations faster - without needing to invest in a GPU - is to write functions in a faster language. C++ is very fast, and there is a package Rcpp that deals with integration of C++-functions and R.\nThe price to pay for using Rcpp is that you need to write functions in C++. However, that might not be as hard as it seems, as you would often only write some functions in C++ that you use often.\nSee here for some nice slides on the speed gains and use of Rcpp by the author of the package.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#a-time-consuming-problem",
    "href": "09-parallel-computing.html#a-time-consuming-problem",
    "title": "9 Parallel computing",
    "section": "",
    "text": "To showcase the need for speed we need a time consuming script. We’ll use assignment 3 from the home exam in Ban400 Fall 2021. In this case, we studied a fictitious Tesla car dealership, where customers could pay for cars using Bitcoin (BTC). Customers would lock in the price (in BTC) when signing a contract, but would only pay when the car arrived a few days later. The dealership pays a fixed amount in Norwegian Kroner for each car. Given that the BTC-exchange rate is volatile, this scheme exposes the card dealership to currency risk, where the risk is increasing with a longer delay between contract agreements and payments (lead_time).\nAssuming the car dealership:\n\nHas an initial equity of 10\nSells one car every day\nRetains all accumulated profits\n\n…will the dealership go bankrupt?\nThe question should be answered separately for different lead times, and with different potential start-dates for the dealership.\nYou can find the data set with NOK/BTC exhange rates here: parallel_data.Rdata\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"parallel_data.Rdata\")\n\nsales_price_mnok &lt;- 1\ncar_cost_mnok &lt;- .95\n\ndf %&gt;%\n  head() %&gt;% \n  knitr::kable()\n\n\n\n\ndate\nNOKBTC\n\n\n\n\n2017-09-15\n22407.12\n\n\n2017-09-16\n25244.89\n\n\n2017-09-17\n25244.89\n\n\n2017-09-18\n25244.89\n\n\n2017-09-19\n24413.31\n\n\n2017-09-20\n24138.77\n\n\n\n\n\nThe solution proposal presented two functions to help solve the problem. calcProfits calculates the daily profits in the data frame, given a data frame, sales price, car cost and lead days. test_neg_equity tests if a specific combination of lead_days and startdates would ever lead to a negative equity.\n\ncalcProfits &lt;-\n  function(df,\n           sales_price_mnok,\n           car_cost_mnok,\n           lead_days) {\n    df %&gt;%\n      mutate(\n        sales_price_BTC = sales_price_mnok / NOKBTC,\n        mNOK_val_sales =\n          lead(NOKBTC, lead_days, order_by = date)\n        * sales_price_BTC,\n        profit_mnok = mNOK_val_sales - car_cost_mnok\n      )\n  }\n\n\ninitial_equity &lt;- 10\n\ntest_neg_equity &lt;-\n  function(df, startdate, lead_days) {\n    tmpdf &lt;-\n      df %&gt;%\n      filter(date &gt;= startdate) %&gt;%\n      calcProfits(sales_price_mnok, car_cost_mnok, lead_days) %&gt;%\n      filter(complete.cases(.))\n    \n    if (nrow(tmpdf) &gt; 0) {\n      tmpdf %&gt;%\n        mutate(cumulative_profits_mnok = cumsum(profit_mnok)) %&gt;%\n        summarise(negative_equity =\n                    1 * (min(\n                      cumulative_profits_mnok + initial_equity\n                    ) &lt; 0)) %&gt;%\n        pull %&gt;%\n        return\n    } else{\n      return(NA_real_)\n    }\n  }\n\nBefore we run the function, lets use some functionality for storing how long time it takes to complete the calculation. Let’s use the tictoc-library:\n\nlibrary(tictoc)\n\n# We can use tictoc to time a function..:\ntic()\nSys.sleep(1)\ntoc()\n\n1.008 sec elapsed\n\n\nTictoc can also store to logs, so we can make comparisons across experiments. However, the log is stored in an awkward format, so let’s make a function for printing out results in a data frame:\n\nprintTicTocLog &lt;-\n  function() {\n    tic.log() %&gt;%\n      unlist %&gt;%\n      tibble(logvals = .) %&gt;%\n      separate(logvals,\n               sep = \":\",\n               into = c(\"Function type\", \"log\")) %&gt;%\n      mutate(log = str_trim(log)) %&gt;%\n      separate(log,\n               sep = \" \",\n               into = c(\"Seconds\"),\n               extra = \"drop\")\n  }\n\ntic.clearlog()\n\ntic(\"Test\")\nSys.sleep(1)\ntoc(log = TRUE)\n\nTest: 1.008 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nTest\n1.008\n\n\n\n\n\nFinally, we make a result data frame with three columns: date, lead_days and neg_eq, where we want to fill in the values of neg_eq. With the combinations below we need to apply the test_neg_equity-function 7305 times. On my machine this takes around a minute. If we wanted to test all the lead days from 1 to 60 we would get almost 90000 function calls, and I would expect the calculations would take 12 times longer.\n\ndf_res &lt;-\n  expand.grid(date = df$date,\n              lead_days = c(1, 5, 10, 30, 60)) %&gt;%\n  mutate(neg_eq = NA) %&gt;%\n  as_tibble()\n\n\ntic.clearlog()\ntic(\"Regular loop\")\n\nfor (i in 1:nrow(df_res)) {\n  df_res$neg_eq[i] &lt;-\n    test_neg_equity(df,\n                    startdate = df_res$date[i],\n                    lead_days = df_res$lead_days[i])\n}\n\ntoc(log = TRUE)\n\nRegular loop: 10.978 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\n\n\n\nWaiting a minute is not very long, but is tedious if we need to do this computation often. And sometimes there are problems that simply takes a long time to compute.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#speeding-up-a-program",
    "href": "09-parallel-computing.html#speeding-up-a-program",
    "title": "9 Parallel computing",
    "section": "",
    "text": "There are several methods we can apply to speed up a program.\n\nWe can find better ways of solving the problem. Sometimes we can reformulate the problem, use analytical results such that we can get the same answer with less effort.\nUse a profiler to understand where the code is spending time, and figure out which parts we should try to make faster. See e.g. https://rstudio.github.io/profvis/\n…or: maybe we can use more hardware to solve the problem faster.\n\nIn the following, we will try to speed up the program by using more hardware.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#resources-on-your-computer",
    "href": "09-parallel-computing.html#resources-on-your-computer",
    "title": "9 Parallel computing",
    "section": "",
    "text": "Your computer has a central processing unit - also known as a CPU or a simply a processor. This is the unit that does the calculations on your laptop. Modern CPU’s consist of several cores, where each of the cores can do computations in parallell with the others. Below is a picture of the activity on my CPU. Not much happening, but 12 so called “logical cores”. In reality, this CPU only has six cores, but through a technology called hyperthreading each core can process two sets of instructions at the same time. This doubles the amount to six. If you have a Windows machine, you can see your processor activity by pressing CTRL+SHIFT+ESC (Mac users see here). Your computer may have a different number of cores. \n\n\n\nProcessor use example",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#embarrassingly-parallel-problems",
    "href": "09-parallel-computing.html#embarrassingly-parallel-problems",
    "title": "9 Parallel computing",
    "section": "",
    "text": "Sometimes, a problem might have to be solve in sequence. Logically, that is how our scripts work: the program starts at the beginning and work its’ way to the end. However, sometimes parts of our code may be such that we can break it up into pieces that can be solved independently of each other. An example of this is the problem below. Here we’re looping through the rows of df_res, and the calculation done at one step doesn’t have any dependence on any of the other steps. If we can break the program up in such parts, we call the problem embarrasingly parallel.\nAn example of a calculation that is at least not immediately embarrassingly parallel is a cumulative sum. Here, the ith value of the cumulative sum depends on the i-1 value.\n\nfor(i in 1:nrow(df_res)) {\n  df_res$neg_eq[i] &lt;-\n    test_neg_equity(df,\n                    startdate = df_res$date[i],\n                    lead_days = df_res$lead_days[i])\n}\n\nIf we have multiple cores available, we might get a speedup if we can put all the cores to work at the same time.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#using-multiple-cores-in-r",
    "href": "09-parallel-computing.html#using-multiple-cores-in-r",
    "title": "9 Parallel computing",
    "section": "",
    "text": "There are many libraries available to write code in parallel. One good example is the doParallel-package.\n\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# The function detectCores finds the number of cores\n# available on the machine. We update the \"Cores\"-value\n# to the minimum of the chosen cores and the available cores.\nmaxcores &lt;- 8\nCores &lt;- min(parallel::detectCores(), maxcores)\n\n# Instantiate the cores:\ncl &lt;- makeCluster(Cores)\n\n# Next we register the cluster..\nregisterDoParallel(cl)\n\n# Take the time as before:\ntic(paste0(\"Parallel loop, \", Cores, \" cores\"))\nres &lt;-\n  foreach(\n    i = 1:nrow(df_res),\n    .combine = 'rbind',\n    .packages = c('magrittr', 'dplyr')\n  ) %dopar%\n  tibble(\n    date = df_res$date[i],\n    lead_days = df_res$lead_days[i],\n    neg_eq =\n      test_neg_equity(\n        df,\n        startdate = df_res$date[i],\n        lead_days = df_res$lead_days[i]\n      )\n  )\n\n# Now that we're done, we close off the clusters\nstopCluster(cl)\n\ntoc(log = TRUE)\n\nParallel loop, 8 cores: 2.961 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\n\n\n\nFrom the times above, you can note that we got a nice speedup from a parallel run, and that the compute time was cut to 75%. That isn’t so bad, for the price of only a few more lines of code - but depends of course also on if you have other tasks that might slow down as you run this script.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#using-purrr",
    "href": "09-parallel-computing.html#using-purrr",
    "title": "9 Parallel computing",
    "section": "",
    "text": "In a previous lecture, we discussed using purrr. Below, it is shown how we can execute the loop using purrr - here with an iteration over the date and lead dates as lists.\n\nlibrary(purrr)\n\ntic(\"purrr\")\ndf_res$neg_eq &lt;-\n  df %&gt;%\n  map2_dbl(as.list(df_res$date),\n           as.list(df_res$lead_days),\n           test_neg_equity,\n           df = .)\n\ntoc(log = TRUE)\n\npurrr: 10.747 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\npurrr\n10.747",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#furrr-purrr-in-parallel",
    "href": "09-parallel-computing.html#furrr-purrr-in-parallel",
    "title": "9 Parallel computing",
    "section": "",
    "text": "furrr is a library that has parallell-versions of purrr-functions. This packages is not very developed, but it allows you to simply switch from map_* to future_map_* (in addition to adding the plan(...-line), and thereby executing your program in parallel:\n\nlibrary(furrr)\n\nLoading required package: future\n\nplan(multisession, workers = Cores)\n\ntic(paste0(\"furrr, \", Cores, \" cores\"))\n\ndf_res$neg_eq &lt;-\n  df %&gt;%\n  future_map2_dbl(as.list(df_res$date),\n                  as.list(df_res$lead_days),\n                  test_neg_equity,\n                  df = .)\n\ntoc(log = TRUE)\n\nfurrr, 8 cores: 2.714 sec elapsed\n\nprintTicTocLog() %&gt;%\n  knitr::kable()\n\n\n\n\nFunction type\nSeconds\n\n\n\n\nRegular loop\n10.978\n\n\nParallel loop, 8 cores\n2.961\n\n\npurrr\n10.747\n\n\nfurrr, 8 cores\n2.714",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#how-fast-can-we-go",
    "href": "09-parallel-computing.html#how-fast-can-we-go",
    "title": "9 Parallel computing",
    "section": "",
    "text": "Given that my stationary computer has 12 logical cores, we might wonder if we can make this go even faster. We can experiment with that, and try the furrr-functionality with different number of cores. You may again need to adjust the number of max-cores to suit your machine:\n\nmaxcores &lt;- 12\n\nfor (Cores in 1:maxcores) {\n  plan(multisession, workers = Cores)\n  \n  tic(paste0(\"furrr, \", Cores, \" cores\"))\n  \n  df_res$neg_eq &lt;-\n    df %&gt;%\n    future_map2_dbl(as.list(df_res$date),\n                    as.list(df_res$lead_days),\n                    test_neg_equity,\n                    df = .)\n  \n  toc(log = TRUE)\n}\n\nfurrr, 1 cores: 10.855 sec elapsed\nfurrr, 2 cores: 5.913 sec elapsed\nfurrr, 3 cores: 4.354 sec elapsed\nfurrr, 4 cores: 3.521 sec elapsed\nfurrr, 5 cores: 3.182 sec elapsed\nfurrr, 6 cores: 2.838 sec elapsed\nfurrr, 7 cores: 2.763 sec elapsed\nfurrr, 8 cores: 2.622 sec elapsed\nfurrr, 9 cores: 2.689 sec elapsed\nfurrr, 10 cores: 2.687 sec elapsed\nfurrr, 11 cores: 2.769 sec elapsed\nfurrr, 12 cores: 2.901 sec elapsed\n\nprintTicTocLog() %&gt;%\n  tail(maxcores) %&gt;%\n  separate(\n    `Function type`,\n    sep = \" \",\n    into = c(\"Function type\", \"nCores\"),\n    extra = \"drop\"\n  ) %&gt;%\n  mutate(\n    Seconds = as.numeric(Seconds),\n    nCores = as.numeric(nCores),\n    lowered_compute_time = Seconds / lag(Seconds, order_by = nCores) - 1,\n    theoretical_max = lag(nCores) / nCores - 1\n  ) %&gt;%\n  ggplot(aes(x = nCores)) +\n  geom_line(aes(y = lowered_compute_time, col = \"Realized performance gain\")) +\n  geom_line(aes(y = theoretical_max, col = \"Theoretical performance gain\")) +\n  theme_classic() +\n  xlab(\"Number of cores\") +\n  ylab(\"Lowered compute time by additional core\") +\n  theme(legend.title = element_blank(),\n        legend.position = 'bottom')\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nIf we have N cores to do an amount of work, we should expect that doing the same amount of work with N+1 cores could reduce the compute time by up to N/(N+1) (e.g. if you have one core, and try out two cores instead, the compute time should be cut in half (e.g. 1/2)). As we see from the case, the actual performance gain starts out close to the theoretical limit, but as we add cores the performance gain is smaller than the N/(N+1). Reasons for decreasing performance gain are:\n\nThere is some overhead involved with dealing with multiple workers.\nYour computer needs to do other things than running your R-scripts (such as keeping the operating system up and running!). This means that if you get close to using 100% of your computer’s CPU-power, performance will decrease.\nThere might be other resource constraints, such as how much memory your computer has, and how much memory each worker demands.\n\nIn short, making things go fast is a very interesting rabbit hole you may (or may not) want to dig into. However - CPU-parallelization is a very useful tool.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "09-parallel-computing.html#other-ways-of-making-things-go-fast",
    "href": "09-parallel-computing.html#other-ways-of-making-things-go-fast",
    "title": "9 Parallel computing",
    "section": "",
    "text": "There are other topics that we cannot cover in Ban400, however, they may be of interest to some students:\n\n\nSome calculations, particularly operations on matrices, can be done on a graphical processing unit (GPU). If you have a good GPU - and not all laptops do - there can be tremendous speed gains by moving some calculations to a GPU.\nIn addition to higher hardware requirements, there is often also more work that needs to be done to install software and ensure drivers and packages work together. I would generally not recommend to start testing your GPU unless you also use Docker. We will not cover how to set this up in Ban400.\nThere are packages in R that allow you to run your own mathematical operations on GPU’s. Also, many of the libraries for machine learning models (e.g. TensorFlow and Keras) let you train the models on your GPU. Some large machine learning models (e.g. large language models) may just be unfeasible to train unless you use one or more GPU.\nIf you want to work with models that require GPU’s, you could consider investing in a stationary computer with the appropriate hardware. Another option is to use cloud services.\n\n\n\nOne way of making computations faster - without needing to invest in a GPU - is to write functions in a faster language. C++ is very fast, and there is a package Rcpp that deals with integration of C++-functions and R.\nThe price to pay for using Rcpp is that you need to write functions in C++. However, that might not be as hard as it seems, as you would often only write some functions in C++ that you use often.\nSee here for some nice slides on the speed gains and use of Rcpp by the author of the package.",
    "crumbs": [
      "Home",
      "PART 2",
      "9. Parallel computing"
    ]
  },
  {
    "objectID": "08-machine-learning.html",
    "href": "08-machine-learning.html",
    "title": "8 Machine learning",
    "section": "",
    "text": "8 Machine learning\nThis course mainly concerns the programming aspect of data science. In other words, we deal with a lot of the infrastructure that is necessary to perform data science in practice. In this session, however, we will briefly visit a core topic: namely the statistical modelling that is used to produce predictions in data science. This is often labelled “machine learning” or “statistical learning” (and sometimes more vaguely as AI and other fancy terms…). We will only give a superficial treatment in this lecture. The topic is covered in detail in BAN404 - Predictive Analytics with R, as well as BAN430 - Forecasting (see also the compendium for that course, which provided the design for this web page), both of which are due to run in the spring.\nIn the video below, we give a summary of the lecture part of this session.\n\n\n\n\n\nIn the physical lecture, we will discuss the material above in more detail, as well as the code for estimating the models and making the plots in the slides. You will find the slides as well as the code (ml-script.R) in the following git-repository: github.com/hotneim/ban400-lectures.\nIn the second half of the lecture we will do a further coding workshop on a different data set, and using a different class of models. You will find the code for that workshop in ml-workshop.R, located in the same repository.\nThe assignment for this material is based on the code provided for this session.",
    "crumbs": [
      "Home",
      "PART 2",
      "8. Machine learning"
    ]
  },
  {
    "objectID": "assignment-02.html",
    "href": "assignment-02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Note\n\n\n\nThis assignment is to be handed in as an .R-file through Canvas. Your answer will be reviewed by a teaching assistant as well as two fellow students. Do take the time to condsider the feedback. You will also receive two random answers from fellow students for review. Try to find one positive aspect as well as one suggestion for improvement in each of the answers. You must complete both peer reviews in order to pass the assignment.\n\n\nThe first part of the assignment is intended to give you to practice writing pipes using a Tidyverse data set where there are good online resources. You don’t have to hand in the first part. The second part uses a survey data set, and should be handed in.\n\n\nWe’ll use a data set from Tidyverse of all flights departing from New York City. The data set can be installed as a package install.packages(\"nycflights13\"). After calling library(nycflights13) you should have a data set flights available.\nYou may want to skim over R4DS on transformations. This chapter contains both a good introduction to the flights data as well as a recap on many of the concepts used in class on data wrangling.\n\nlibrary(nycflights13) \nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAssignment 1: Complete the first exercise of 4.2.5 in R4DS (repeated below):\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nSee also this solution manual for alternative ways of solving the assignments.\n\n# 1\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 2\nflights |&gt; \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 3\nflights |&gt; \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 4\nflights |&gt; \n  filter(month %in% c(7, 8, 9))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 5\nflights |&gt; \n  filter(dep_delay &lt;=0 ) |&gt; \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 29 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3    18     1844           1847        -3       39           2219\n 7  2013     4    17     1635           1640        -5     2049           1845\n 8  2013     4    18      558            600        -2     1149            850\n 9  2013     4    18      655            700        -5     1213            950\n10  2013     5    22     1827           1830        -3     2217           2010\n# ℹ 19 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 6\nflights |&gt; \n  filter(dep_delay &gt;= 60) |&gt; \n  mutate(delay_flight_decrease = dep_delay - arr_delay) |&gt; \n  filter(delay_flight_decrease &gt; 30)\n\n# A tibble: 1,844 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, delay_flight_decrease &lt;dbl&gt;\n\n\n\n\n\nAssignment 2: Answer the questions below\n\nSpeed is distance divided by airtime. Use the planes dataframe (included in the nycflights13-package) to find the fastest and slowest aircraft model measured by speed in average km/h. Your answer should be a tibble containing the fastest and slowest plane\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nflights |&gt; \n  inner_join(planes, by=join_by(tailnum)) |&gt; \n  mutate(speed_kmh = 1.609344 * distance / (air_time/60)) |&gt; \n  summarise(avg_speed_kmh = mean(speed_kmh, na.rm=TRUE), .by=model) |&gt; \n  filter(avg_speed_kmh %in% c(max(avg_speed_kmh), min(avg_speed_kmh)))\n\n# A tibble: 2 × 2\n  model       avg_speed_kmh\n  &lt;chr&gt;               &lt;dbl&gt;\n1 CL-600-2B19          501.\n2 777-222              777.\n\n\n\n\n\n\n\n\nFor this homework you will practice your data wrangling skills using a survey data set that you can download here: data-ESS8NO.dta. The data comes from the European Social Survey, and is available at [europeansocialsurvey.org] in various formats. Further, the data from ESS contains many variables with abbreviated/encoded names. The file contains survey responses from Norway in 2016. To get you started with the assignment, you may use the commands below to read in the data set. You may need to install the package foreign first in order to use the read.dta function.\n\nlibrary(foreign)\n\ndf &lt;- \n  read.dta(\"data-ESS8NO.dta\") %&gt;%\n  transmute(\n    party = prtvtbno, \n    age = agea, \n    religiosity = rlgdgr, \n    income_decile = hinctnta)\n\nThe variables of interest prtvtbno, agea, rlgdgr and hinctnta are defined in the attached documentation, together with more information on the actual questions. To keep it simple, we rename them to party, age, religiosity and income_decile.\n\nProvide summary statistics of age of respondents split by the party the respondents voted for last election. Who has the oldest/youngest voters? Where is the standard deviation of voters age the largest? Do not report numbers for parties with less than 25 respondents.\nThe variables religiosity, income_decile and party are encoded as factors (take a look at ?factor for en explanation). Further, they contain some non-responses such as “Don’t know”, “Refusal” and “No answer”. Find a method for filtering out all the non-responses. How many respondents did not provide an eligible response to each of the questions? How many answered both the party and income question?\nFilter out all ineligible responses for both income and party. Calculate the average religiosity of each party. Provide your answer as a data frame with one row pr. party, sorted by average religiosity.\n(Slightly trickier!) For each party with more than 75 voters, calculate the ratio of the number of respondents in income deciles 9 and 10 over the number of respondents in income deciles 1 and 2. Which party has the highest high- to low-income voters?\n\nFor completeness: When working with survey data, we usually have to apply weights to ensure estimates are representative of the population. This is because a survey sample may be a non-random sample of the general population. The survey data set provides the weights. You don’t have to worry about weights in this assignment, but please store the link “survey data -&gt; weights?!?” in your mind for future work.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignment-02.html#part-1",
    "href": "assignment-02.html#part-1",
    "title": "Assignment 2",
    "section": "",
    "text": "We’ll use a data set from Tidyverse of all flights departing from New York City. The data set can be installed as a package install.packages(\"nycflights13\"). After calling library(nycflights13) you should have a data set flights available.\nYou may want to skim over R4DS on transformations. This chapter contains both a good introduction to the flights data as well as a recap on many of the concepts used in class on data wrangling.\n\nlibrary(nycflights13) \nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nAssignment 1: Complete the first exercise of 4.2.5 in R4DS (repeated below):\nIn a single pipeline for each condition, find all flights that meet the condition:\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\nSee also this solution manual for alternative ways of solving the assignments.\n\n# 1\nflights |&gt; \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 10,200 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      811            630       101     1047            830\n 2  2013     1     1      848           1835       853     1001           1950\n 3  2013     1     1      957            733       144     1056            853\n 4  2013     1     1     1114            900       134     1447           1222\n 5  2013     1     1     1505           1310       115     1638           1431\n 6  2013     1     1     1525           1340       105     1831           1626\n 7  2013     1     1     1549           1445        64     1912           1656\n 8  2013     1     1     1558           1359       119     1718           1515\n 9  2013     1     1     1732           1630        62     2028           1825\n10  2013     1     1     1803           1620       103     2008           1750\n# ℹ 10,190 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 2\nflights |&gt; \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n# A tibble: 9,313 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      623            627        -4      933            932\n 4  2013     1     1      728            732        -4     1041           1038\n 5  2013     1     1      739            739         0     1104           1038\n 6  2013     1     1      908            908         0     1228           1219\n 7  2013     1     1     1028           1026         2     1350           1339\n 8  2013     1     1     1044           1045        -1     1352           1351\n 9  2013     1     1     1114            900       134     1447           1222\n10  2013     1     1     1205           1200         5     1503           1505\n# ℹ 9,303 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 3\nflights |&gt; \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n# A tibble: 139,504 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      554            600        -6      812            837\n 5  2013     1     1      554            558        -4      740            728\n 6  2013     1     1      558            600        -2      753            745\n 7  2013     1     1      558            600        -2      924            917\n 8  2013     1     1      558            600        -2      923            937\n 9  2013     1     1      559            600        -1      941            910\n10  2013     1     1      559            600        -1      854            902\n# ℹ 139,494 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 4\nflights |&gt; \n  filter(month %in% c(7, 8, 9))\n\n# A tibble: 86,326 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     7     1        1           2029       212      236           2359\n 2  2013     7     1        2           2359         3      344            344\n 3  2013     7     1       29           2245       104      151              1\n 4  2013     7     1       43           2130       193      322             14\n 5  2013     7     1       44           2150       174      300            100\n 6  2013     7     1       46           2051       235      304           2358\n 7  2013     7     1       48           2001       287      308           2305\n 8  2013     7     1       58           2155       183      335             43\n 9  2013     7     1      100           2146       194      327             30\n10  2013     7     1      100           2245       135      337            135\n# ℹ 86,316 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 5\nflights |&gt; \n  filter(dep_delay &lt;=0 ) |&gt; \n  filter(arr_delay &gt;= 120)\n\n# A tibble: 29 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1    27     1419           1420        -1     1754           1550\n 2  2013    10     7     1350           1350         0     1736           1526\n 3  2013    10     7     1357           1359        -2     1858           1654\n 4  2013    10    16      657            700        -3     1258           1056\n 5  2013    11     1      658            700        -2     1329           1015\n 6  2013     3    18     1844           1847        -3       39           2219\n 7  2013     4    17     1635           1640        -5     2049           1845\n 8  2013     4    18      558            600        -2     1149            850\n 9  2013     4    18      655            700        -5     1213            950\n10  2013     5    22     1827           1830        -3     2217           2010\n# ℹ 19 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n# 6\nflights |&gt; \n  filter(dep_delay &gt;= 60) |&gt; \n  mutate(delay_flight_decrease = dep_delay - arr_delay) |&gt; \n  filter(delay_flight_decrease &gt; 30)\n\n# A tibble: 1,844 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1     2205           1720       285       46           2040\n 2  2013     1     1     2326           2130       116      131             18\n 3  2013     1     3     1503           1221       162     1803           1555\n 4  2013     1     3     1839           1700        99     2056           1950\n 5  2013     1     3     1850           1745        65     2148           2120\n 6  2013     1     3     1941           1759       102     2246           2139\n 7  2013     1     3     1950           1845        65     2228           2227\n 8  2013     1     3     2015           1915        60     2135           2111\n 9  2013     1     3     2257           2000       177       45           2224\n10  2013     1     4     1917           1700       137     2135           1950\n# ℹ 1,834 more rows\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, delay_flight_decrease &lt;dbl&gt;\n\n\n\n\n\nAssignment 2: Answer the questions below\n\nSpeed is distance divided by airtime. Use the planes dataframe (included in the nycflights13-package) to find the fastest and slowest aircraft model measured by speed in average km/h. Your answer should be a tibble containing the fastest and slowest plane\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1\nflights |&gt; \n  inner_join(planes, by=join_by(tailnum)) |&gt; \n  mutate(speed_kmh = 1.609344 * distance / (air_time/60)) |&gt; \n  summarise(avg_speed_kmh = mean(speed_kmh, na.rm=TRUE), .by=model) |&gt; \n  filter(avg_speed_kmh %in% c(max(avg_speed_kmh), min(avg_speed_kmh)))\n\n# A tibble: 2 × 2\n  model       avg_speed_kmh\n  &lt;chr&gt;               &lt;dbl&gt;\n1 CL-600-2B19          501.\n2 777-222              777.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignment-02.html#part-2-survey-data",
    "href": "assignment-02.html#part-2-survey-data",
    "title": "Assignment 2",
    "section": "",
    "text": "For this homework you will practice your data wrangling skills using a survey data set that you can download here: data-ESS8NO.dta. The data comes from the European Social Survey, and is available at [europeansocialsurvey.org] in various formats. Further, the data from ESS contains many variables with abbreviated/encoded names. The file contains survey responses from Norway in 2016. To get you started with the assignment, you may use the commands below to read in the data set. You may need to install the package foreign first in order to use the read.dta function.\n\nlibrary(foreign)\n\ndf &lt;- \n  read.dta(\"data-ESS8NO.dta\") %&gt;%\n  transmute(\n    party = prtvtbno, \n    age = agea, \n    religiosity = rlgdgr, \n    income_decile = hinctnta)\n\nThe variables of interest prtvtbno, agea, rlgdgr and hinctnta are defined in the attached documentation, together with more information on the actual questions. To keep it simple, we rename them to party, age, religiosity and income_decile.\n\nProvide summary statistics of age of respondents split by the party the respondents voted for last election. Who has the oldest/youngest voters? Where is the standard deviation of voters age the largest? Do not report numbers for parties with less than 25 respondents.\nThe variables religiosity, income_decile and party are encoded as factors (take a look at ?factor for en explanation). Further, they contain some non-responses such as “Don’t know”, “Refusal” and “No answer”. Find a method for filtering out all the non-responses. How many respondents did not provide an eligible response to each of the questions? How many answered both the party and income question?\nFilter out all ineligible responses for both income and party. Calculate the average religiosity of each party. Provide your answer as a data frame with one row pr. party, sorted by average religiosity.\n(Slightly trickier!) For each party with more than 75 voters, calculate the ratio of the number of respondents in income deciles 9 and 10 over the number of respondents in income deciles 1 and 2. Which party has the highest high- to low-income voters?\n\nFor completeness: When working with survey data, we usually have to apply weights to ensure estimates are representative of the population. This is because a survey sample may be a non-random sample of the general population. The survey data set provides the weights. You don’t have to worry about weights in this assignment, but please store the link “survey data -&gt; weights?!?” in your mind for future work.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "11-documentation-and-deployment.html",
    "href": "11-documentation-and-deployment.html",
    "title": "12 Documentation, reproducability and deployment",
    "section": "",
    "text": "12 Documentation, reproducability and deployment\nIn this lecture we’ll discuss documentation and reproducability. We’ll walk through the following slides in class, and work on a small in-class assignment at the end.\n\nDocumentation and code standards\nReproducability\nA simple webservice example",
    "crumbs": [
      "Home",
      "PART 2",
      "12. Documentation and deployment"
    ]
  },
  {
    "objectID": "assignment-04.html",
    "href": "assignment-04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Note\n\n\n\nThis assignment is to be handed in as an .R-file through Canvas. Your answer will be reviewed by a teaching assistant as well as two fellow students. Do take the time to condsider the feedback. You will also receive two random answers from fellow students for review. Try to find one positive aspect as well as one suggestion for improvement in each of the answers. You must complete both peer reviews in order to pass the assignment.\n\n\nIn this assignment we will use what we have learned about functions and loops to explore some basic concepts that we may (or may not) remember from our basic statistics courses.\n\n\nAssume that we observe n independent realizations X_1,\\ldots,X_n of the random variable X having mean \\textrm{E}(X) = \\mu and variance \\textrm{Var}(X) = \\sigma^2. We know that the sample mean \\overline X is unbiased:\n\\textrm{E}(\\overline X) = \\mu. Furthermore, we know that the variance of the sample mean is given by\n\\textrm{Var}(\\overline X) = \\textrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2}\\cdot n \\cdot \\textrm{Var}(X) = \\frac{\\sigma^2}{n}, and hence that the standard deviation of the sample mean is given by \\textrm{SD}(X) = \\sigma/\\sqrt{n}. All this makes very good sense: As the sample size increases, the precision of the sample mean as an estimator for the population mean increases as well, because the standard deviation decreases towards zero with the speed of 1/\\sqrt{n}. Let us set up a simulation experiment to see how this works in practice.\n\nWrite a function with arguments N , mu and sigma that simulates N observations from a normal distribution with mean mu and standard deviation sigma, and returns the mean of the sample. The default value of mu should be 0 and the default value of sigma should be 1. Give the function a suitable name.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nThe function for generating observations from the normal distribution is rnorm(), see the help file to see what the arguments are called, and pay particular attention to make sure that you do not mix up variance and standard deviation. Repeated applications of the function should look something like this:\n\nsim_norm(N = 10)\n\n[1] 0.2721969\n\nsim_norm(N = 10)\n\n[1] -0.6247754\n\nsim_norm(N = 10)\n\n[1] 0.05945398\n\n\n… or with a different value of the population mean:\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.47533\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.18526\n\nsim_norm(N = 10, mu = 15)\n\n[1] 14.22334\n\n\n\n\n\n\nDefine a variable M and assign to it an integer, say M &lt;- 20. Create an empty vector of length M, and use a for-loop to fill the vector with sample means from repeated applications of the function you made in question 1. Use whatever sample size that you wish.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nReplace the dots (...) in the code chunk below.\n\nM &lt;- ...\nsample_means &lt;- rep(NA, M)\n\nfor(i in ...) {\n  sample_means[...] &lt;- sim_norm(...)\n}\n\nsample_means\n\nUsing N = 10 and M = 20 should give something like this (however not with identical results because you will sample different numbers).\n\n\n [1] -0.23573099 -0.49772998 -0.18881073  0.34319272 -0.44347659  0.29516097\n [7] -0.26850149 -0.46515385 -0.03407636  0.10550928 -0.03804800 -0.35927626\n[13]  0.25476781  0.88334876 -0.40717371  0.38919841 -0.17935563  0.35936155\n[19]  0.32686868  0.33651186\n\n\n\n\n\n\nCalculate the standard deviation of all the sample means that you just calculated above. Compare it with the true value.\n\n\n\n\n\n\n\nExpand for a little hint.\n\n\n\n\n\nThe theoretical value is \\sigma/\\sqrt{n} as we saw in the introduction, or just 1/\\sqrt{n} if you used the default value of sigma = 1.\n\n\n\nNow, we want to do a systematic experiment where we calculate the sample variance for a relatively large number of sample means (i.e. M relatively large) for different values of N. We would like to do that by creating a table with the following structure:\n\n\n\nN\nst_dev\nsigma\ntheoretical\n\n\n\n\n10\n\n1\n1.3162278\n\n\n\\ldots\n\n\\ldots\n\\ldots\n\n\n200\n\n1\n0.07071068\n\n\n\nThe first column contains the sample size that we use for calculating the sample means, the second column is empty for now, but will eventually contain the empirical standard deviation of M means for that sample size, the third column contains the value of sigma that we have chosen for the experiment, and the final column contains the theoretical value for the standard deviation of the means (\\sigma/\\sqrt{n}).\n\nCreate this table as a data frame (or a tibble) in R, with a suitable sequence of sample sizes\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou can use the seq() function to create a sequence of numbers, by for example specifying the minimum and maximum value and the interval between each entry (see ?seq for details). You can make a tibble-type data frame (a tibble is just a data frame that has a few additional features that are particularly useful in the tidyverse) using the following structure:\n\nlibrary(dplyr)    # Contains the tibble-function\nsimexp &lt;- tibble(N = ...the vector of N-values...,\n                 st_dev = NA,\n                 sigma = ...)\n\nAnd then you can make the theoretical-column using for instance a pipe and a mutate().\n\n\n\n\nFill in the value of the st_dev-column using a for-loop and the function you made in question 1.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou must loop over all the rows of the data frame that you created in the last question, and fill in the standard deviations such as the one we calculated in question 3. That is: we can solve this by putting a for loop inside a for loop. In that case we must be careful to not mix up the two counting indices.\nYour code might look something like this, assuming that you have defined the number M and that you have initialized the data frame in the previous question:\n\nfor(i in 1:nrow(simexp)) {  # \"Outer\" for loop, using \"i\" as counting index\n  \n  # Input the code from question 3 to create a vector of sample means.\n  # You must, however change the counting index to something else (\"j\" for example)\n  # Within this \"inner\" loop, you must also pick out the correct N-value from the \n  # data frame with \"simexp$N[i]\".\n  \n  # Insert the standard deviation of the sample means into the data frame.\n  # Can be something like this:\n  simexp$st_dev[i] &lt;- sd(sample_means)\n  \n}\n\nNB! It is good if some alarm bells go off here. Nesting for loops inside of each other is not something that we like, because they are so inefficient. A natural next step here is to see if we can get rid of one of them by vectorizing or some other technique (that might be slightly too advanced for us at this point).\n\n\n\n\nMake a graph where you compare the observed standard deviations with the theoretical value.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nHere is a simple plot using M = 200; observed as solid line, theoretical as dashed line. Looks good!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the real world you will sometimes encounter data with strange distributions. One such example is insurance claims. For many insurance products, e.g. fire insurance, most customers will never have a claim. But occasionally a house may burn down, leading to a large claim because it must be rebuilt from the ground up. For the insurance industry it is vital that the expected claim cost is estimated correctly.\nTo see what claims data can look like, we can use the “tweedie”-package. We can interpret an observation as the total claim cost for one insurance held for a full year. The parameters phi and power control the shape of the distribution, and mu is the expected value.\nThe function rtweedie(n, mu, phi, power) generates n observations from the tweedie distribution with the specified parameters. You can try to generate 10 insurance claims from the distribution with mean mu = 10 and shape parameters phi = 10 000 and power = 1.9 using the following line:\n\nlibrary(tweedie)\nrtweedie(n = 10, mu = 10000, phi = 1000, power = 1.9)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nMost likely you will observe 10 zeros, each of which belong to the majority of houses that do not burn down in a given year. If you do observe a non-zero, however, this would represent the total amount claimed by a single (unlucky) insurance holder that year.\nSo what is the expected value of the claim for each customer? The long-run average across all customers of the insurance company, i.e. the total amount of money that the company on average pays out to their policy holders divided by the number of policy holders and the number of years of data?\nWe know the answer to this question in our particular case because we generate the observations from a known probability distribution: The answer is mu = 10 000. Knowing this number is obviously very important to the company, because it dictates that the price of each of the insurance policies should be something like\n10 000 + \\text{Costs} + \\textrm{Safety margin} + \\textrm{Profits}, that is, the amount that the insurance company expects to pay back to the customer in damages, plus the costs of running the company, plus some safety margin to make sure that the natural fluctuations from year to year do not cause the company to lose money in bad years, plus some profits that make the whole endeavor worthwhile as a business.\nThe expected payout, however, is not known in practice, and must be estimated from historical data. We usually do this by taking the average, but we see immediately that it is not obvious that this will work in our situation above where we had just ten observations. If you sampled ten zeros, then the average is zero, and we have barely any information at all regarding the amount that the insurance companly must charge for these policies. In the event that you sampled one or more non-zeros, it is quite unlikely that the average over ten observations is anywhere near the true expected value of 10 000.\nWe can ramp this experiment up a notch, by rather sampling 100 000 observations from this distributions (or claimed amount from 100 000 customers over a given year if you wish) and then taking the average:\n\nx &lt;- rtweedie(n = 100000, mu = 10000, phi = 1000, power = 1.9)\nmean(x)\n\nThis number is likely not that far from the true expectation. The story line here is as follows: How large must the sample be for the insurance company to make useful statistical inferences about their population of customers?\nOne classical method for making statements regarding the unknown population mean is the t-test. You may recall from your introduction to statistics that the t-statistic provides a measure of the distance from the observed sample mean to some hypothesized population mean that is approximately normally distributed. This is due to the Cental Limit Theorem, which postulates that the distribution of the sample mean (and essentially any other statistic that is based on a sum of random variables such as the t-statistic) converges towards the normal distribution… which is almost always true. One exception is if the underlying distribution of the observations is exceptionally weird. We have already seen that the claim-type data that we generate from the tweedie distribution are quite special, and that we need a fair amount of data in order to see the usual convergence of the sample mean towards the population mean.\nSo: How large must the sample size be in the insurance context? Let us explore.\nIn the code snippet below, we first define the variables N and true_mu and set their value to 100 and 10 000, respectively. We then sample n observations from the tweedie distribution having expected value equal to true_mu (and phi = 1000 and power = 1.9 as above).\nFinally, we use the sampled data to test the null hypothesis whether \\mu = 10 000, a hypothesis that we know is true. However, if the p-value of the test is smaller than 5%, then we reject the null hypothesis at the 5% level.\n\nN &lt;- 100\ntrue_mu &lt;- 10000\nsample &lt;- rtweedie(N, mu = true_mu, phi = 1000, power = 1.9)\nt.test(sample, mu = true_mu)\n\nYou can check this several times and see what you get. If the t-test works as it should (i.e., if the sample is large enough for the Central Limit Theorem to provide a reasonable approximation for the distribution of the test statistic), then we should reject the null hypothesis 5% of the times.\nBut, why repeat such a trivial task manually? Let us put it in a loop, an perform a proper simulation experiment. The idea is to simulate M datasets, each with N observations, and run the t-test on each one of the M datasets. Again, with a 5% significance level we should expect to reject the null in about 5% of the times that we do this, if we have enough data.\nPerform the following tasks:\n\nCreate a function simTweedieTest() that takes N as argument. The function should simulate a data set with a tweedie-distribution with parameters mu = 10000, phi = 100, power = 1.9, and run a t-test on the simulated data using the null hypothesis \\mu_0 = 10000. The function should return the p-value of the test.\n\n\n\n\n\n\n\nYou must figure out how to extract the p-value from a t-test. Google it first. Click here to see a line of code that you can adapt to use in your function.\n\n\n\n\n\n\nt.test(rtweedie(N = ..., mu = ..., phi = ..., power = ...), mu = ...)$p.value\n\n\n\n\n\nCreate a function MTweedieTests(), that takes M, N and alpha as arguments. This function should call the simTweedieTests() function M times with a sample size of N. The function MTweedieTests() should then return percentage of tests where the p-value is lower than alpha (e.g. if we run tests on M = 10 datasets, and have p-values lower than \\alpha = .05 in two of the tests, the function should return 0.2).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis can be solved by making an empty vector of size M, which we then fill with p-values using a for-loop. If you want to try to make the code briefer, without using the for-loop explicitly, you can check out the function replicate(M, fun(...)). `replicate() works well for random number generating functions.\n\n\n\n\nCreate a data frame tibble(N = c(10, 100, 1000, 5000), M = 100, share_reject = NA). Use a loop and the MTweedieTests-function to fill in the values in the share_reject-column. Create a figure with N on the X-axis and share_reject on the Y-axis. What does this tell you of the validity of the t-test in on this specific distribution? What does “large enough sample” mean for this?\n(Trickier) How general are the findings in the previous question? And can we be sure we wrote the code correctly? If the data follows a normal distribution instead of a tweedie distribution we should expect that the t-test works better at lower sample sizes. Create a figure similar to question 3, but with two curves: one for tweedie-distributed data and one for normally distributed data. You will have to rewrite the functions from questions 1-3. Think carefully on how you structure the functions: avoid duplicating code, use sensible names for arguments and functions, and ensure that the mean of the data and the test is consistent.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignment-04.html#problem-1",
    "href": "assignment-04.html#problem-1",
    "title": "Assignment 4",
    "section": "",
    "text": "Assume that we observe n independent realizations X_1,\\ldots,X_n of the random variable X having mean \\textrm{E}(X) = \\mu and variance \\textrm{Var}(X) = \\sigma^2. We know that the sample mean \\overline X is unbiased:\n\\textrm{E}(\\overline X) = \\mu. Furthermore, we know that the variance of the sample mean is given by\n\\textrm{Var}(\\overline X) = \\textrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2}\\cdot n \\cdot \\textrm{Var}(X) = \\frac{\\sigma^2}{n}, and hence that the standard deviation of the sample mean is given by \\textrm{SD}(X) = \\sigma/\\sqrt{n}. All this makes very good sense: As the sample size increases, the precision of the sample mean as an estimator for the population mean increases as well, because the standard deviation decreases towards zero with the speed of 1/\\sqrt{n}. Let us set up a simulation experiment to see how this works in practice.\n\nWrite a function with arguments N , mu and sigma that simulates N observations from a normal distribution with mean mu and standard deviation sigma, and returns the mean of the sample. The default value of mu should be 0 and the default value of sigma should be 1. Give the function a suitable name.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nThe function for generating observations from the normal distribution is rnorm(), see the help file to see what the arguments are called, and pay particular attention to make sure that you do not mix up variance and standard deviation. Repeated applications of the function should look something like this:\n\nsim_norm(N = 10)\n\n[1] 0.2721969\n\nsim_norm(N = 10)\n\n[1] -0.6247754\n\nsim_norm(N = 10)\n\n[1] 0.05945398\n\n\n… or with a different value of the population mean:\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.47533\n\nsim_norm(N = 10, mu = 15)\n\n[1] 15.18526\n\nsim_norm(N = 10, mu = 15)\n\n[1] 14.22334\n\n\n\n\n\n\nDefine a variable M and assign to it an integer, say M &lt;- 20. Create an empty vector of length M, and use a for-loop to fill the vector with sample means from repeated applications of the function you made in question 1. Use whatever sample size that you wish.\n\n\n\n\n\n\n\nExpand for some hints.\n\n\n\n\n\nReplace the dots (...) in the code chunk below.\n\nM &lt;- ...\nsample_means &lt;- rep(NA, M)\n\nfor(i in ...) {\n  sample_means[...] &lt;- sim_norm(...)\n}\n\nsample_means\n\nUsing N = 10 and M = 20 should give something like this (however not with identical results because you will sample different numbers).\n\n\n [1] -0.23573099 -0.49772998 -0.18881073  0.34319272 -0.44347659  0.29516097\n [7] -0.26850149 -0.46515385 -0.03407636  0.10550928 -0.03804800 -0.35927626\n[13]  0.25476781  0.88334876 -0.40717371  0.38919841 -0.17935563  0.35936155\n[19]  0.32686868  0.33651186\n\n\n\n\n\n\nCalculate the standard deviation of all the sample means that you just calculated above. Compare it with the true value.\n\n\n\n\n\n\n\nExpand for a little hint.\n\n\n\n\n\nThe theoretical value is \\sigma/\\sqrt{n} as we saw in the introduction, or just 1/\\sqrt{n} if you used the default value of sigma = 1.\n\n\n\nNow, we want to do a systematic experiment where we calculate the sample variance for a relatively large number of sample means (i.e. M relatively large) for different values of N. We would like to do that by creating a table with the following structure:\n\n\n\nN\nst_dev\nsigma\ntheoretical\n\n\n\n\n10\n\n1\n1.3162278\n\n\n\\ldots\n\n\\ldots\n\\ldots\n\n\n200\n\n1\n0.07071068\n\n\n\nThe first column contains the sample size that we use for calculating the sample means, the second column is empty for now, but will eventually contain the empirical standard deviation of M means for that sample size, the third column contains the value of sigma that we have chosen for the experiment, and the final column contains the theoretical value for the standard deviation of the means (\\sigma/\\sqrt{n}).\n\nCreate this table as a data frame (or a tibble) in R, with a suitable sequence of sample sizes\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou can use the seq() function to create a sequence of numbers, by for example specifying the minimum and maximum value and the interval between each entry (see ?seq for details). You can make a tibble-type data frame (a tibble is just a data frame that has a few additional features that are particularly useful in the tidyverse) using the following structure:\n\nlibrary(dplyr)    # Contains the tibble-function\nsimexp &lt;- tibble(N = ...the vector of N-values...,\n                 st_dev = NA,\n                 sigma = ...)\n\nAnd then you can make the theoretical-column using for instance a pipe and a mutate().\n\n\n\n\nFill in the value of the st_dev-column using a for-loop and the function you made in question 1.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nYou must loop over all the rows of the data frame that you created in the last question, and fill in the standard deviations such as the one we calculated in question 3. That is: we can solve this by putting a for loop inside a for loop. In that case we must be careful to not mix up the two counting indices.\nYour code might look something like this, assuming that you have defined the number M and that you have initialized the data frame in the previous question:\n\nfor(i in 1:nrow(simexp)) {  # \"Outer\" for loop, using \"i\" as counting index\n  \n  # Input the code from question 3 to create a vector of sample means.\n  # You must, however change the counting index to something else (\"j\" for example)\n  # Within this \"inner\" loop, you must also pick out the correct N-value from the \n  # data frame with \"simexp$N[i]\".\n  \n  # Insert the standard deviation of the sample means into the data frame.\n  # Can be something like this:\n  simexp$st_dev[i] &lt;- sd(sample_means)\n  \n}\n\nNB! It is good if some alarm bells go off here. Nesting for loops inside of each other is not something that we like, because they are so inefficient. A natural next step here is to see if we can get rid of one of them by vectorizing or some other technique (that might be slightly too advanced for us at this point).\n\n\n\n\nMake a graph where you compare the observed standard deviations with the theoretical value.\n\n\n\n\n\n\n\nExpand for a hint\n\n\n\n\n\nHere is a simple plot using M = 200; observed as solid line, theoretical as dashed line. Looks good!",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignment-04.html#problem-2",
    "href": "assignment-04.html#problem-2",
    "title": "Assignment 4",
    "section": "",
    "text": "In the real world you will sometimes encounter data with strange distributions. One such example is insurance claims. For many insurance products, e.g. fire insurance, most customers will never have a claim. But occasionally a house may burn down, leading to a large claim because it must be rebuilt from the ground up. For the insurance industry it is vital that the expected claim cost is estimated correctly.\nTo see what claims data can look like, we can use the “tweedie”-package. We can interpret an observation as the total claim cost for one insurance held for a full year. The parameters phi and power control the shape of the distribution, and mu is the expected value.\nThe function rtweedie(n, mu, phi, power) generates n observations from the tweedie distribution with the specified parameters. You can try to generate 10 insurance claims from the distribution with mean mu = 10 and shape parameters phi = 10 000 and power = 1.9 using the following line:\n\nlibrary(tweedie)\nrtweedie(n = 10, mu = 10000, phi = 1000, power = 1.9)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nMost likely you will observe 10 zeros, each of which belong to the majority of houses that do not burn down in a given year. If you do observe a non-zero, however, this would represent the total amount claimed by a single (unlucky) insurance holder that year.\nSo what is the expected value of the claim for each customer? The long-run average across all customers of the insurance company, i.e. the total amount of money that the company on average pays out to their policy holders divided by the number of policy holders and the number of years of data?\nWe know the answer to this question in our particular case because we generate the observations from a known probability distribution: The answer is mu = 10 000. Knowing this number is obviously very important to the company, because it dictates that the price of each of the insurance policies should be something like\n10 000 + \\text{Costs} + \\textrm{Safety margin} + \\textrm{Profits}, that is, the amount that the insurance company expects to pay back to the customer in damages, plus the costs of running the company, plus some safety margin to make sure that the natural fluctuations from year to year do not cause the company to lose money in bad years, plus some profits that make the whole endeavor worthwhile as a business.\nThe expected payout, however, is not known in practice, and must be estimated from historical data. We usually do this by taking the average, but we see immediately that it is not obvious that this will work in our situation above where we had just ten observations. If you sampled ten zeros, then the average is zero, and we have barely any information at all regarding the amount that the insurance companly must charge for these policies. In the event that you sampled one or more non-zeros, it is quite unlikely that the average over ten observations is anywhere near the true expected value of 10 000.\nWe can ramp this experiment up a notch, by rather sampling 100 000 observations from this distributions (or claimed amount from 100 000 customers over a given year if you wish) and then taking the average:\n\nx &lt;- rtweedie(n = 100000, mu = 10000, phi = 1000, power = 1.9)\nmean(x)\n\nThis number is likely not that far from the true expectation. The story line here is as follows: How large must the sample be for the insurance company to make useful statistical inferences about their population of customers?\nOne classical method for making statements regarding the unknown population mean is the t-test. You may recall from your introduction to statistics that the t-statistic provides a measure of the distance from the observed sample mean to some hypothesized population mean that is approximately normally distributed. This is due to the Cental Limit Theorem, which postulates that the distribution of the sample mean (and essentially any other statistic that is based on a sum of random variables such as the t-statistic) converges towards the normal distribution… which is almost always true. One exception is if the underlying distribution of the observations is exceptionally weird. We have already seen that the claim-type data that we generate from the tweedie distribution are quite special, and that we need a fair amount of data in order to see the usual convergence of the sample mean towards the population mean.\nSo: How large must the sample size be in the insurance context? Let us explore.\nIn the code snippet below, we first define the variables N and true_mu and set their value to 100 and 10 000, respectively. We then sample n observations from the tweedie distribution having expected value equal to true_mu (and phi = 1000 and power = 1.9 as above).\nFinally, we use the sampled data to test the null hypothesis whether \\mu = 10 000, a hypothesis that we know is true. However, if the p-value of the test is smaller than 5%, then we reject the null hypothesis at the 5% level.\n\nN &lt;- 100\ntrue_mu &lt;- 10000\nsample &lt;- rtweedie(N, mu = true_mu, phi = 1000, power = 1.9)\nt.test(sample, mu = true_mu)\n\nYou can check this several times and see what you get. If the t-test works as it should (i.e., if the sample is large enough for the Central Limit Theorem to provide a reasonable approximation for the distribution of the test statistic), then we should reject the null hypothesis 5% of the times.\nBut, why repeat such a trivial task manually? Let us put it in a loop, an perform a proper simulation experiment. The idea is to simulate M datasets, each with N observations, and run the t-test on each one of the M datasets. Again, with a 5% significance level we should expect to reject the null in about 5% of the times that we do this, if we have enough data.\nPerform the following tasks:\n\nCreate a function simTweedieTest() that takes N as argument. The function should simulate a data set with a tweedie-distribution with parameters mu = 10000, phi = 100, power = 1.9, and run a t-test on the simulated data using the null hypothesis \\mu_0 = 10000. The function should return the p-value of the test.\n\n\n\n\n\n\n\nYou must figure out how to extract the p-value from a t-test. Google it first. Click here to see a line of code that you can adapt to use in your function.\n\n\n\n\n\n\nt.test(rtweedie(N = ..., mu = ..., phi = ..., power = ...), mu = ...)$p.value\n\n\n\n\n\nCreate a function MTweedieTests(), that takes M, N and alpha as arguments. This function should call the simTweedieTests() function M times with a sample size of N. The function MTweedieTests() should then return percentage of tests where the p-value is lower than alpha (e.g. if we run tests on M = 10 datasets, and have p-values lower than \\alpha = .05 in two of the tests, the function should return 0.2).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThis can be solved by making an empty vector of size M, which we then fill with p-values using a for-loop. If you want to try to make the code briefer, without using the for-loop explicitly, you can check out the function replicate(M, fun(...)). `replicate() works well for random number generating functions.\n\n\n\n\nCreate a data frame tibble(N = c(10, 100, 1000, 5000), M = 100, share_reject = NA). Use a loop and the MTweedieTests-function to fill in the values in the share_reject-column. Create a figure with N on the X-axis and share_reject on the Y-axis. What does this tell you of the validity of the t-test in on this specific distribution? What does “large enough sample” mean for this?\n(Trickier) How general are the findings in the previous question? And can we be sure we wrote the code correctly? If the data follows a normal distribution instead of a tweedie distribution we should expect that the t-test works better at lower sample sizes. Create a figure similar to question 3, but with two curves: one for tweedie-distributed data and one for normally distributed data. You will have to rewrite the functions from questions 1-3. Think carefully on how you structure the functions: avoid duplicating code, use sensible names for arguments and functions, and ensure that the mean of the data and the test is consistent.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignment-03.html",
    "href": "assignment-03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Assignment 3\n\n\n\n\n\n\nNote\n\n\n\nThis assignment is to be handed in as an .R-file through Canvas. Your answer will be reviewed by a teaching assistant as well as two fellow students. Do take the time to condsider the feedback. You will also receive two random answers from fellow students for review. Try to find one positive aspect as well as one suggestion for improvement in each of the answers. You must complete both peer reviews in order to pass the assignment.\n\n\nIn this assignment we will visualize some of the data that was published here as part of the Tidy Tuesday project. There is one observation for each country of the world, and several interesting variables. One of them is the amount of mismanaged plastic waste in each country in a given year, and we will here build a plot using this data by adding more and more layers of complexity.\nDownload the data here, and load it into memory:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nplastic &lt;- read_csv(\"data-plastic.csv\")\n\nThe warning message just says that the first column name is empty in the csv-file (confirm that by looking at the file if you wish), and that it has been given the generic name X1.\nWe would like to visualize the association between the GDP and the amount of mismanaged plastic waste in the countries of the world. Below you will find four figures of increasing complexity. Replicate them as best as you can, and feel free to spend a few minutes looking at and interpreting the graphs.\n\nStep 1: A basic scatterplot of the log-transformed variables\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Make the dots reflect country size and continent\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Fix labels, add theme (you can choose whatever you want), change color palette\n\n\n\n\n\n\n\n\n\n\n\nStep 4: Add text labels for the countries and a smoother\n\n\n\n\n\n\n\n\n\n\n\nStep 5 (if you have time): Be creative. Add more layers to the plot above, or learn something else from the data set using ggplot2",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignment-01.html",
    "href": "assignment-01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Note\n\n\n\nThis assignment is to be handed in as an .R-file through Canvas. Your answer will be reviewed by a teaching assistant as well as two fellow students. Do take the time to condsider the feedback. You will also receive two random answers from fellow students for review. Try to find one positive aspect as well as one suggestion for improvement in each of the answers. You must complete both peer reviews in order to pass the assignment.\n\n\n\n\nWe will look at the monthly returns on the NASDAQ composite stock index from August 2013 to June 2018, as well as the returns on 16 individual stocks listed on NASDAQ. The data is contained in the file data-nasdaq-returns.xls, and has been collected from the Yahoo Finance website.\nPut the data in an appropriate folder on your computer. Perform the following tasks:\n\nRead the data into R and take a first look at the data set. The main index is in the NASDAQ-column.\n\n\n\n\n\n\n\nClick here to see how the top of the data set should look like after you have loaded it into R.\n\n\n\n\n\n\n\n# A tibble: 59 × 11\n   Date     NASDAQ    ADBE     AMZN     AAPL     BBBY     CSCO    CMCSA     COST\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 2013-0… -0.0101 -0.0329 -0.0696   0.0739  -3.64e-2 -0.0933  -0.0686  -0.0484 \n 2 2013-0…  0.0494  0.127   0.107   -0.0217   4.79e-2  0.00513  0.0695   0.0291 \n 3 2013-1…  0.0386  0.0430  0.152    0.0920  -5.17e-4 -0.0378   0.0535   0.0243 \n 4 2013-1…  0.0351  0.0461  0.0781   0.0619   9.14e-3 -0.0598   0.0466   0.0611 \n 5 2013-1…  0.0283  0.0532  0.0130   0.00886  2.87e-2  0.0540   0.0412  -0.0525 \n 6 2014-0… -0.0176 -0.0116 -0.106   -0.114   -2.29e-1 -0.0235   0.0466  -0.0576 \n 7 2014-0…  0.0486  0.148   0.00946  0.0500   6.03e-2 -0.00503 -0.0520   0.0388 \n 8 2014-0… -0.0257 -0.0430 -0.0737   0.0198   1.43e-2  0.0280  -0.0324  -0.0448 \n 9 2014-0… -0.0203 -0.0636 -0.101    0.0948  -1.02e-1  0.0303   0.0338   0.0352 \n10 2014-0…  0.0306  0.0452  0.0273   0.0702  -2.08e-2  0.0633   0.00846  0.00293\n# ℹ 49 more rows\n# ℹ 2 more variables: DLTR &lt;dbl&gt;, EXPE &lt;dbl&gt;\n\n\n\n\n\n\nMake a new data frame containing only the date column and returns on the main index as well as one of the individual stocks of your choosing. Name the new data frame appropriately.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\nFor example, after picking ADBE as the stock:\n\n\n# A tibble: 59 × 3\n   Date        NASDAQ    ADBE\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329\n 2 2013-09-01  0.0494  0.127 \n 3 2013-10-01  0.0386  0.0430\n 4 2013-11-01  0.0351  0.0461\n 5 2013-12-01  0.0283  0.0532\n 6 2014-01-01 -0.0176 -0.0116\n 7 2014-02-01  0.0486  0.148 \n 8 2014-03-01 -0.0257 -0.0430\n 9 2014-04-01 -0.0203 -0.0636\n10 2014-05-01  0.0306  0.0452\n# ℹ 49 more rows\n\n\n\n\n\n\nMake a scatterplot of the two variables in your newly created data frame.\n\n\n\n\n\n\n\nClick here to see the plot should look like.\n\n\n\n\n\nStill, using ADBE, will of course look a bit different if you have chosen a different stock:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function sign(x) returns the sign of x, that is, it returns -1 if x is negative and 1 if x is positive. Make two new columns, named sign_NASDAQ and a corresponding name for the stock that you have chosen to include, that contains the sign of the return, indicating whether the index or stock went up or down that day.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 5\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329          -1        -1\n 2 2013-09-01  0.0494  0.127            1         1\n 3 2013-10-01  0.0386  0.0430           1         1\n 4 2013-11-01  0.0351  0.0461           1         1\n 5 2013-12-01  0.0283  0.0532           1         1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1\n 7 2014-02-01  0.0486  0.148            1         1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1\n10 2014-05-01  0.0306  0.0452           1         1\n# ℹ 49 more rows\n\n\n\n\n\n\nMake another column consisting of the sum of the two sign columns divided by two. The resulting value will then be -1 if both the index and the stock went down that day, 0 if they went in separate directions, and 1 if both went up.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 6\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign   sum\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329          -1        -1    -1\n 2 2013-09-01  0.0494  0.127            1         1     1\n 3 2013-10-01  0.0386  0.0430           1         1     1\n 4 2013-11-01  0.0351  0.0461           1         1     1\n 5 2013-12-01  0.0283  0.0532           1         1     1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1    -1\n 7 2014-02-01  0.0486  0.148            1         1     1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1    -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1    -1\n10 2014-05-01  0.0306  0.0452           1         1     1\n# ℹ 49 more rows\n\n\n\n\n\n\nWe would like to count the number of days for which the new sum-column is either -1, 0, or 1. Do that by applying the function table() to the sum-column. (Recall that we can pick out individual columns using the dollar-sign).\n\n\n\n\n\n\n\nClick here to see how the output should look like.\n\n\n\n\n\nFor the ADBE-stock:\n\n\n\n-1  0  1 \n15 10 34 \n\n\n\n\n\n\nIn the tasks above you may (or may not) have created several intermediate data frames under different names for each problem, or perhaps you have overwritten the data frame for each new task. Let us rather complete task 1, 2, 4 and 5 in one single operation, where you just append each task to the previous using the pipe-operator. That way you only need to come up with one name for the data set.\n\n\n\n\n\n\n\nClick here to see to see a hint if you need to.\n\n\n\n\n\nThe code may look something like this, replace the dots:\n\nstock_data &lt;-\n  read_excel(...) %&gt;%       \n  select(...) %&gt;%                  \n  mutate(...) %&gt;%          \n  mutate(...) %&gt;%              \n  mutate(...)       \n\n\n\n\n\n\n\nThe .csv-file (comma separated values) is a common format for storing data in a plain text file. The file data-missile.csv contains data on North Korean missile launches from 1984 until 2017. Put the file in folder on your computer and inspect the contents by opening it in a text editor such as Notepad or Textedit.\nR ships with a function read.csv() that we can use to read csv-files in the same way as we use read_excel() to read excel-files. We will, however, use a function from the readr-package called read_csv() for this purpose that does almost the same thing as the default read.csv()-function. There are some subtle differences between these two functions that are not very important, but read_csv() works a little bit better together with many other functions and packages that we will use later.\nLoad readr using the library() function. If you get an error message telling you that there is no package called 'readr', then you need to install it first using the install.packages()-function.\nLoad the data into into R using the following command:\n\nmissile &lt;- read_csv(\"data-missile.csv\")\n\nLook at the data. The variable «apogee» is the highest altitude reached by the missile in km. Calculate the following statistics for this variable:\n\nThe mean.\nThe median.\nThe standard deviation.\n\n\n\n\n\n\n\nMaybe you get some unexpected results here. You need to troubleshoot the problem in order to solve the issue. Click here if you need some hints on what to try.\n\n\n\n\n\nThe problem is that you get NA-values right? Why is this? Look at the data, and you will see that many values are missing, and they should be ignored when calculating the mean, median and standard deviation. Look at the help files for the functions in question (e.g. ?mean) to see if there is something you can to to fix the issue.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignment-01.html#problem-1",
    "href": "assignment-01.html#problem-1",
    "title": "Assignment 1",
    "section": "",
    "text": "We will look at the monthly returns on the NASDAQ composite stock index from August 2013 to June 2018, as well as the returns on 16 individual stocks listed on NASDAQ. The data is contained in the file data-nasdaq-returns.xls, and has been collected from the Yahoo Finance website.\nPut the data in an appropriate folder on your computer. Perform the following tasks:\n\nRead the data into R and take a first look at the data set. The main index is in the NASDAQ-column.\n\n\n\n\n\n\n\nClick here to see how the top of the data set should look like after you have loaded it into R.\n\n\n\n\n\n\n\n# A tibble: 59 × 11\n   Date     NASDAQ    ADBE     AMZN     AAPL     BBBY     CSCO    CMCSA     COST\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 2013-0… -0.0101 -0.0329 -0.0696   0.0739  -3.64e-2 -0.0933  -0.0686  -0.0484 \n 2 2013-0…  0.0494  0.127   0.107   -0.0217   4.79e-2  0.00513  0.0695   0.0291 \n 3 2013-1…  0.0386  0.0430  0.152    0.0920  -5.17e-4 -0.0378   0.0535   0.0243 \n 4 2013-1…  0.0351  0.0461  0.0781   0.0619   9.14e-3 -0.0598   0.0466   0.0611 \n 5 2013-1…  0.0283  0.0532  0.0130   0.00886  2.87e-2  0.0540   0.0412  -0.0525 \n 6 2014-0… -0.0176 -0.0116 -0.106   -0.114   -2.29e-1 -0.0235   0.0466  -0.0576 \n 7 2014-0…  0.0486  0.148   0.00946  0.0500   6.03e-2 -0.00503 -0.0520   0.0388 \n 8 2014-0… -0.0257 -0.0430 -0.0737   0.0198   1.43e-2  0.0280  -0.0324  -0.0448 \n 9 2014-0… -0.0203 -0.0636 -0.101    0.0948  -1.02e-1  0.0303   0.0338   0.0352 \n10 2014-0…  0.0306  0.0452  0.0273   0.0702  -2.08e-2  0.0633   0.00846  0.00293\n# ℹ 49 more rows\n# ℹ 2 more variables: DLTR &lt;dbl&gt;, EXPE &lt;dbl&gt;\n\n\n\n\n\n\nMake a new data frame containing only the date column and returns on the main index as well as one of the individual stocks of your choosing. Name the new data frame appropriately.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\nFor example, after picking ADBE as the stock:\n\n\n# A tibble: 59 × 3\n   Date        NASDAQ    ADBE\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329\n 2 2013-09-01  0.0494  0.127 \n 3 2013-10-01  0.0386  0.0430\n 4 2013-11-01  0.0351  0.0461\n 5 2013-12-01  0.0283  0.0532\n 6 2014-01-01 -0.0176 -0.0116\n 7 2014-02-01  0.0486  0.148 \n 8 2014-03-01 -0.0257 -0.0430\n 9 2014-04-01 -0.0203 -0.0636\n10 2014-05-01  0.0306  0.0452\n# ℹ 49 more rows\n\n\n\n\n\n\nMake a scatterplot of the two variables in your newly created data frame.\n\n\n\n\n\n\n\nClick here to see the plot should look like.\n\n\n\n\n\nStill, using ADBE, will of course look a bit different if you have chosen a different stock:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function sign(x) returns the sign of x, that is, it returns -1 if x is negative and 1 if x is positive. Make two new columns, named sign_NASDAQ and a corresponding name for the stock that you have chosen to include, that contains the sign of the return, indicating whether the index or stock went up or down that day.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 5\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329          -1        -1\n 2 2013-09-01  0.0494  0.127            1         1\n 3 2013-10-01  0.0386  0.0430           1         1\n 4 2013-11-01  0.0351  0.0461           1         1\n 5 2013-12-01  0.0283  0.0532           1         1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1\n 7 2014-02-01  0.0486  0.148            1         1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1\n10 2014-05-01  0.0306  0.0452           1         1\n# ℹ 49 more rows\n\n\n\n\n\n\nMake another column consisting of the sum of the two sign columns divided by two. The resulting value will then be -1 if both the index and the stock went down that day, 0 if they went in separate directions, and 1 if both went up.\n\n\n\n\n\n\n\nClick here to see how the top of the new data set should look like.\n\n\n\n\n\n\n\n# A tibble: 59 × 6\n   Date        NASDAQ    ADBE NASDAQ_sign ADBE_sign   sum\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2013-08-01 -0.0101 -0.0329          -1        -1    -1\n 2 2013-09-01  0.0494  0.127            1         1     1\n 3 2013-10-01  0.0386  0.0430           1         1     1\n 4 2013-11-01  0.0351  0.0461           1         1     1\n 5 2013-12-01  0.0283  0.0532           1         1     1\n 6 2014-01-01 -0.0176 -0.0116          -1        -1    -1\n 7 2014-02-01  0.0486  0.148            1         1     1\n 8 2014-03-01 -0.0257 -0.0430          -1        -1    -1\n 9 2014-04-01 -0.0203 -0.0636          -1        -1    -1\n10 2014-05-01  0.0306  0.0452           1         1     1\n# ℹ 49 more rows\n\n\n\n\n\n\nWe would like to count the number of days for which the new sum-column is either -1, 0, or 1. Do that by applying the function table() to the sum-column. (Recall that we can pick out individual columns using the dollar-sign).\n\n\n\n\n\n\n\nClick here to see how the output should look like.\n\n\n\n\n\nFor the ADBE-stock:\n\n\n\n-1  0  1 \n15 10 34 \n\n\n\n\n\n\nIn the tasks above you may (or may not) have created several intermediate data frames under different names for each problem, or perhaps you have overwritten the data frame for each new task. Let us rather complete task 1, 2, 4 and 5 in one single operation, where you just append each task to the previous using the pipe-operator. That way you only need to come up with one name for the data set.\n\n\n\n\n\n\n\nClick here to see to see a hint if you need to.\n\n\n\n\n\nThe code may look something like this, replace the dots:\n\nstock_data &lt;-\n  read_excel(...) %&gt;%       \n  select(...) %&gt;%                  \n  mutate(...) %&gt;%          \n  mutate(...) %&gt;%              \n  mutate(...)",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignment-01.html#problem-2",
    "href": "assignment-01.html#problem-2",
    "title": "Assignment 1",
    "section": "",
    "text": "The .csv-file (comma separated values) is a common format for storing data in a plain text file. The file data-missile.csv contains data on North Korean missile launches from 1984 until 2017. Put the file in folder on your computer and inspect the contents by opening it in a text editor such as Notepad or Textedit.\nR ships with a function read.csv() that we can use to read csv-files in the same way as we use read_excel() to read excel-files. We will, however, use a function from the readr-package called read_csv() for this purpose that does almost the same thing as the default read.csv()-function. There are some subtle differences between these two functions that are not very important, but read_csv() works a little bit better together with many other functions and packages that we will use later.\nLoad readr using the library() function. If you get an error message telling you that there is no package called 'readr', then you need to install it first using the install.packages()-function.\nLoad the data into into R using the following command:\n\nmissile &lt;- read_csv(\"data-missile.csv\")\n\nLook at the data. The variable «apogee» is the highest altitude reached by the missile in km. Calculate the following statistics for this variable:\n\nThe mean.\nThe median.\nThe standard deviation.\n\n\n\n\n\n\n\nMaybe you get some unexpected results here. You need to troubleshoot the problem in order to solve the issue. Click here if you need some hints on what to try.\n\n\n\n\n\nThe problem is that you get NA-values right? Why is this? Look at the data, and you will see that many values are missing, and they should be ignored when calculating the mean, median and standard deviation. Look at the help files for the functions in question (e.g. ?mean) to see if there is something you can to to fix the issue.",
    "crumbs": [
      "Home",
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "10-many-models.html",
    "href": "10-many-models.html",
    "title": "10 Many models",
    "section": "",
    "text": "10 Many models\nIn this lesson we will do something naughty; we will estimate a huge number of simple models to a data set. You may recall from your basic courses in statistics and econometrics that, as a general rule, we need to have some sort of a plan before we try to build for instance a linear regression model to explain variation in Y using a (generally unknown sub-)set of explanatory variables X_1, \\ldots, X_p. We do not want just to fit models until we get something interesting (tempting as it may be) because the results may then be entirely co-incidental, see this web page for several absurd examples of spurious correlations that are found by just trawling the web for time series that are similar.\nWe will of course not in any way suggest that the algorithm\nwhile(model not interesting) {keep looking}\nis anything other than cheating. We will however explore the other extreme from conservative inferential regression model building: What if we fit all possible models? Everything. This may result in some interesting insights because we interpret the estimated models more like derived data points that can be analyzed statistically in their own right rather than the result of a model-building exercise.\nThis lesson follows Chapter 25 of the first edition of R for Data Science (in the second edition, this chapter is taken out) by Hadley Wickham. In the video window at the bottom of this page we go through the main script file for this lesson. At the end of the video we also discuss the assignment questions for this week (and you will see that this video was not recorded this year, but the main content is the same). The script for the lesson is available in this repository.\nIf you want to explore these ideas a bit more conceptually, you can read the paper I just ran four million regressions by Xavier X. Sala-i-Martin.",
    "crumbs": [
      "Home",
      "PART 2",
      "10. Many models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is the companion website for the course BAN400 - R Programming for Data Science, given at The Norwegian School of Economics (NHH). The purpose of this website is to provide study material such as lecture videos, exercises and assignments for students taking the course.\nBAN400 has previously consisted of two separate modules; one intensive one-week introduction to R that could be taken separately as a 2.5 ECTS course as BAN420, as well as the main course itself (pun intended), which, together with BAN420, completed the 7.5 ECTS unit BAN400.\nFrom the fall semester of 2023, we offer BAN400 as one regular course. We will, however, still make an explicit transition from Part 1, where we introduce basic programming, to Part 2 where we will learn a number of useful techniques that are particularly useful when working with data.\nAll announcements and course administration such as homework delivery and feedback will be carried out through the course page at Canvas, which is the learning management system used by NHH. You will need to sign up to the course in order to gain access to the Canvas page.\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTopic\nLecturer\nAssignment\nSeminar\nAssignment due\n\n\n\n\n35\n27.08 08:15 - 12:00 (Aud M)\nIntroduction to R\nHO\nAssignment 1\n29.08 08:15 - 10:00 (LAB2)\n03.09.2024 (07:00)\n\n\n36\n03.09 08:15 - 12:00 (Aud M)\nTidy data\nOPMH\nAssignment 2\n05.09 08:15 - 10:00 (AUD G)\n10.09.2024 (07:00)\n\n\n37\n10.09 08:15 - 12:00 (Aud M)\nPlotting\nHO\nAssignment 3\n12.09 08:15 - 10:00 (BORCH)\n17.09.2024 (07:00)\n\n\n38\n17.09 08:15 - 12:00 (Aud M)\nFunctions and loops\nOPMH\nAssignment 4\n19.09 08:15 - 10:00 (LAB2)\n24.09.2024 (07:00)\n\n\n39\n24.09 08:15 - 12:00 (Aud M)\nProject organization\nHO\n\n26.09 08:15 - 10:00 (LAB2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTopic\nLecturer\nAssignment\nSeminar\nAssignment due\n\n\n\n\n40\n01.10 08:15 - 12:00 (Aud M)\nGit and Github\nHO\nGit\n03.10 08:15 - 10:00 (LAB2)\n08.10.2024 (07:00)\n\n\n41\n08.10 08:15 - 12:00 (Aud M)\nMachine learning\nHO\nMachine learning\n10.10 08:15 - 10:00 (LAB2)\n15.10.2024 (07:00)\n\n\n42\n15.10 08:15 - 12:00 (Aud M)\nIterations\nOPMH\nIterations\n17.10 08:15 - 10:00 (AUD G)\n22.10.2024 (07:00)\n\n\n43\n22.10 08:15 - 12:00 (Aud M)\nParallel computing\nOPMH\nParallel computing\n24.10 08:15 - 10:00 (LAB2)\n29.10.2024 (07:00)\n\n\n44\n29.10 08:15 - 12:00 (Aud M)\nMany models and Making maps\nHO\nMany Models/Making maps\n31.10 08:15 - 10:00 (LAB2)\n07.11.2024 (07:00)\n\n\n45\n05.11 08:15 - 12:00 (Aud M)\nDocumentation and deployment\nOPMH\n\n07:11 08:15 - 10:00 (LAB2)\n\n\n\n\n\n\nNB! Please read this guide on submitting assignments through Github Classroom before clicking on the assignment links!\n\n\n\n\n\nThe Compendium (This web page)\n\nThis page contains the study material needed for the course as well as links to other sources when needed. Most of the modules contain video lectures as well as comments, links and sometimes small exercises.\n\nText book\n\nWe will link to R for Data Science (R4DS) by Hadley Wickham many times. This is a great reference to bookmark once and for all.\n\nLectures\n\nLectures are held on Tuesdays from 08:15 to 12:00. The lectures will have the same core content as this web page, but some lectures will contain additional discussions and coding workshops.\n\nAssignments and course approval\n\nMost lectures have an accompanying assignment. See the overview above for a detailed overview and deadlines.\nThe assignments from the first part of the course can be found here on this web page, and they must be handed in via Canvas. You must also complete two peer reviews for each of these assignments for approval.\nThe remaining assignments will be administered via Github, please stand by for links and instructions for that.\nTo obtain course approval (and hence to be eligible to take the exam), you must hand in:\n\nAll four assignments from the first part of the course, and\nminimum four of the remaining six assignments.\n\nPlease make careful note of the deadlines.\n\nTeaching assistants and seminar\n\nThe teaching assistants will give feedback on your written work.\nThey will also run a weekly seminar, each X at XX:15-XX:00, mostly in X (but with some exceptions). In the seminar the TAs will discuss the assignment from the previous week, and provide support for next week’s assignment.",
    "crumbs": [
      "Home",
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Welcome!",
    "section": "",
    "text": "Week\nLecture\nTopic\nLecturer\nAssignment\nSeminar\nAssignment due\n\n\n\n\n35\n27.08 08:15 - 12:00 (Aud M)\nIntroduction to R\nHO\nAssignment 1\n29.08 08:15 - 10:00 (LAB2)\n03.09.2024 (07:00)\n\n\n36\n03.09 08:15 - 12:00 (Aud M)\nTidy data\nOPMH\nAssignment 2\n05.09 08:15 - 10:00 (AUD G)\n10.09.2024 (07:00)\n\n\n37\n10.09 08:15 - 12:00 (Aud M)\nPlotting\nHO\nAssignment 3\n12.09 08:15 - 10:00 (BORCH)\n17.09.2024 (07:00)\n\n\n38\n17.09 08:15 - 12:00 (Aud M)\nFunctions and loops\nOPMH\nAssignment 4\n19.09 08:15 - 10:00 (LAB2)\n24.09.2024 (07:00)\n\n\n39\n24.09 08:15 - 12:00 (Aud M)\nProject organization\nHO\n\n26.09 08:15 - 10:00 (LAB2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLecture\nTopic\nLecturer\nAssignment\nSeminar\nAssignment due\n\n\n\n\n40\n01.10 08:15 - 12:00 (Aud M)\nGit and Github\nHO\nGit\n03.10 08:15 - 10:00 (LAB2)\n08.10.2024 (07:00)\n\n\n41\n08.10 08:15 - 12:00 (Aud M)\nMachine learning\nHO\nMachine learning\n10.10 08:15 - 10:00 (LAB2)\n15.10.2024 (07:00)\n\n\n42\n15.10 08:15 - 12:00 (Aud M)\nIterations\nOPMH\nIterations\n17.10 08:15 - 10:00 (AUD G)\n22.10.2024 (07:00)\n\n\n43\n22.10 08:15 - 12:00 (Aud M)\nParallel computing\nOPMH\nParallel computing\n24.10 08:15 - 10:00 (LAB2)\n29.10.2024 (07:00)\n\n\n44\n29.10 08:15 - 12:00 (Aud M)\nMany models and Making maps\nHO\nMany Models/Making maps\n31.10 08:15 - 10:00 (LAB2)\n07.11.2024 (07:00)\n\n\n45\n05.11 08:15 - 12:00 (Aud M)\nDocumentation and deployment\nOPMH\n\n07:11 08:15 - 10:00 (LAB2)\n\n\n\n\n\n\nNB! Please read this guide on submitting assignments through Github Classroom before clicking on the assignment links!",
    "crumbs": [
      "Home",
      "Course overview"
    ]
  },
  {
    "objectID": "index.html#practical-information",
    "href": "index.html#practical-information",
    "title": "Welcome!",
    "section": "",
    "text": "The Compendium (This web page)\n\nThis page contains the study material needed for the course as well as links to other sources when needed. Most of the modules contain video lectures as well as comments, links and sometimes small exercises.\n\nText book\n\nWe will link to R for Data Science (R4DS) by Hadley Wickham many times. This is a great reference to bookmark once and for all.\n\nLectures\n\nLectures are held on Tuesdays from 08:15 to 12:00. The lectures will have the same core content as this web page, but some lectures will contain additional discussions and coding workshops.\n\nAssignments and course approval\n\nMost lectures have an accompanying assignment. See the overview above for a detailed overview and deadlines.\nThe assignments from the first part of the course can be found here on this web page, and they must be handed in via Canvas. You must also complete two peer reviews for each of these assignments for approval.\nThe remaining assignments will be administered via Github, please stand by for links and instructions for that.\nTo obtain course approval (and hence to be eligible to take the exam), you must hand in:\n\nAll four assignments from the first part of the course, and\nminimum four of the remaining six assignments.\n\nPlease make careful note of the deadlines.\n\nTeaching assistants and seminar\n\nThe teaching assistants will give feedback on your written work.\nThey will also run a weekly seminar, each X at XX:15-XX:00, mostly in X (but with some exceptions). In the seminar the TAs will discuss the assignment from the previous week, and provide support for next week’s assignment.",
    "crumbs": [
      "Home",
      "Course overview"
    ]
  },
  {
    "objectID": "02-data-wrangling.html",
    "href": "02-data-wrangling.html",
    "title": "2 Data wrangling",
    "section": "",
    "text": "In this section we will learn how to handle data in a very efficient way. We will learn to\n\nfilter a data set based on variable values,\nselect variables,\ncreate new variables,\ngroup data based on variables,\ndummarise the data, and\njoin different data sets.\n\nWe will obviously use R to solve these problems, but we do have the choice between different coding styles to do it. One way is to only use functions that already ship with R, or we can use functions from additional packages to solve the same problems. We choose the latter, and not only that: We will thoughout this course use a specific set of packages, an ecosystem if you wish (or even a philosophy of data work), called the tidyverse.\nThe tidyverse (tidyverse.org) is a set of R packages that work very well together, follows a consistent logic, and that enables us to write extremely clean code. We have already touched upon the difference between data wrangling using base R and the tidyverse in Chapter 1.9.\nSee the video below for some more details regarding the tidyverse. Some formulations in the video gives the impression that this material should be consumed on a specific day, but that are just some residue from a time when this part of the course was given intensively. You can find the cheat sheet here. See also this webpage for further information about one of the most central packages in tidyverse, dplyr.\n\n\n\n\n\n\nFirst, we warm up with a first few functions from dplyr. You can download the data set here: data-geilo.xlsx.\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readxl)\n\n# Import data from Excel-file\nsales &lt;- read_excel(\"data-geilo.xlsx\", sheet = \"Sales\")\n\n# Look at data: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# All sales where cocoa &gt; 0: \nsales[sales$cocoa &gt; 0,]\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Similarly with dplyr has a function for this: \nfilter(sales, cocoa &gt; 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# or even better: combine dplyr and magrittr: \nsales |&gt; \n  filter(cocoa &gt; 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Some governing principles in with dplyr/tidyverse functions: \n# \n#  - The first argument is a data frame.\n#  - The subsequent arguments describe what to do with the data frame. You \n#    can refer to columns in the data frame directly without using $.\n#  - The result is a new data frame\n\n# Sorting the data with dplyr: \nsales |&gt; \n  arrange(cocoa) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     5       NA      3     1     1\n6     7       NA      1     1     1\n\n# Sorting the data with several columns: \nsales |&gt; \n  arrange(cocoa, orange, swix) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     7       NA      1     1     1\n6    20       NA      1     1     2\n\n\nExercise: Report the top of the dataset, sorted by\n\ncocoa (increasing)\nswix (decreasing)\norange (decreasing)\n\nYou should obtain a result equal to the data frame below. Note we only show the first six entries of the result.\n\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21       42      5     0     7\n2    19        7      4     0     2\n3    12       10      3     0     0\n4    20       NA      1     1     2\n5     5       NA      3     1     1\n6     9       NA      2     1     1\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nsales |&gt; \n  arrange(cocoa, desc(swix), desc(orange)) |&gt; \n  head()\n\n# Alternatively: \n sales |&gt;\n   arrange(cocoa, -swix, -orange) |&gt;\n   head()\n \n# Note however that (-) requires num. vectors. desc can take e.g. factors as well.)\n\n\n\n\n\n\n\n\n\n\n# Selecting some variables: \nhead(sales[,c(\"swix\", \"cocoa\")])\n\nsales |&gt; \n  select(cocoa, swix) |&gt; \n  head()\n\n# create new variables: \nsales |&gt; \n  mutate(items = cocoa+swix+orange) |&gt; \n  head()\n\nsales |&gt; \n  transmute(\n    items = cocoa+swix+orange,\n    trans = trans) |&gt; \n  head()\n\n# Summarise data: \nsales |&gt; \n  summarise(sum_cocoa = sum(cocoa))\n\n# Assignment 1: how many items were sold in total?\n# Assignment 2: What was the max and min number of items bought by\n# the people that also bought cocoa?\n\nExercise:\n\nHow many items were sold in total?\nWhat is the min. and max. number of items purchased by customers that also bought at least one cocoa?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Exercise 1: \n\nsales |&gt; \n  transmute(items = cocoa + swix + orange) |&gt; \n  summarise(sum_items = sum(items))\n\n# Exercise 2: \nsales |&gt; \n  filter(cocoa&gt;0) |&gt; \n  transmute(\n    items = cocoa + swix + orange) |&gt; \n  summarise(\n    max_items = max(items),\n    min_items = min(items)\n  )\n\n\n\n\n\n\n\n\n\n\n# Compare the output from this command: \nsales\n\n# ..to this one: \nsales  |&gt; group_by(customer)\n\n# Note how we in the second command have added some meta-information on groups\n# to the data frame. Groups change the results we get when applying\n# functions to the data frame. See below: \n\nsales |&gt; \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\nsales |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\n# When we apply \"summarise\" to a data frame we are reducing it \n# to the summary statistics that we list in the call to the function. \n# In the command above, this is a sum and a count. There are many\n# such functions we can use - just keep in mind that the function\n# should return one item per group (or just one item if you don't\n# have groups). \n\n# Make sure you are aware of the difference between mutate and \n# summarise: mutate *adds* a variable to the data frame, \n# summarise aggregates it. Look at the difference below on \n# the result when we change from \"summarise\" to \"mutate\":\n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(\n    sum_orange = sum(orange),\n    no_transactions = n()\n  )\n\n# Note also that the tibble we get after the mutate is still \n# grouped by customer. That means that if we apply new\n# transformations to the tibble, dplyr-functions will in \n# general still obey the grouping. If you want to remove\n# the grouping you can do that by applying ungroup to the \n# data frame: \n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(\n    sum_orange = sum(orange),\n    no_transactions = n()\n  ) |&gt; \n  ungroup()\n\nExercise: How many cocoas were bought in total by customers who also bought more than two oranges? (Hint: How does mutate work on a grouped data frame?)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(sum.orange = sum(orange)) |&gt; \n  filter(sum.orange &gt; 2) |&gt; \n  ungroup() |&gt;\n  summarise(sum.cocoa = sum(cocoa))\n\n\n\n\n\n\n\nApplying a group_by to a data frame keeps the data frame grouped until either you have summarized the data frame or you apply ungroup to the data frame. This is important to be aware of, many of the Tidyverse-functions behave differently on grouped and ungrouped data sets, and it can be easy to forget to ungroup-it after it is no longer needed.\nRecently however, dplyr-functions support a separate .by-argument (or in some cases, by), that allows you to group the data frame directly in a function call instead of grouping in a separate step before. The grouping will then only apply to the called function, and doesn’t have to be undone with a ungroup afterwards.\nAs an example - two ways below of summing the number of cocoas bought by customer with more than two transactions. Both give the same results, but maybe in this case the .by-argument makes the code both more readable and condensed.\n\nsales |&gt;\n  group_by(customer) |&gt;\n  filter(n() &gt; 2) |&gt;\n  ungroup() |&gt;\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         &lt;dbl&gt;\n1           25\n\nsales |&gt;\n  filter(n() &gt; 2, .by = customer) |&gt;\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         &lt;dbl&gt;\n1           25\n\n\n\n\n\n\n\nFinally, we will join data frames. See the R4DS-book. Joins are necessary when we want to analyse two or more data sets jointly. In our simple example, we have two data sets: one with transactions, and one with customers:\n\n# Read in both sheets\nsales &lt;- read_excel(\"data-geilo.xlsx\", sheet=\"Sales\")\ncustomers &lt;- read_excel(\"data-geilo.xlsx\", sheet=\"Customers\")\n\n# See that sales stores transactions, as well as a link to a \n# customer number: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# In the \"customers\"-file we find info on each customer: \nhead(customers)\n\n# A tibble: 6 × 3\n  customer hotel    room\n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1        1 Vestlia   394\n2        2 Vestlia   489\n3        3 Holms     219\n4        4 Holms     155\n5        5 Holms     204\n6        6 Holms       6\n\n\ndplyr has six join functions, but of these left_join() and inner_join() are probably the most useful. Both functions take need two data frames as arguments, and joins them together into a single data frame. By default, the functions will match together rows based on column names that occur in both data sets.\n\nleft_join() returns all values from the first data frame, with all columns and values from the second data frame where there is a match between the two data sets. This means that we don’t keep any records from the second data set that are unmatched to the first data set.\ninner_join() keeps only records that are matched in both data sets.\n\nSee examples below, as well as the output produced.\n\nx &lt;- tibble(id=c(1,2,3))\ny &lt;- tibble(id=c(1,2,4), value=c(\"a\",\"b\",\"c\"))\n\nx |&gt; left_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n\nx |&gt; right_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     4 c    \n\nx |&gt; full_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n4     4 c    \n\nx |&gt; inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n\n# Filtering joins: \nx |&gt; semi_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 1\n     id\n  &lt;dbl&gt;\n1     1\n2     2\n\nx |&gt; anti_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 1 × 1\n     id\n  &lt;dbl&gt;\n1     3\n\n\nWhen joining, you should pay careful attention to the join conditions. In the joins above are equivalent to writing the join condition explicitly - which in the case below means that id from the first data set should be equal to id in the second data set to get a match. These join conditions may be exanded to join on e.g. inequalities and multiple columns with different names in the different data sets.\n\nx |&gt; left_join(y, by=join_by(id==id))\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n\n\nExercise: Use the dplyr join verbs to create the following four different results:\n\nA dataframe with transactions and customer info filled in\nA dataframe with transactions for all registered customers\nA dataframe with transactions for customers that are not registered\nA dataframe that combines all the information in both data sets\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# 1: \nsales |&gt; \n  left_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 7\n  trans customer orange cocoa  swix hotel    room\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     6        1      1     3     3 Vestlia   394\n2    18        1      1     3     1 Vestlia   394\n3     2        3      1     5     0 Holms     219\n4    15        3      3     3     0 Holms     219\n5    16        3      3     1     0 Holms     219\n6    17        5      0     5     1 Holms     204\n\n# 2: \nsales |&gt; \n  semi_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     6        1      1     3     3\n2    18        1      1     3     1\n3     2        3      1     5     0\n4    15        3      3     3     0\n5    16        3      3     1     0\n6    17        5      0     5     1\n\n# 3: (why does this give different res. than is.na()?)\nsales |&gt; \n  anti_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21       42      5     0     7\n2     4       NA      2     2     2\n3     5       NA      3     1     1\n4     7       NA      1     1     1\n5     8       NA      8     2     1\n6     9       NA      2     1     1\n\n# 4:\nfull &lt;- \n  sales  |&gt; \n  full_join(customers, by=join_by(customer==customer)) \n\n\n\n\n\n\n\nA word of caution on tidyverse: dplyr is somewhat contested among R-users. Some claim it is very easy to learn (although I’m not aware of any studies). Critics argue that it is slow compared to data.table. data.tablecan (sometimes) beat Python/Pandas in terms of speed not, but dplyr can not do that. Note also that the tidyverse ecosystem in general is continually being updated. This means that if you try to run the commands from this course on a fresh install of R and Tidyverse in a few years, it might not work unless you update the syntax.\nThink about your usage of programming: If it is occasional scripting, ad-hoc reports etc, then speed is often not important, and occasionally changing syntax might not be an issue.\nHowever, in my experience the tidyverse is significantly “faster” than alternatives for important use cases - which involve many exploratory analyses, data visualizations, and trying out many different ways of modelling a problem (i.e. usually what constrains your time may be how fast you can express what you want - not how long you need to wait for results).\nLet’s wrap up this chapter with a final exercise:\nExercise: Create a summary statistics with the following properties:\n\nCustomer on rows, with all customers as well as non-registered customers (non-registered can be in a single row)\nIn addition to customer numbers, four columns:\n\nNumber of transactions in total\nTotal sales of each type of item\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Why doesn't this work!?\n#(check e.g. customer nr 2!):\nfull |&gt;  \n  arrange(customer) |&gt; \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |&gt; \n  group_by(customer) |&gt; \n  summarise(count         = n(),\n            sale.orange   = sum(orange  , na.rm=T),\n            sale.cocoa    = sum(cocoa   , na.rm=T),\n            sale.swix     = sum(swix    , na.rm=T))\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                1           0          0         0\n 4 3                3           7          9         0\n 5 4                1           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                1           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10\n\n# A better way: \nfull |&gt; \n  arrange(customer) |&gt; \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    count         = sum(!is.na(trans)),\n    sale.orange   = sum(orange  , na.rm = T),\n    sale.cocoa    = sum(cocoa   , na.rm = T),\n    sale.swix     = sum(swix    , na.rm = T)\n  )\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                0           0          0         0\n 4 3                3           7          9         0\n 5 4                0           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                0           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#the-basics-of-dplyr",
    "href": "02-data-wrangling.html#the-basics-of-dplyr",
    "title": "2 Data wrangling",
    "section": "",
    "text": "First, we warm up with a first few functions from dplyr. You can download the data set here: data-geilo.xlsx.\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readxl)\n\n# Import data from Excel-file\nsales &lt;- read_excel(\"data-geilo.xlsx\", sheet = \"Sales\")\n\n# Look at data: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# All sales where cocoa &gt; 0: \nsales[sales$cocoa &gt; 0,]\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Similarly with dplyr has a function for this: \nfilter(sales, cocoa &gt; 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# or even better: combine dplyr and magrittr: \nsales |&gt; \n  filter(cocoa &gt; 0)\n\n# A tibble: 18 × 5\n   trans customer orange cocoa  swix\n   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1        6      1     1     1\n 2     2        3      1     5     0\n 3     3        9      0     2     1\n 4     4       NA      2     2     2\n 5     5       NA      3     1     1\n 6     6        1      1     3     3\n 7     7       NA      1     1     1\n 8     8       NA      8     2     1\n 9     9       NA      2     1     1\n10    10       10      2    14     0\n11    11       NA      3     2     2\n12    13       NA      2     2     0\n13    14       NA      3     4     0\n14    15        3      3     3     0\n15    16        3      3     1     0\n16    17        5      0     5     1\n17    18        1      1     3     1\n18    20       NA      1     1     2\n\n# Some governing principles in with dplyr/tidyverse functions: \n# \n#  - The first argument is a data frame.\n#  - The subsequent arguments describe what to do with the data frame. You \n#    can refer to columns in the data frame directly without using $.\n#  - The result is a new data frame\n\n# Sorting the data with dplyr: \nsales |&gt; \n  arrange(cocoa) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     5       NA      3     1     1\n6     7       NA      1     1     1\n\n# Sorting the data with several columns: \nsales |&gt; \n  arrange(cocoa, orange, swix) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12       10      3     0     0\n2    19        7      4     0     2\n3    21       42      5     0     7\n4     1        6      1     1     1\n5     7       NA      1     1     1\n6    20       NA      1     1     2\n\n\nExercise: Report the top of the dataset, sorted by\n\ncocoa (increasing)\nswix (decreasing)\norange (decreasing)\n\nYou should obtain a result equal to the data frame below. Note we only show the first six entries of the result.\n\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21       42      5     0     7\n2    19        7      4     0     2\n3    12       10      3     0     0\n4    20       NA      1     1     2\n5     5       NA      3     1     1\n6     9       NA      2     1     1\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nsales |&gt; \n  arrange(cocoa, desc(swix), desc(orange)) |&gt; \n  head()\n\n# Alternatively: \n sales |&gt;\n   arrange(cocoa, -swix, -orange) |&gt;\n   head()\n \n# Note however that (-) requires num. vectors. desc can take e.g. factors as well.)",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#selecting-and-creating-new-variables-and-summarising-a-data-frame",
    "href": "02-data-wrangling.html#selecting-and-creating-new-variables-and-summarising-a-data-frame",
    "title": "2 Data wrangling",
    "section": "",
    "text": "# Selecting some variables: \nhead(sales[,c(\"swix\", \"cocoa\")])\n\nsales |&gt; \n  select(cocoa, swix) |&gt; \n  head()\n\n# create new variables: \nsales |&gt; \n  mutate(items = cocoa+swix+orange) |&gt; \n  head()\n\nsales |&gt; \n  transmute(\n    items = cocoa+swix+orange,\n    trans = trans) |&gt; \n  head()\n\n# Summarise data: \nsales |&gt; \n  summarise(sum_cocoa = sum(cocoa))\n\n# Assignment 1: how many items were sold in total?\n# Assignment 2: What was the max and min number of items bought by\n# the people that also bought cocoa?\n\nExercise:\n\nHow many items were sold in total?\nWhat is the min. and max. number of items purchased by customers that also bought at least one cocoa?\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Exercise 1: \n\nsales |&gt; \n  transmute(items = cocoa + swix + orange) |&gt; \n  summarise(sum_items = sum(items))\n\n# Exercise 2: \nsales |&gt; \n  filter(cocoa&gt;0) |&gt; \n  transmute(\n    items = cocoa + swix + orange) |&gt; \n  summarise(\n    max_items = max(items),\n    min_items = min(items)\n  )",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#groups-and-summarise",
    "href": "02-data-wrangling.html#groups-and-summarise",
    "title": "2 Data wrangling",
    "section": "",
    "text": "# Compare the output from this command: \nsales\n\n# ..to this one: \nsales  |&gt; group_by(customer)\n\n# Note how we in the second command have added some meta-information on groups\n# to the data frame. Groups change the results we get when applying\n# functions to the data frame. See below: \n\nsales |&gt; \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\nsales |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    sum.orange = sum(orange),\n    no.trans = n()) \n\n# When we apply \"summarise\" to a data frame we are reducing it \n# to the summary statistics that we list in the call to the function. \n# In the command above, this is a sum and a count. There are many\n# such functions we can use - just keep in mind that the function\n# should return one item per group (or just one item if you don't\n# have groups). \n\n# Make sure you are aware of the difference between mutate and \n# summarise: mutate *adds* a variable to the data frame, \n# summarise aggregates it. Look at the difference below on \n# the result when we change from \"summarise\" to \"mutate\":\n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(\n    sum_orange = sum(orange),\n    no_transactions = n()\n  )\n\n# Note also that the tibble we get after the mutate is still \n# grouped by customer. That means that if we apply new\n# transformations to the tibble, dplyr-functions will in \n# general still obey the grouping. If you want to remove\n# the grouping you can do that by applying ungroup to the \n# data frame: \n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(\n    sum_orange = sum(orange),\n    no_transactions = n()\n  ) |&gt; \n  ungroup()\n\nExercise: How many cocoas were bought in total by customers who also bought more than two oranges? (Hint: How does mutate work on a grouped data frame?)\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nsales |&gt; \n  group_by(customer) |&gt; \n  mutate(sum.orange = sum(orange)) |&gt; \n  filter(sum.orange &gt; 2) |&gt; \n  ungroup() |&gt;\n  summarise(sum.cocoa = sum(cocoa))",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#persistent-or-temporary-group_by",
    "href": "02-data-wrangling.html#persistent-or-temporary-group_by",
    "title": "2 Data wrangling",
    "section": "",
    "text": "Applying a group_by to a data frame keeps the data frame grouped until either you have summarized the data frame or you apply ungroup to the data frame. This is important to be aware of, many of the Tidyverse-functions behave differently on grouped and ungrouped data sets, and it can be easy to forget to ungroup-it after it is no longer needed.\nRecently however, dplyr-functions support a separate .by-argument (or in some cases, by), that allows you to group the data frame directly in a function call instead of grouping in a separate step before. The grouping will then only apply to the called function, and doesn’t have to be undone with a ungroup afterwards.\nAs an example - two ways below of summing the number of cocoas bought by customer with more than two transactions. Both give the same results, but maybe in this case the .by-argument makes the code both more readable and condensed.\n\nsales |&gt;\n  group_by(customer) |&gt;\n  filter(n() &gt; 2) |&gt;\n  ungroup() |&gt;\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         &lt;dbl&gt;\n1           25\n\nsales |&gt;\n  filter(n() &gt; 2, .by = customer) |&gt;\n  summarise(sum(cocoa))\n\n# A tibble: 1 × 1\n  `sum(cocoa)`\n         &lt;dbl&gt;\n1           25",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#joining-data-frames",
    "href": "02-data-wrangling.html#joining-data-frames",
    "title": "2 Data wrangling",
    "section": "",
    "text": "Finally, we will join data frames. See the R4DS-book. Joins are necessary when we want to analyse two or more data sets jointly. In our simple example, we have two data sets: one with transactions, and one with customers:\n\n# Read in both sheets\nsales &lt;- read_excel(\"data-geilo.xlsx\", sheet=\"Sales\")\ncustomers &lt;- read_excel(\"data-geilo.xlsx\", sheet=\"Customers\")\n\n# See that sales stores transactions, as well as a link to a \n# customer number: \nhead(sales)\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1        6      1     1     1\n2     2        3      1     5     0\n3     3        9      0     2     1\n4     4       NA      2     2     2\n5     5       NA      3     1     1\n6     6        1      1     3     3\n\n# In the \"customers\"-file we find info on each customer: \nhead(customers)\n\n# A tibble: 6 × 3\n  customer hotel    room\n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1        1 Vestlia   394\n2        2 Vestlia   489\n3        3 Holms     219\n4        4 Holms     155\n5        5 Holms     204\n6        6 Holms       6\n\n\ndplyr has six join functions, but of these left_join() and inner_join() are probably the most useful. Both functions take need two data frames as arguments, and joins them together into a single data frame. By default, the functions will match together rows based on column names that occur in both data sets.\n\nleft_join() returns all values from the first data frame, with all columns and values from the second data frame where there is a match between the two data sets. This means that we don’t keep any records from the second data set that are unmatched to the first data set.\ninner_join() keeps only records that are matched in both data sets.\n\nSee examples below, as well as the output produced.\n\nx &lt;- tibble(id=c(1,2,3))\ny &lt;- tibble(id=c(1,2,4), value=c(\"a\",\"b\",\"c\"))\n\nx |&gt; left_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n\nx |&gt; right_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     4 c    \n\nx |&gt; full_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n4     4 c    \n\nx |&gt; inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n\n# Filtering joins: \nx |&gt; semi_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 1\n     id\n  &lt;dbl&gt;\n1     1\n2     2\n\nx |&gt; anti_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 1 × 1\n     id\n  &lt;dbl&gt;\n1     3\n\n\nWhen joining, you should pay careful attention to the join conditions. In the joins above are equivalent to writing the join condition explicitly - which in the case below means that id from the first data set should be equal to id in the second data set to get a match. These join conditions may be exanded to join on e.g. inequalities and multiple columns with different names in the different data sets.\n\nx |&gt; left_join(y, by=join_by(id==id))\n\n# A tibble: 3 × 2\n     id value\n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 &lt;NA&gt; \n\n\nExercise: Use the dplyr join verbs to create the following four different results:\n\nA dataframe with transactions and customer info filled in\nA dataframe with transactions for all registered customers\nA dataframe with transactions for customers that are not registered\nA dataframe that combines all the information in both data sets\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# 1: \nsales |&gt; \n  left_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 7\n  trans customer orange cocoa  swix hotel    room\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     6        1      1     3     3 Vestlia   394\n2    18        1      1     3     1 Vestlia   394\n3     2        3      1     5     0 Holms     219\n4    15        3      3     3     0 Holms     219\n5    16        3      3     1     0 Holms     219\n6    17        5      0     5     1 Holms     204\n\n# 2: \nsales |&gt; \n  semi_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     6        1      1     3     3\n2    18        1      1     3     1\n3     2        3      1     5     0\n4    15        3      3     3     0\n5    16        3      3     1     0\n6    17        5      0     5     1\n\n# 3: (why does this give different res. than is.na()?)\nsales |&gt; \n  anti_join(customers, by=join_by(customer==customer)) |&gt; \n  arrange(customer) |&gt; \n  head()\n\n# A tibble: 6 × 5\n  trans customer orange cocoa  swix\n  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    21       42      5     0     7\n2     4       NA      2     2     2\n3     5       NA      3     1     1\n4     7       NA      1     1     1\n5     8       NA      8     2     1\n6     9       NA      2     1     1\n\n# 4:\nfull &lt;- \n  sales  |&gt; \n  full_join(customers, by=join_by(customer==customer))",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "02-data-wrangling.html#wrapping-up",
    "href": "02-data-wrangling.html#wrapping-up",
    "title": "2 Data wrangling",
    "section": "",
    "text": "A word of caution on tidyverse: dplyr is somewhat contested among R-users. Some claim it is very easy to learn (although I’m not aware of any studies). Critics argue that it is slow compared to data.table. data.tablecan (sometimes) beat Python/Pandas in terms of speed not, but dplyr can not do that. Note also that the tidyverse ecosystem in general is continually being updated. This means that if you try to run the commands from this course on a fresh install of R and Tidyverse in a few years, it might not work unless you update the syntax.\nThink about your usage of programming: If it is occasional scripting, ad-hoc reports etc, then speed is often not important, and occasionally changing syntax might not be an issue.\nHowever, in my experience the tidyverse is significantly “faster” than alternatives for important use cases - which involve many exploratory analyses, data visualizations, and trying out many different ways of modelling a problem (i.e. usually what constrains your time may be how fast you can express what you want - not how long you need to wait for results).\nLet’s wrap up this chapter with a final exercise:\nExercise: Create a summary statistics with the following properties:\n\nCustomer on rows, with all customers as well as non-registered customers (non-registered can be in a single row)\nIn addition to customer numbers, four columns:\n\nNumber of transactions in total\nTotal sales of each type of item\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\n# Why doesn't this work!?\n#(check e.g. customer nr 2!):\nfull |&gt;  \n  arrange(customer) |&gt; \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |&gt; \n  group_by(customer) |&gt; \n  summarise(count         = n(),\n            sale.orange   = sum(orange  , na.rm=T),\n            sale.cocoa    = sum(cocoa   , na.rm=T),\n            sale.swix     = sum(swix    , na.rm=T))\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                1           0          0         0\n 4 3                3           7          9         0\n 5 4                1           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                1           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10\n\n# A better way: \nfull |&gt; \n  arrange(customer) |&gt; \n  mutate(customer=replace_na(as.character(customer), \"Unregistered\")) |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    count         = sum(!is.na(trans)),\n    sale.orange   = sum(orange  , na.rm = T),\n    sale.cocoa    = sum(cocoa   , na.rm = T),\n    sale.swix     = sum(swix    , na.rm = T)\n  )\n\n# A tibble: 12 × 5\n   customer     count sale.orange sale.cocoa sale.swix\n   &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 1                2           2          6         4\n 2 10               2           5         14         0\n 3 2                0           0          0         0\n 4 3                3           7          9         0\n 5 4                0           0          0         0\n 6 42               1           5          0         7\n 7 5                1           0          5         1\n 8 6                1           1          1         1\n 9 7                1           4          0         2\n10 8                0           0          0         0\n11 9                1           0          2         1\n12 Unregistered     9          25         16        10",
    "crumbs": [
      "Home",
      "PART 1",
      "2. Data Wrangling"
    ]
  },
  {
    "objectID": "07-iterations.html",
    "href": "07-iterations.html",
    "title": "7 Iterations in R",
    "section": "",
    "text": "In this section, we’ll discuss the contents of R4DS chapter 27. We have already covered writing loops. In this section we’ll expand on that topic using specialized functions for iterating over objects. These functions are perhaps slightly more abstract than explicit loops. However, by using them we can write scripts that may be easier to understand than the bulkiness we would have had with explicit loops.\nWe’ll first get accustomed to some of the purrr-functions in a few simple examples, before moving on to applying the functions on a more interesting data set.\n\n\n\n\nLets start with creating a data set with some random numbers:\n\nlibrary(purrr) \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;-\n  tibble(\n    a = rnorm(10),\n    b = rnorm(10),\n    c = rnorm(10),\n    d = rnorm(10),\n    e = rnorm(10)\n  )\n\nSay we would like to calculate the median over all the columns in the data set. We could of course do this by typing up the following\n\nmedian(df$a) \n\n[1] -0.1697745\n\nmedian(df$b) \n\n[1] 0.01155021\n\nmedian(df$c) \n\n[1] -0.162939\n\nmedian(df$d) \n\n[1] 0.7295159\n\nmedian(df$e)\n\n[1] 0.2412489\n\n\nIn addition to being verbose, we would need to make changes to these lines if we were to add or subtract columns to our data frame. And further, if we wanted different metrics (e.g. column means as well as medians), we would have to write even more code.\nWe can however write a function for iterating over columns. Consider the function below:\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\n\nNote a few items:\n\nThe loop iterates over seq_along(df), which in our case evaluates to 1,2,3,4,5. This is slightly more robust than iterating over 1:ncols(df), as also works for edge-cases where there are no columns in the data set.\nThe second argument is a function name. The function applies the function given to each of the columns\n\nWe can now call this function a few times, using different summary functions:\n\ncol_summary(df, mean)\n\n[1] -0.021238256  0.004991739 -0.264287354  0.734379068  0.258744700\n\ncol_summary(df, median)\n\n[1] -0.16977448  0.01155021 -0.16293895  0.72951592  0.24124891\n\ncol_summary(df, sd)\n\n[1] 0.7436039 0.7382673 0.5574406 0.9227294 0.7181865\n\n\nWith this method we can get summaries of all columns in the data frame, without needing to changing these lines if we add or subtract columns.\n\n\n\nThe package purrr comes with functions specifically designed for iterations. The example above could be solved with the function map. See how this also preserves the names of the columns\n\nmap(df, mean)\n\n$a\n[1] -0.02123826\n\n$b\n[1] 0.004991739\n\n$c\n[1] -0.2642874\n\n$d\n[1] 0.7343791\n\n$e\n[1] 0.2587447\n\n\nThe output from map-functions is a list. If you expect the return from the map call to be boolan, integer, double or character vectors, you can ensure you do indeed get such a vector in return by using map_lgl, map_int, map_dbl or map_chr-respectively.\nIf we want the results as e.g. a data frame, we can combine them using an appropriate function - below all the column means are combined into a data frame with one row. Not also how we can add arguments to the function applied by map by adding arguments to the map-call:\n\ndf |&gt; \n  map(mean, trim=.1) |&gt; \n  bind_cols()\n\n# A tibble: 1 × 5\n        a       b      c     d     e\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0444 0.00318 -0.241 0.683 0.202\n\n\nWhen we call map, we need to supply a single function that will be applied to each of the objects we are iterating over. If we want to apply multiple functions at the same time, we could store this as a new function, and call this once. However, we can also call an anonymous function (see also the “Function” chapter of the compendium”):\n\ndf |&gt;\n  map(\n    {\n      \\(x) mean(x) / sd(x)\n    }\n  ) |&gt; \n  bind_cols()\n\n# A tibble: 1 × 5\n        a       b      c     d     e\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0286 0.00676 -0.474 0.796 0.360\n\n\nWith map functions we can do a lot more interesting stuff than making column summaries. Run the code below yourself line by line. In the example below, we\n\nUse a built-in data frame with cars\nSplit the data set into a list of data frames, split by the values of the cyl-column\nApply a linear regression model to each of the data frames separately\nSummarize each of the regression models\nExtract the R^2-statistic from each regression model\nCombine the R^2-values into a data frame\n\n\nmtcars |&gt;                       \n  split( ~ cyl) |&gt;              \n  map({\n    \\(x)lm(mpg ~ wt, data = x)\n  }) |&gt;\n  map(summary) |&gt;\n  map({\n    \\(x) x$r.squared\n  }) |&gt;\n  bind_cols()\n\n# A tibble: 1 × 3\n    `4`   `6`   `8`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.509 0.465 0.423\n\n\nThe lambda function \\(x) x$r.squared returns named element r.squared in x. This is a common operation, so the map-function can also be supplied with a character string instead of a function. The example below does the same as the example below, but with a name-argument instead of a function:\n\nmtcars |&gt;  \n  split(~cyl) |&gt; \n  map({\\(x)lm(mpg~wt, data=x) }) |&gt; \n  map(summary) |&gt; \n  map(\"r.squared\")|&gt;\n  bind_cols()\n\n# A tibble: 1 × 3\n    `4`   `6`   `8`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.509 0.465 0.423\n\n\nThe map-functions also support integer values, in which case e.g. map(x,2) returns the second value of each item of x.\n\n\n\nThe map-functions will generally give an error if they receive an error when applied to any of the items it is applied to. In the example below we get an error due to a character-entry in the list. This might seem frustrating, but it forces you to actively make a choice of how you want to deal with the errors.\n\nx &lt;- list(1, 10, \"a\")\ny &lt;- x |&gt; map(log)\n\nError in `map()`:\nℹ In index: 3.\nCaused by error:\n! non-numeric argument to mathematical function\n\n\nWe can wrap the function call in the function safely. This may be a useful tool when you are developing code, as it lets you see which records caused errors:\n\ny &lt;- x |&gt; map(safely(log))\n\ny |&gt; \n  map(\"error\") |&gt; \n  map(is_null)\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] FALSE\n\n\npossibly is a different option. This function will insert a default value (NA_real below) in case the function call fails.\n\nx |&gt; map(possibly(log, NA_real_))\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 2.302585\n\n[[3]]\n[1] NA\n\n\n\n\n\nIf we want to iterate over two lists (of the same length) simultaneously, we can do so by using map2:\n\nmu &lt;- list(-10000, 0, 10000)\nsigma &lt;- list(1, 5, 10)\nmap2(mu,sigma,rnorm,n=5)\n\n[[1]]\n[1] -10000.734 -10001.251 -10000.081  -9998.925  -9999.689\n\n[[2]]\n[1] -2.4451539  4.0080684  4.7973024 -6.9174788 -0.2422151\n\n[[3]]\n[1]  9984.647  9988.328 10010.705  9984.966 10001.435\n\n\nWe can also iterate over more than two lists simultaneously. The lists then need to be combined in a single list, and the names of each list must match the names of the arguments in the function we want to apply.\n\nmu    &lt;- list(-10000, 0, 100)\nsigma &lt;- list(     1, 5,  10)\nn     &lt;- list(     1, 10, 25)\n\nlist(mean = mu,\n     sd = sigma,\n     n = n) |&gt; \n  pmap(rnorm)\n\n[[1]]\n[1] -9999.763\n\n[[2]]\n [1]  -6.6822076   3.9995616   4.4973030  -3.0816987 -12.8640992  -0.9532343\n [7]   2.7074262   6.4620594  11.8810861  -9.7703983\n\n[[3]]\n [1]  85.67133 102.87778  98.37155  96.94958 107.88470  97.28449 103.62930\n [8]  85.21948 105.17261 108.83569  88.40065 106.67600 111.86420 101.47729\n[15] 102.09363  95.12237  92.99952  93.43436 101.22989 111.59884  95.57329\n[22] 107.62195 115.82183 102.48285 102.45011\n\n\n\n\n\n\n\n\nLet’s apply the purrr-functions on a more realistic data set. We will play with an API from Vegvesenet, the Norwegian governmental road authority. Vegvesenet has an API we can query for data on traffic volumes at many sensor stations in Norway.\nThe API uses graphQL for requests. Let’s define a function where we can submit queries to an external API (we will not spend time discussing the query language or the API any further..).\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(DescTools)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rlang)\nlibrary(lubridate)\nlibrary(anytime)\n\nGQL &lt;- function(query,\n                ...,\n                .token = NULL,\n                .variables = NULL,\n                .operationName = NULL,\n                .url = url) {\n  pbody &lt;-\n    list(query = query,\n         variables = .variables,\n         operationName = .operationName)\n  if (is.null(.token)) {\n    res &lt;- POST(.url, body = pbody, encode = \"json\", ...)\n  } else {\n    auth_header &lt;- paste(\"bearer\", .token)\n    res &lt;-\n      POST(\n        .url,\n        body = pbody,\n        encode = \"json\",\n        add_headers(Authorization = auth_header),\n        ...\n      )\n  }\n  res &lt;- content(res, as = \"parsed\", encoding = \"UTF-8\")\n  if (!is.null(res$errors)) {\n    warning(toJSON(res$errors))\n  }\n  res$data\n}\n\n# The URL we will use is stored below: \nurl &lt;- \"https://www.vegvesen.no/trafikkdata/api/\"\n\n\n# Let's figure out which sensor stations that are operable. \n# The query below extracts all the stations, with a date for \n# when the station was in operation as well as a long/latitude. \nqry &lt;-\n  '\n{\n    trafficRegistrationPoints {\n        id\n        name\n        latestData {\n            volumeByDay\n        }\n        location {\n            coordinates {\n                latLon {\n                    lat\n                    lon\n                }\n            }\n        }\n    }\n}\n'\n\n# Allright - let's try submitting the query: \nstations &lt;-GQL(qry)\n\nWe now have the a long list in memory - 14mb! - with just a little information on each station. We can note that this is a list, not a dataframe. For our purposes, it would be better if the list was instead a data frame, with one row pr. sensor station.\nNote that the list itself only contains one entry:\n\nlength(stations)\n\n[1] 1\n\n\n…however, this first entry contains 6436 data points. You might get a slightly different answer, as Vegvesenet is changing the number of sensors. Note that when we subset a list, using [[i]] selects the contents of the item i, and [i] is a list.\n\nlength(stations[[1]])\n\n[1] 7895\n\n\n\n\n\nLet’s look at the first entry of this long list. We can see there is a station ID, station name, date time of latest recording from the station and coordinates. This looks like something that could fit well within a data frame, with columns id, name, latestdata, lat, and lon. The question is how! You might want to refer to chapter 24 of R4DS for more on hierarchical data.\n\nstations[[1]][[1]]\n\n$id\n[1] \"52742V2282262\"\n\n$name\n[1] \"ØRBEKK EV6\"\n\n$latestData\n$latestData$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n$location\n$location$coordinates\n$location$coordinates$latLon\n$location$coordinates$latLon$lat\n[1] 60.41426\n\n$location$coordinates$latLon$lon\n[1] 11.24117\n\n\nWe could perhaps hope that we can force this list into a data frame. For this we will use as_tibble:\n\nstations[[1]][[1]] |&gt;  \n  as_tibble()\n\n# A tibble: 1 × 4\n  id            name       latestData   location        \n  &lt;chr&gt;         &lt;chr&gt;      &lt;named list&gt; &lt;named list&gt;    \n1 52742V2282262 ØRBEKK EV6 &lt;chr [1]&gt;    &lt;named list [1]&gt;\n\n\nExercise:\nWe now want to apply this as_tibble transformation to each of the stations, and combine them in a single data frame. Transform the list into a data frame, with at id and name as columns, and one row per station. We can fix the date time and locations columns later, but use one of the map-functions from purrr.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nUsing the map-function we traverse all the entries in the stations list, and transform these lists to data frames. We can then combine the list of data frames into a single data frame by calling list_rbind.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind()\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nThere is still some work left to do with the date time and location columns. As you can see below, they are still in a list format. We can try to pull out the insides of the contents of the latestData-column. It is formatted as a list, but actually only contains one date time entry.\n\nstations[[1]] |&gt;  \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  head(1) |&gt;  \n  select(latestData) |&gt;  \n  pull()\n\n$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n\n\n\nExercise:\nMutate the contents of the latestData-columns, such that it is in a character format. You don’t have to format it to a proper date time (yet..). This task has two complications: one is to apply transformation to all entries of the list - another is to deal with missing values - that might cause errors.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nWe can use map_chr. This function will try its best to return a character vector (other variants support different return types).Below, we are asking map_chr to return the first item of each sub list in latestData. However, this will fail if it meets an entry that does not have anything stored under latestdata!\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt;\n  mutate(latestData = map_chr(latestData, 1))\n\nError in `mutate()`:\nℹ In argument: `latestData = map_chr(latestData, 1)`.\nCaused by error in `map_chr()`:\nℹ In index: 481.\nℹ With name: volumeByDay.\nCaused by error:\n! Result must be length 1, not 0.\n\n\nWe could write a custom “unlisting”-function. The function below unlists the elements of latestData - if there are any elements there. If it the content is null, the function just returns an empty character string.\n\nunlist_safe &lt;- \n  function(x){\n    x &lt;- unlist(x)\n    if(is.null(x)){\n      return(NA_character_)\n  }else{\n    return(x)\n  }\n}\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, unlist_safe))\n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nAlternatively, we can use the defaults in map_chr. It will now have a safe fallback value it can use if it doesn’t find the element we are looking for in latestData. A simple solution is to use the .default-argument, and set this to missing:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(\n    latestData = map_chr(latestData,1, .default=NA_character_)\n  ) \n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\n\n\n\n\n\n\nNext, let’s format the date format. Date formats can be tricky, but is an obstacle you just have to learn to work with. We can reformat the latestData column into a date by simply using as.Date - however - we now have lost information on the time of day. Let’s see if we can retain all the information in the column.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default=NA_character_)) |&gt; \n  mutate(latestData = as.Date(latestData))\n\n# A tibble: 7,895 × 4\n   id            name            latestData location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;date&gt;     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nThere are several functions we can use to transform the string into a date time variable. as_datetime in lubridate works in this case. Note that the interpretation of dates may be dependent on the time zone settings on your laptop. Here, we are explicitly stating that we want the a Europe/Berlin tz on the variable:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) \n\n# A tibble: 7,895 × 4\n   id            name            latestData          location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nExercise: Finalizing the transformation\nLet’s take on the final location variable. Complete the operation by unpacking the location column into two columns: lat and lon. You may use the functions you have already seen, or see of you can find mode specialized functions.\nNote: This a nested list i.e. the contents of a cell in location is a list with one entry. This list contains two other lists..\nThe script should return a data frame similar to the one below (only the first few entries shown).\n\n\n\n\n\nid\nname\nlatestData\nlat\nlon\n\n\n\n\n52742V2282262\nØRBEKK EV6\n2024-08-23\n60.41426\n11.241171\n\n\n41517V704478\nBASTERUD\n2017-11-14\n60.76916\n11.171156\n\n\n01050V1126115\nTrollvika\n2024-08-23\n69.23921\n17.981692\n\n\n24748V22148\nPinesund\n2024-08-23\n58.79845\n9.054728\n\n\n11446V1175840\nMelsomvik\n2024-08-23\n59.22947\n10.338938\n\n\n99015V249528\nHjelset øst\n2023-05-09\n62.78733\n7.521145\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nWe can use a similar solution we used before. First we use unlist to remove one level from the list, and then extract the contents using map_dbl - remember these are numbers, not text.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = \"\"))  |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  mutate(location = map(location, unlist)) |&gt;  \n  mutate(\n    lat = map_dbl(location, \"latLon.lat\"),\n    lon = map_dbl(location, \"latLon.lon\")\n  ) %&gt;% \n  select(-location)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n\nAlternatively, we can use unnest_wider twice. This one does some work for us, and gives the same result:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  unnest_wider(location) |&gt; \n  unnest_wider(latLon)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows",
    "crumbs": [
      "Home",
      "PART 2",
      "7. Iterations"
    ]
  },
  {
    "objectID": "07-iterations.html#introductory-examples",
    "href": "07-iterations.html#introductory-examples",
    "title": "7 Iterations in R",
    "section": "",
    "text": "Lets start with creating a data set with some random numbers:\n\nlibrary(purrr) \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;-\n  tibble(\n    a = rnorm(10),\n    b = rnorm(10),\n    c = rnorm(10),\n    d = rnorm(10),\n    e = rnorm(10)\n  )\n\nSay we would like to calculate the median over all the columns in the data set. We could of course do this by typing up the following\n\nmedian(df$a) \n\n[1] -0.1697745\n\nmedian(df$b) \n\n[1] 0.01155021\n\nmedian(df$c) \n\n[1] -0.162939\n\nmedian(df$d) \n\n[1] 0.7295159\n\nmedian(df$e)\n\n[1] 0.2412489\n\n\nIn addition to being verbose, we would need to make changes to these lines if we were to add or subtract columns to our data frame. And further, if we wanted different metrics (e.g. column means as well as medians), we would have to write even more code.\nWe can however write a function for iterating over columns. Consider the function below:\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\n\nNote a few items:\n\nThe loop iterates over seq_along(df), which in our case evaluates to 1,2,3,4,5. This is slightly more robust than iterating over 1:ncols(df), as also works for edge-cases where there are no columns in the data set.\nThe second argument is a function name. The function applies the function given to each of the columns\n\nWe can now call this function a few times, using different summary functions:\n\ncol_summary(df, mean)\n\n[1] -0.021238256  0.004991739 -0.264287354  0.734379068  0.258744700\n\ncol_summary(df, median)\n\n[1] -0.16977448  0.01155021 -0.16293895  0.72951592  0.24124891\n\ncol_summary(df, sd)\n\n[1] 0.7436039 0.7382673 0.5574406 0.9227294 0.7181865\n\n\nWith this method we can get summaries of all columns in the data frame, without needing to changing these lines if we add or subtract columns.\n\n\n\nThe package purrr comes with functions specifically designed for iterations. The example above could be solved with the function map. See how this also preserves the names of the columns\n\nmap(df, mean)\n\n$a\n[1] -0.02123826\n\n$b\n[1] 0.004991739\n\n$c\n[1] -0.2642874\n\n$d\n[1] 0.7343791\n\n$e\n[1] 0.2587447\n\n\nThe output from map-functions is a list. If you expect the return from the map call to be boolan, integer, double or character vectors, you can ensure you do indeed get such a vector in return by using map_lgl, map_int, map_dbl or map_chr-respectively.\nIf we want the results as e.g. a data frame, we can combine them using an appropriate function - below all the column means are combined into a data frame with one row. Not also how we can add arguments to the function applied by map by adding arguments to the map-call:\n\ndf |&gt; \n  map(mean, trim=.1) |&gt; \n  bind_cols()\n\n# A tibble: 1 × 5\n        a       b      c     d     e\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0444 0.00318 -0.241 0.683 0.202\n\n\nWhen we call map, we need to supply a single function that will be applied to each of the objects we are iterating over. If we want to apply multiple functions at the same time, we could store this as a new function, and call this once. However, we can also call an anonymous function (see also the “Function” chapter of the compendium”):\n\ndf |&gt;\n  map(\n    {\n      \\(x) mean(x) / sd(x)\n    }\n  ) |&gt; \n  bind_cols()\n\n# A tibble: 1 × 5\n        a       b      c     d     e\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.0286 0.00676 -0.474 0.796 0.360\n\n\nWith map functions we can do a lot more interesting stuff than making column summaries. Run the code below yourself line by line. In the example below, we\n\nUse a built-in data frame with cars\nSplit the data set into a list of data frames, split by the values of the cyl-column\nApply a linear regression model to each of the data frames separately\nSummarize each of the regression models\nExtract the R^2-statistic from each regression model\nCombine the R^2-values into a data frame\n\n\nmtcars |&gt;                       \n  split( ~ cyl) |&gt;              \n  map({\n    \\(x)lm(mpg ~ wt, data = x)\n  }) |&gt;\n  map(summary) |&gt;\n  map({\n    \\(x) x$r.squared\n  }) |&gt;\n  bind_cols()\n\n# A tibble: 1 × 3\n    `4`   `6`   `8`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.509 0.465 0.423\n\n\nThe lambda function \\(x) x$r.squared returns named element r.squared in x. This is a common operation, so the map-function can also be supplied with a character string instead of a function. The example below does the same as the example below, but with a name-argument instead of a function:\n\nmtcars |&gt;  \n  split(~cyl) |&gt; \n  map({\\(x)lm(mpg~wt, data=x) }) |&gt; \n  map(summary) |&gt; \n  map(\"r.squared\")|&gt;\n  bind_cols()\n\n# A tibble: 1 × 3\n    `4`   `6`   `8`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.509 0.465 0.423\n\n\nThe map-functions also support integer values, in which case e.g. map(x,2) returns the second value of each item of x.\n\n\n\nThe map-functions will generally give an error if they receive an error when applied to any of the items it is applied to. In the example below we get an error due to a character-entry in the list. This might seem frustrating, but it forces you to actively make a choice of how you want to deal with the errors.\n\nx &lt;- list(1, 10, \"a\")\ny &lt;- x |&gt; map(log)\n\nError in `map()`:\nℹ In index: 3.\nCaused by error:\n! non-numeric argument to mathematical function\n\n\nWe can wrap the function call in the function safely. This may be a useful tool when you are developing code, as it lets you see which records caused errors:\n\ny &lt;- x |&gt; map(safely(log))\n\ny |&gt; \n  map(\"error\") |&gt; \n  map(is_null)\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] FALSE\n\n\npossibly is a different option. This function will insert a default value (NA_real below) in case the function call fails.\n\nx |&gt; map(possibly(log, NA_real_))\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 2.302585\n\n[[3]]\n[1] NA\n\n\n\n\n\nIf we want to iterate over two lists (of the same length) simultaneously, we can do so by using map2:\n\nmu &lt;- list(-10000, 0, 10000)\nsigma &lt;- list(1, 5, 10)\nmap2(mu,sigma,rnorm,n=5)\n\n[[1]]\n[1] -10000.734 -10001.251 -10000.081  -9998.925  -9999.689\n\n[[2]]\n[1] -2.4451539  4.0080684  4.7973024 -6.9174788 -0.2422151\n\n[[3]]\n[1]  9984.647  9988.328 10010.705  9984.966 10001.435\n\n\nWe can also iterate over more than two lists simultaneously. The lists then need to be combined in a single list, and the names of each list must match the names of the arguments in the function we want to apply.\n\nmu    &lt;- list(-10000, 0, 100)\nsigma &lt;- list(     1, 5,  10)\nn     &lt;- list(     1, 10, 25)\n\nlist(mean = mu,\n     sd = sigma,\n     n = n) |&gt; \n  pmap(rnorm)\n\n[[1]]\n[1] -9999.763\n\n[[2]]\n [1]  -6.6822076   3.9995616   4.4973030  -3.0816987 -12.8640992  -0.9532343\n [7]   2.7074262   6.4620594  11.8810861  -9.7703983\n\n[[3]]\n [1]  85.67133 102.87778  98.37155  96.94958 107.88470  97.28449 103.62930\n [8]  85.21948 105.17261 108.83569  88.40065 106.67600 111.86420 101.47729\n[15] 102.09363  95.12237  92.99952  93.43436 101.22989 111.59884  95.57329\n[22] 107.62195 115.82183 102.48285 102.45011",
    "crumbs": [
      "Home",
      "PART 2",
      "7. Iterations"
    ]
  },
  {
    "objectID": "07-iterations.html#purrr-and-traffic-data",
    "href": "07-iterations.html#purrr-and-traffic-data",
    "title": "7 Iterations in R",
    "section": "",
    "text": "Let’s apply the purrr-functions on a more realistic data set. We will play with an API from Vegvesenet, the Norwegian governmental road authority. Vegvesenet has an API we can query for data on traffic volumes at many sensor stations in Norway.\nThe API uses graphQL for requests. Let’s define a function where we can submit queries to an external API (we will not spend time discussing the query language or the API any further..).\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(DescTools)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(rlang)\nlibrary(lubridate)\nlibrary(anytime)\n\nGQL &lt;- function(query,\n                ...,\n                .token = NULL,\n                .variables = NULL,\n                .operationName = NULL,\n                .url = url) {\n  pbody &lt;-\n    list(query = query,\n         variables = .variables,\n         operationName = .operationName)\n  if (is.null(.token)) {\n    res &lt;- POST(.url, body = pbody, encode = \"json\", ...)\n  } else {\n    auth_header &lt;- paste(\"bearer\", .token)\n    res &lt;-\n      POST(\n        .url,\n        body = pbody,\n        encode = \"json\",\n        add_headers(Authorization = auth_header),\n        ...\n      )\n  }\n  res &lt;- content(res, as = \"parsed\", encoding = \"UTF-8\")\n  if (!is.null(res$errors)) {\n    warning(toJSON(res$errors))\n  }\n  res$data\n}\n\n# The URL we will use is stored below: \nurl &lt;- \"https://www.vegvesen.no/trafikkdata/api/\"\n\n\n# Let's figure out which sensor stations that are operable. \n# The query below extracts all the stations, with a date for \n# when the station was in operation as well as a long/latitude. \nqry &lt;-\n  '\n{\n    trafficRegistrationPoints {\n        id\n        name\n        latestData {\n            volumeByDay\n        }\n        location {\n            coordinates {\n                latLon {\n                    lat\n                    lon\n                }\n            }\n        }\n    }\n}\n'\n\n# Allright - let's try submitting the query: \nstations &lt;-GQL(qry)\n\nWe now have the a long list in memory - 14mb! - with just a little information on each station. We can note that this is a list, not a dataframe. For our purposes, it would be better if the list was instead a data frame, with one row pr. sensor station.\nNote that the list itself only contains one entry:\n\nlength(stations)\n\n[1] 1\n\n\n…however, this first entry contains 6436 data points. You might get a slightly different answer, as Vegvesenet is changing the number of sensors. Note that when we subset a list, using [[i]] selects the contents of the item i, and [i] is a list.\n\nlength(stations[[1]])\n\n[1] 7895\n\n\n\n\n\nLet’s look at the first entry of this long list. We can see there is a station ID, station name, date time of latest recording from the station and coordinates. This looks like something that could fit well within a data frame, with columns id, name, latestdata, lat, and lon. The question is how! You might want to refer to chapter 24 of R4DS for more on hierarchical data.\n\nstations[[1]][[1]]\n\n$id\n[1] \"52742V2282262\"\n\n$name\n[1] \"ØRBEKK EV6\"\n\n$latestData\n$latestData$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n$location\n$location$coordinates\n$location$coordinates$latLon\n$location$coordinates$latLon$lat\n[1] 60.41426\n\n$location$coordinates$latLon$lon\n[1] 11.24117\n\n\nWe could perhaps hope that we can force this list into a data frame. For this we will use as_tibble:\n\nstations[[1]][[1]] |&gt;  \n  as_tibble()\n\n# A tibble: 1 × 4\n  id            name       latestData   location        \n  &lt;chr&gt;         &lt;chr&gt;      &lt;named list&gt; &lt;named list&gt;    \n1 52742V2282262 ØRBEKK EV6 &lt;chr [1]&gt;    &lt;named list [1]&gt;\n\n\nExercise:\nWe now want to apply this as_tibble transformation to each of the stations, and combine them in a single data frame. Transform the list into a data frame, with at id and name as columns, and one row per station. We can fix the date time and locations columns later, but use one of the map-functions from purrr.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nUsing the map-function we traverse all the entries in the stations list, and transform these lists to data frames. We can then combine the list of data frames into a single data frame by calling list_rbind.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind()\n\n# A tibble: 7,895 × 4\n   id            name            latestData   location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;named list&gt; &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 7 78755V249572  Sogge           &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord &lt;chr [1]&gt;    &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        &lt;chr [1]&gt;    &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   &lt;chr [1]&gt;    &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nThere is still some work left to do with the date time and location columns. As you can see below, they are still in a list format. We can try to pull out the insides of the contents of the latestData-column. It is formatted as a list, but actually only contains one date time entry.\n\nstations[[1]] |&gt;  \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  head(1) |&gt;  \n  select(latestData) |&gt;  \n  pull()\n\n$volumeByDay\n[1] \"2024-08-23T00:00:00+02:00\"\n\n\n\n\n\nExercise:\nMutate the contents of the latestData-columns, such that it is in a character format. You don’t have to format it to a proper date time (yet..). This task has two complications: one is to apply transformation to all entries of the list - another is to deal with missing values - that might cause errors.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nWe can use map_chr. This function will try its best to return a character vector (other variants support different return types).Below, we are asking map_chr to return the first item of each sub list in latestData. However, this will fail if it meets an entry that does not have anything stored under latestdata!\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt;\n  mutate(latestData = map_chr(latestData, 1))\n\nError in `mutate()`:\nℹ In argument: `latestData = map_chr(latestData, 1)`.\nCaused by error in `map_chr()`:\nℹ In index: 481.\nℹ With name: volumeByDay.\nCaused by error:\n! Result must be length 1, not 0.\n\n\nWe could write a custom “unlisting”-function. The function below unlists the elements of latestData - if there are any elements there. If it the content is null, the function just returns an empty character string.\n\nunlist_safe &lt;- \n  function(x){\n    x &lt;- unlist(x)\n    if(is.null(x)){\n      return(NA_character_)\n  }else{\n    return(x)\n  }\n}\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, unlist_safe))\n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nAlternatively, we can use the defaults in map_chr. It will now have a safe fallback value it can use if it doesn’t find the element we are looking for in latestData. A simple solution is to use the .default-argument, and set this to missing:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(\n    latestData = map_chr(latestData,1, .default=NA_character_)\n  ) \n\n# A tibble: 7,895 × 4\n   id            name            latestData                location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;chr&gt;                     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14T00:00:00+01:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09T00:00:00+02:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20T00:00:00+02:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23T00:00:00+02:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\n\n\n\n\n\n\nNext, let’s format the date format. Date formats can be tricky, but is an obstacle you just have to learn to work with. We can reformat the latestData column into a date by simply using as.Date - however - we now have lost information on the time of day. Let’s see if we can retain all the information in the column.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default=NA_character_)) |&gt; \n  mutate(latestData = as.Date(latestData))\n\n# A tibble: 7,895 × 4\n   id            name            latestData location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;date&gt;     &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nThere are several functions we can use to transform the string into a date time variable. as_datetime in lubridate works in this case. Note that the interpretation of dates may be dependent on the time zone settings on your laptop. Here, we are explicitly stating that we want the a Europe/Berlin tz on the variable:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\")) \n\n# A tibble: 7,895 × 4\n   id            name            latestData          location        \n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;named list&gt;    \n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00 &lt;named list [1]&gt;\n 2 41517V704478  BASTERUD        2017-11-14 00:00:00 &lt;named list [1]&gt;\n 3 01050V1126115 Trollvika       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 4 24748V22148   Pinesund        2024-08-23 00:00:00 &lt;named list [1]&gt;\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00 &lt;named list [1]&gt;\n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00 &lt;named list [1]&gt;\n 7 78755V249572  Sogge           2024-08-23 00:00:00 &lt;named list [1]&gt;\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00 &lt;named list [1]&gt;\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00 &lt;named list [1]&gt;\n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00 &lt;named list [1]&gt;\n# ℹ 7,885 more rows\n\n\nExercise: Finalizing the transformation\nLet’s take on the final location variable. Complete the operation by unpacking the location column into two columns: lat and lon. You may use the functions you have already seen, or see of you can find mode specialized functions.\nNote: This a nested list i.e. the contents of a cell in location is a list with one entry. This list contains two other lists..\nThe script should return a data frame similar to the one below (only the first few entries shown).\n\n\n\n\n\nid\nname\nlatestData\nlat\nlon\n\n\n\n\n52742V2282262\nØRBEKK EV6\n2024-08-23\n60.41426\n11.241171\n\n\n41517V704478\nBASTERUD\n2017-11-14\n60.76916\n11.171156\n\n\n01050V1126115\nTrollvika\n2024-08-23\n69.23921\n17.981692\n\n\n24748V22148\nPinesund\n2024-08-23\n58.79845\n9.054728\n\n\n11446V1175840\nMelsomvik\n2024-08-23\n59.22947\n10.338938\n\n\n99015V249528\nHjelset øst\n2023-05-09\n62.78733\n7.521145\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\nWe can use a similar solution we used before. First we use unlist to remove one level from the list, and then extract the contents using map_dbl - remember these are numbers, not text.\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = \"\"))  |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  mutate(location = map(location, unlist)) |&gt;  \n  mutate(\n    lat = map_dbl(location, \"latLon.lat\"),\n    lon = map_dbl(location, \"latLon.lon\")\n  ) %&gt;% \n  select(-location)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows\n\n\nAlternatively, we can use unnest_wider twice. This one does some work for us, and gives the same result:\n\nstations[[1]] |&gt; \n  map(as_tibble) |&gt; \n  list_rbind() |&gt; \n  mutate(latestData = map_chr(latestData, 1, .default = NA_character_)) |&gt; \n  mutate(latestData = as_datetime(latestData, tz = \"Europe/Berlin\"))  |&gt; \n  unnest_wider(location) |&gt; \n  unnest_wider(latLon)\n\n# A tibble: 7,895 × 5\n   id            name            latestData            lat   lon\n   &lt;chr&gt;         &lt;chr&gt;           &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 52742V2282262 ØRBEKK EV6      2024-08-23 00:00:00  60.4 11.2 \n 2 41517V704478  BASTERUD        2017-11-14 00:00:00  60.8 11.2 \n 3 01050V1126115 Trollvika       2024-08-23 00:00:00  69.2 18.0 \n 4 24748V22148   Pinesund        2024-08-23 00:00:00  58.8  9.05\n 5 11446V1175840 Melsomvik       2024-08-23 00:00:00  59.2 10.3 \n 6 99015V249528  Hjelset øst     2023-05-09 00:00:00  62.8  7.52\n 7 78755V249572  Sogge           2024-08-23 00:00:00  62.5  7.71\n 8 83652V319725  Strandgata nord 2024-06-20 00:00:00  58.9  5.74\n 9 51934V2523546 SNARUD N        2024-08-23 00:00:00  60.8 11.0 \n10 65271V443150  Vestby syd ny   2024-08-23 00:00:00  59.6 10.7 \n# ℹ 7,885 more rows",
    "crumbs": [
      "Home",
      "PART 2",
      "7. Iterations"
    ]
  },
  {
    "objectID": "03-graphics.html",
    "href": "03-graphics.html",
    "title": "3 Graphics",
    "section": "",
    "text": "We will next explore the rich posibilities for making convincing visualizations in R; in particular using the ggplot2-package. We will start from the very basic syntax of making a plot, and then work our way towards making publication ready illustrations.\nThere are a lot of online resources for ggplot2, for example:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\nTwenty rules for good graphics, a useful blog post by statistician Rob Hyndman.\nThe books by Edward Tufte are classic works in the more general topic of data visualization.\n\n\n\n\n\nWe introduce the ggplot2-package and look at the basic syntax of that package and how it differs from the plotting engine that is built into the basic installation of R. Make sure that you have installed this package (using install.package(\"ggplot2\")) before proceeding to the rest of the lesson.\nWe can repeat the links that are mentioned at the end of the video:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.\n\n\n\n\n\n\nNot only is ggplot2 picky in the sense that it only accepts data frames as the first argument to the ggplot()-function, we also need to be careful about the “shape” of the data frame. In this video we discuss the difference between storing data in the “wide” format versus the “long” format. We always prefer the latter when doing data science, in particular when using ggplot2 for creating visualizations.\nThe basic principle for the long format (which is discussed in length by Hadley Wickham, chief data scientist at RStudio and master mind of the tidyverse, in his article Tidy Data) is the following:\n\nOne observation is one row, one variable is one column.\n\nHere you can download the data set for this lesson: data-temps.csv.\n\nlibrary(ggplot2)      # For plotting\nlibrary(tidyr)        # For pivot_longer() and pivot_wider()\nlibrary(readr)        # For reading csv\nlibrary(dplyr)        # For pipe etc.\n\ntemps &lt;- read_csv(\"data-temps.csv\")\n\nlong &lt;-\n  temps %&gt;% \n  pivot_longer(cols = -machine, \n               names_to = \"when\", \n               values_to = \"temperature\")\n\nlong %&gt;% pivot_wider(id_cols = machine, \n                     names_from = when, \n                     values_from = temperature)\n\nggplot(long, aes(x = machine, y = temperature)) +\n  geom_point()\n\n# Color based on a variable (map a variable to color)\nggplot(long, aes(x = machine, y = temperature, colour = when)) +\n  geom_point()\n\n\n\n\n\n\nWe load the data that we are going to use in the lessons to follow. We look at hourly traffic volume over the Sotra bridge during 2017 and 2018. This bridge is particularly vulnerable to traffic jams during rush hour, which we will look at in more detail through visualizations using ggplot2.\nYou can download the data here: data-sotra.Rdata. Make sure that you have set the correct working directory and loaded the data set into R before proceeding to the next lesson.\n\nload(\"data-sotra.Rdata\")\nhead(traffic.df)\ntail(traffic.df)\n\n\n\n\n\n\nWe look at some basic plots for visualizing a single variable. First, we make a histogram and a density plot for the continuous variable hourly.volume, and then we look at two different ways to make bar plots for a categorical variable, using geom_bar() and geom_col(). In the first of these cases we provide ggplot() with the raw data set, so that the occurrences in the categorical variable has to be counted before being presented in the plot. In the second case we provide the counts directly, which is something that easily happens in practice.\nWe also consider the practical problem of sorting the bars in the plot according to their size. We did that by modifying the data directly using the fct_reorder()-function that can be found in the forcats package.\n\n# Histogram\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_histogram(bins = 100)\n\n# Density plot\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_density()\n\n# Bar plot for raw data, ggplot does the counting\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar()\n\n# Bar plot for finished counts. \n# (group_by() and summarise() are used to simulate that situation)\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\n# # Ordering the bars by size\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %&gt;% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\nExercise:\nInvestigate the possibility of making the following modifications to the last bar plot that we made:\n\nFlip the coordinate system so that the bars become horizontal.\nReverse the order of the bars, so that the tallest (or now: longest) bar comes first.\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1. Just change the aesthetic mapping so that weekday.holiday now goes to the\n# y-axis, and the count goes to the x-axis:\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %&gt;% \n  ggplot(aes(y = weekday.holiday, x = count)) +      # &lt;- Change the aesthetic mapping\n  geom_col()\n\n\n\n\n\n\n\n# 2. A quick look at \"?fct_reorder()\" reveals that there is a \".desc\"-argument\n# in that function. Flipping that to TRUE does the trick.\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count, .desc = TRUE)) %&gt;% \n  ggplot(aes(y = weekday.holiday, x = count)) +                # /\\\n  geom_col()                                                   # |  Add this argument\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe move on to look at a couple of alternatives when visualizing two variables. The first one is the scatter plot (using geom_point()) with some useful options, and the second one is to add text labels to a plot.\nIn the latter example we see how we can add information from different data frames in the same plot, and also that we can update the aesthetic mapping on later layers if we need to do that.\nAgain, the cheat sheet reveals that we are barely scratching the surface, there are many more geom_*()-functions that we can use to create just about any plot that you can imagine!\n\n# The basic scatterplot with alpha and smoother\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# A data frame containing the information required to plot text labels on top of\n# the bars\ntext_labels &lt;- \n  df.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(label = paste(count, \"days\"))\n\n# The final plot with text labels\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n\n\n\n\n\nCreating plots and graphs for various purposes takes more time than you think, not in the least because you may want to tweak all the small details so that it looks just perfect. The ggplot2-package (as well as various add-on packages) contain all the tools you will ever need for this (and many more). In this video we look at some ways to work with this. The most important thing in this lesson is perhaps not the functions that we use, but rather the process we follow when solving the problem using a combination of documentation, googling, trial and error.\n\ndf.traffic %&gt;% \n  tail(n = 500) %&gt;% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\nExercise: Filter out the last 24 hours instead, and make one tick mark for each hour showing the time on a date + 24 hour format (i.e 31.12.2018 12:00, 31.12.2018 13:00, etc). Make adjustments that make the labels readable. Also, add points for each observation\nHint 1: look at the options we deleted when adjusting the size of the labels.\nHint 2: Look at this blog post for more options when formatting date strings.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %&gt;% \n  tail(n = 24) %&gt;%                                    # Change the filtering\n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() + \n  geom_point() +                                      # Add points as well  \n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 hour\",                             # Change interval,\n                   labels = function(x) format(x, \"%d.%m%Y %H:%M\")) +  # format, and size \n  theme(axis.text.x = element_text(size = 7, angle = 90))              # of labels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is important in many situations to be able to visualize group membership, and in this lesson we see two such princinples: using colour in a single plot by mapping the group membership variable to the colour dimension of the plot, and one where we use facet_wrap() in order to make one panel for each group.\n\n# Using colour\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# Using facet_wrap\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth() +\n  facet_wrap(~ weekday.holiday) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\nExercise:\nLet us make a different plot now. We want to make one line plot per day, and colour them by which day it is. Try to make the following plot.\n\nObviously we must use geom_line() instead of geom_point()\nOne problem is to get one line per day. In order to do that you need a variable that uniquely identifies the date of the day (which we have), and a way to map that variable to the group dimension of the plot. See if you find something useful under the headline Aesthetics in the help file for geom_line().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday, group = date)) +\n  geom_line(alpha = .2) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\n\n\n\n\n\n\n\n\nWe look at some options for making final adjustments to your plot in order to make them publication ready, including the possibility of including several plots to the same canvas by means of the patchwork-package.\nYou can find the blog post about the mmtable2-package here.\n\n# Store the three plots under separate variable names\np1 &lt;-\n  df.traffic %&gt;% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\") +\n  labs(colour = \"Weekday\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np2 &lt;- df.traffic %&gt;% \n  tail(n = 500) %&gt;% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\np3 &lt;- ggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n# Assemble using the patchwork-package\nlibrary(patchwork)\n(p1 + p2)/p3",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#introduction-to-ggplot2",
    "href": "03-graphics.html#introduction-to-ggplot2",
    "title": "3 Graphics",
    "section": "",
    "text": "We introduce the ggplot2-package and look at the basic syntax of that package and how it differs from the plotting engine that is built into the basic installation of R. Make sure that you have installed this package (using install.package(\"ggplot2\")) before proceeding to the rest of the lesson.\nWe can repeat the links that are mentioned at the end of the video:\n\nThe official online reference.\nData Visualization: A Practical Introduction, a well written free and online e-book.\nThe ggplot2 cheat sheet; which is a superb quick-reference.",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#wideorlong",
    "href": "03-graphics.html#wideorlong",
    "title": "3 Graphics",
    "section": "",
    "text": "Not only is ggplot2 picky in the sense that it only accepts data frames as the first argument to the ggplot()-function, we also need to be careful about the “shape” of the data frame. In this video we discuss the difference between storing data in the “wide” format versus the “long” format. We always prefer the latter when doing data science, in particular when using ggplot2 for creating visualizations.\nThe basic principle for the long format (which is discussed in length by Hadley Wickham, chief data scientist at RStudio and master mind of the tidyverse, in his article Tidy Data) is the following:\n\nOne observation is one row, one variable is one column.\n\nHere you can download the data set for this lesson: data-temps.csv.\n\nlibrary(ggplot2)      # For plotting\nlibrary(tidyr)        # For pivot_longer() and pivot_wider()\nlibrary(readr)        # For reading csv\nlibrary(dplyr)        # For pipe etc.\n\ntemps &lt;- read_csv(\"data-temps.csv\")\n\nlong &lt;-\n  temps %&gt;% \n  pivot_longer(cols = -machine, \n               names_to = \"when\", \n               values_to = \"temperature\")\n\nlong %&gt;% pivot_wider(id_cols = machine, \n                     names_from = when, \n                     values_from = temperature)\n\nggplot(long, aes(x = machine, y = temperature)) +\n  geom_point()\n\n# Color based on a variable (map a variable to color)\nggplot(long, aes(x = machine, y = temperature, colour = when)) +\n  geom_point()",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#introducing-the-data",
    "href": "03-graphics.html#introducing-the-data",
    "title": "3 Graphics",
    "section": "",
    "text": "We load the data that we are going to use in the lessons to follow. We look at hourly traffic volume over the Sotra bridge during 2017 and 2018. This bridge is particularly vulnerable to traffic jams during rush hour, which we will look at in more detail through visualizations using ggplot2.\nYou can download the data here: data-sotra.Rdata. Make sure that you have set the correct working directory and loaded the data set into R before proceeding to the next lesson.\n\nload(\"data-sotra.Rdata\")\nhead(traffic.df)\ntail(traffic.df)",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#one-variable",
    "href": "03-graphics.html#one-variable",
    "title": "3 Graphics",
    "section": "",
    "text": "We look at some basic plots for visualizing a single variable. First, we make a histogram and a density plot for the continuous variable hourly.volume, and then we look at two different ways to make bar plots for a categorical variable, using geom_bar() and geom_col(). In the first of these cases we provide ggplot() with the raw data set, so that the occurrences in the categorical variable has to be counted before being presented in the plot. In the second case we provide the counts directly, which is something that easily happens in practice.\nWe also consider the practical problem of sorting the bars in the plot according to their size. We did that by modifying the data directly using the fct_reorder()-function that can be found in the forcats package.\n\n# Histogram\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_histogram(bins = 100)\n\n# Density plot\nggplot(df.traffic, aes(x = hourly.volume)) +\n  geom_density()\n\n# Bar plot for raw data, ggplot does the counting\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar()\n\n# Bar plot for finished counts. \n# (group_by() and summarise() are used to simulate that situation)\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\n# # Ordering the bars by size\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %&gt;% \n  ggplot(aes(x = weekday.holiday, y = count)) +\n  geom_col()\n\nExercise:\nInvestigate the possibility of making the following modifications to the last bar plot that we made:\n\nFlip the coordinate system so that the bars become horizontal.\nReverse the order of the bars, so that the tallest (or now: longest) bar comes first.\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n# 1. Just change the aesthetic mapping so that weekday.holiday now goes to the\n# y-axis, and the count goes to the x-axis:\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count)) %&gt;% \n  ggplot(aes(y = weekday.holiday, x = count)) +      # &lt;- Change the aesthetic mapping\n  geom_col()\n\n\n\n\n\n\n\n# 2. A quick look at \"?fct_reorder()\" reveals that there is a \".desc\"-argument\n# in that function. Flipping that to TRUE does the trick.\ndf.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(weekday.holiday = fct_reorder(weekday.holiday, count, .desc = TRUE)) %&gt;% \n  ggplot(aes(y = weekday.holiday, x = count)) +                # /\\\n  geom_col()                                                   # |  Add this argument",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#two-variables",
    "href": "03-graphics.html#two-variables",
    "title": "3 Graphics",
    "section": "",
    "text": "We move on to look at a couple of alternatives when visualizing two variables. The first one is the scatter plot (using geom_point()) with some useful options, and the second one is to add text labels to a plot.\nIn the latter example we see how we can add information from different data frames in the same plot, and also that we can update the aesthetic mapping on later layers if we need to do that.\nAgain, the cheat sheet reveals that we are barely scratching the surface, there are many more geom_*()-functions that we can use to create just about any plot that you can imagine!\n\n# The basic scatterplot with alpha and smoother\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# A data frame containing the information required to plot text labels on top of\n# the bars\ntext_labels &lt;- \n  df.traffic %&gt;% \n  group_by(weekday.holiday) %&gt;% \n  summarize(count = n()) %&gt;% \n  mutate(label = paste(count, \"days\"))\n\n# The final plot with text labels\nggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#scales-and-axes",
    "href": "03-graphics.html#scales-and-axes",
    "title": "3 Graphics",
    "section": "",
    "text": "Creating plots and graphs for various purposes takes more time than you think, not in the least because you may want to tweak all the small details so that it looks just perfect. The ggplot2-package (as well as various add-on packages) contain all the tools you will ever need for this (and many more). In this video we look at some ways to work with this. The most important thing in this lesson is perhaps not the functions that we use, but rather the process we follow when solving the problem using a combination of documentation, googling, trial and error.\n\ndf.traffic %&gt;% \n  tail(n = 500) %&gt;% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\nExercise: Filter out the last 24 hours instead, and make one tick mark for each hour showing the time on a date + 24 hour format (i.e 31.12.2018 12:00, 31.12.2018 13:00, etc). Make adjustments that make the labels readable. Also, add points for each observation\nHint 1: look at the options we deleted when adjusting the size of the labels.\nHint 2: Look at this blog post for more options when formatting date strings.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %&gt;% \n  tail(n = 24) %&gt;%                                    # Change the filtering\n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() + \n  geom_point() +                                      # Add points as well  \n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 hour\",                             # Change interval,\n                   labels = function(x) format(x, \"%d.%m%Y %H:%M\")) +  # format, and size \n  theme(axis.text.x = element_text(size = 7, angle = 90))              # of labels",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#visualizing-groups",
    "href": "03-graphics.html#visualizing-groups",
    "title": "3 Graphics",
    "section": "",
    "text": "It is important in many situations to be able to visualize group membership, and in this lesson we see two such princinples: using colour in a single plot by mapping the group membership variable to the colour dimension of the plot, and one where we use facet_wrap() in order to make one panel for each group.\n\n# Using colour\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  geom_smooth()\n\n# Using facet_wrap\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume)) +\n  geom_point(alpha = .05) +\n  geom_smooth() +\n  facet_wrap(~ weekday.holiday) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")\n\nExercise:\nLet us make a different plot now. We want to make one line plot per day, and colour them by which day it is. Try to make the following plot.\n\nObviously we must use geom_line() instead of geom_point()\nOne problem is to get one line per day. In order to do that you need a variable that uniquely identifies the date of the day (which we have), and a way to map that variable to the group dimension of the plot. See if you find something useful under the headline Aesthetics in the help file for geom_line().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\ndf.traffic %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday, group = date)) +\n  geom_line(alpha = .2) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\")",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "03-graphics.html#final-adjustments",
    "href": "03-graphics.html#final-adjustments",
    "title": "3 Graphics",
    "section": "",
    "text": "We look at some options for making final adjustments to your plot in order to make them publication ready, including the possibility of including several plots to the same canvas by means of the patchwork-package.\nYou can find the blog post about the mmtable2-package here.\n\n# Store the three plots under separate variable names\np1 &lt;-\n  df.traffic %&gt;% \n  mutate(weekday.holiday = recode(.$weekday.holiday, \n                                  mandag = \"Monday\",\n                                  tirsdag = \"Tuesday\")) %&gt;% \n  filter(hourly.volume != 0) %&gt;% \n  ggplot(aes(x = hour, y = hourly.volume, colour = weekday.holiday)) +\n  geom_point(alpha = .05) +\n  xlab(\"Hour of day\") +\n  ylab(\"Traffic volume\") +\n  labs(colour = \"Weekday\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np2 &lt;- df.traffic %&gt;% \n  tail(n = 500) %&gt;% \n  ggplot(aes(x = from.time, y = hourly.volume)) +\n  geom_line() +\n  xlab(\"Time\") +\n  ylab(\"Hourly volume\") +\n  scale_x_datetime(date_breaks = \"1 day\",\n                   labels = function(x) format(x, \"%d.%m\")) +\n  theme(axis.text.x = element_text(size = 5))\n\np3 &lt;- ggplot(df.traffic, aes(x = weekday.holiday)) +\n  geom_bar() +\n  ylim(c(0, 3000)) +\n  geom_text(aes(x = weekday.holiday, y = count, label = label),\n            data = text_labels,\n            nudge_y = 130,\n            size = 3)\n\n# Assemble using the patchwork-package\nlibrary(patchwork)\n(p1 + p2)/p3",
    "crumbs": [
      "Home",
      "PART 1",
      "3. Graphics"
    ]
  },
  {
    "objectID": "12-making-maps.html",
    "href": "12-making-maps.html",
    "title": "11 Making Maps",
    "section": "",
    "text": "11 Making Maps\nUnder construction",
    "crumbs": [
      "Home",
      "PART 2",
      "11. Making maps"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html",
    "href": "04-functions-and-loops.html",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "We will next introduce two vital programming techniques: functions and loops. With these techniques we will be able to leverage that we are working with a programming language, as opposed to manually moving around data in a spreadsheet. This will allow us to both do more calculations as well as writing more complex code.\nIn programming, functions and loops are essential building blocks that allow you to create efficient and reusable code. Functions allow you to encapsulate a piece of code into a named block, which you can then call from other parts of your program. Loops allow you to repeat a block of code a certain number of times, or until a certain condition is met.\nFunctions and loops are particularly important in data science, where you often need to perform the same operation on a large dataset. By encapsulating these operations into functions and using loops to apply them to the entire dataset, you can save yourself a lot of time and effort.\nIn this chapter, we will introduce the basics of functions and loops in R programming. We will introduce loops and functions in turn before we bring it all together in the end of the chapter.\n\n\n\n\n\n# We can write a first, simple loop. Note that\n# 1:  We iterate through all numbers 1:10, starting at the beginning. \n# 2: \"i\" is available as a variable when \"print(i)\" is executed\nfor(i in 1:10) {\n    print(i)\n}\n\n\n# We can loop through other collections as well: \nfor(animal in c(\"cats\", \"dogs\", \"hamsters\")){\n  print(paste0(\"I like \", animal, \"!\"))\n}\n\n\n# Let's say we have a vector x, and want to calculate the cumulative\n# sum of it. \nx &lt;- seq(from = 1, to = 100, by = 2)\ny &lt;- rep(NA, length(x))\n\n\n# We *could* write out the necessary calculations as below:\ny[1] &lt;- sum(x[1:1])\ny[2] &lt;- sum(x[1:2])\ny[3] &lt;- sum(x[1:3])\n\n# No no no we write a loop instead\nfor(i in 1:length(x)) {\n    y[i] &lt;- sum(x[1:i])\n}\n\n# For this particular example, we have a base R-function that \n# also does the job: \ncumsum(x)\n\nExercise:\nThe Fibonacci sequence 1,1,2,3,5,8,13,… is defined by F_n = F_{n-1}+F_{n-2} for n&gt;2. This sequence possesses many mysterious qualities. Look at this remarkable picture for instance. It displays the ratio of subsequent Fibonacci numbers, i.e. F_{n}/F_{n-1}, that quickly converges to the golden ratio \\frac{1+\\sqrt{5}}{2}= 1.618.... Can you reproduce this figure in R?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nn &lt;- 10\n\n# Create a data frame for storing results:\ndf &lt;- \n  tibble(\n    number = 1:n,\n    F = NA_integer_)\n\n# The first two values are just 1: \ndf$F[1:2] &lt;- 1\n\n# Calculate the rest of the sequence\nfor(i in 3:n) {\n  df$F[i] &lt;- df$F[i-2] + df$F[i-1]\n}\n\n# We can calculate the subsequent ratios using a one-liner like this:\ndf$F[2:n]/df$F[1:(n-1)]\n\n# Alternatively, we can use the lag-function in dplyr. Note that\n# the first ratio is missing. \ndf &lt;- \n  df |&gt; \n  mutate(ratio = F / lag(F, order_by = number))\n  \n# Define the golden ratio\nphi = (1 + sqrt(5))/2\n\n# We can plot this with ggplot. \ndf |&gt; \n  ggplot(aes(x = number, y = ratio)) +\n  geom_line(colour = \"red\") +\n  geom_point(colour = \"red\") +\n  geom_hline(yintercept = phi, colour = \"blue\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nLet us look at the following problem. We have downloaded some stock price data files directly from Yahoo Finance (data-stockprices.zip). We are really only interested in the closing price for each stock, and we want to extract those columns and put them into a data frame. From what we know already, we can start imagining how we can do this - and remember that it is just an administrative job. We are not talking about doing any analyses, at least not yet. This is just a dirty job, but one that has to be done! It is obviously repetitive, so we’ll solve it using loops.\nWe have a folder with files. We want the “close” column in each of them in a data set because, eventually, I want to plot these time series in figures. The first obvious problem here is of course that the data is distributed across several files. Before we start loading them, however, we should recall our discussions on data structure in the previous chapter (Wide or long?).\nThe “Excel/Human”-way of storing data would probably be something like this: The first column contains the dates, and then we would have one column for each of the stock. This would make the data fit the shape of the screen alright, but the data would be wide and not suitable for further data analysis and plotting operations. Why? Because there would be several observations for each row; and furthermore: Are we certain that the dates for the different stocks match up exactly?\nSurely, it must be better to store this information in the long format; with one observation per row, and one column per variable. In this case, that would be three variables: The date, the stock, and the closing price for that stock at that date.\n\n# Let us play a bit with first one and see what we get:\n\n# Take the first one and select the date and the correct column\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(readr)\n\napple &lt;- \n  read_csv(\"AAPL.csv\") |&gt;  \n  select(Date, Close) |&gt;  \n  mutate(Date = ymd(Date)) |&gt; \n  mutate(stock = \"AAPL\") |&gt;  \n  rename(date = Date,\n         close = Close) \n\n# ... boring, repetitive, and also, what if we get some other files tomorrow?\nfiles &lt;- dir(pattern = \"*.csv\")\n\n# How many files do we have?\nlength(files)\n\n# How many rows?\nnrow(apple)\n\n# Let us initialize an empty data frame where we fill inn the correct columns inside\n# a for loop\nstockprices &lt;- \n  tibble(\n    date = ymd(),\n    close = numeric(),\n    stock = character())\n\n# Filling the rows\nfor(stockfile in files){\n  stockprices &lt;- \n    read_csv(stockfile) |&gt;  \n    select(Date, Close) |&gt;  \n    mutate(Date = ymd(Date)) |&gt; \n    rename(date = Date,\n           close = Close) |&gt; \n    mutate(stock = tools::file_path_sans_ext(stockfile)) |&gt; \n    bind_rows(stockprices)\n}\n\n\n\n\n\n\nIn non-compiled languages such as R, loops tend to be slow, so experienced programmers often tries to avoid using them. Sometimes we can avoid using loops by finding a function that performs a particular task directly on a vector (or list) of elements, meaning that we do not have to explicitly write out the looping ourselves. This is called vectorizing our code.\n\n# For example, we calculated the cumulative sum of a vector of numbers by\n# creating the following loop:\n\npartial_sum &lt;- function(x) {\n  ps &lt;- rep(NA, length(x))\n  for(i in 1:length(x)) {\n    ps[i] &lt;- sum(x[1:i])\n  }\n  return(ps)\n}\n\n# Incidentally, there is a function that does exactly the same operation in R\n# directly on the vector x:\ncumsum(x)\n\nlibrary(microbenchmark)\ntest &lt;- microbenchmark(partial_sum(x), cumsum(x))\n\n# Why such a big difference? The built-in function is written in a much faster\n# language. Always vectorize your code if possible. This can make a huge\n# difference in bigger projects when we are dealing with hours and days instead\n# of nanoseconds.\n# \n\n# There are many ways to both vectorize and speed up our code \n# in R (particularly the \"apply\"-family of functions!). We will return\n# to this topic in BAN400. \n\n\n\n\n\n\nWriting your own functions is a great tool that can allow you to both do more complex calculations and simplify your code. Another benefit is that it allows you to free up memory in your own brain when developing code. Once you have figured out how to solve a specific problem, you can store your solution in a function. This way, you can re use your solution many times, without having to remember exactly how it was solved.\nThe book Clean Code presents principles and guidelines when writing functions. A few principles we should keep in mind when writing functions are:\n\nFunctions should be short\nA function should do one thing\nUse understandable names of functions and arguments\n\nNote that we use the terms “principles” here, and not rules. However, to motivate why these are good principles to strive for when writing code, consider what a function would look like if it does not adhere to the principles….it will be a complicated mess that is hard to understand, debug and use.\n\n# We use functions all the time\nplot(1:10, (1:10)^2, type = \"l\")\n\n# or we get out a number:\nmean(stockprices$close)\n\n# or perhaps a numerical summary of a data set\nsummary(stockprices)\n\n# Perhaps we want to make one of those ourselves. \nsubtract &lt;- function(number1, number2) {\n    return(number1 - number2)\n}\n\nsubtract(10, 5)\nsubtract(5, 10)\n\nsubtract_sqrt &lt;- function(number1, number2) {\n    sqrt(number1 - number2)\n}\n\nsubtract_sqrt(10, 5)\nsubtract_sqrt(5, 10)\n\nsubtract_sqrt &lt;- function(number1, number2) {\n    if(number2 &gt; number1) {\n        stop(\"Can't take the square root of a negative number, make sure that a &gt;= b!\")\n    } else {\n        sqrt(number1 - number2)\n    }\n}\n\nsubtract_sqrt(5, 10)\nsubtract_sqrt(10, 5)\n\nExercise:\nMake a function that plots the stock value time series for a given stock.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nplotprice &lt;- function(ticker, pricedata) {\n  \n  pricedata |&gt; \n    filter(stock == ticker) |&gt; \n    ggplot(aes(x = date, y = close)) +\n    geom_line()\n  \n}\n\nplotprice(\"AAPL\", stockprices)\n\n\n\n\n\n\n\n\n\nOkay, so let us try to make use of this if we imagine the following problem. We have collected the stock price data as before, and we have been tasked with presenting this data in a meeting. You really want to be able to create pretty plots on the fly with different stocks, perhaps in different formats, and you want the option to save the plot as well as a pdf file. Also, you want to be able to normalize the plots with a swith in the function so that they show percentage deviations from the level on the first day in the observation period rather than the raw price.\n\n# We build a function step by step. An we start with a simple version much like\n# the one we built in the last lesson:\n\nplotStocks &lt;- function(data) {\n  data |&gt; \n    ggplot() +\n    geom_line(mapping = aes(x = date, y = close, colour = stock)) +\n    xlab(\"\") +\n    ylab(\"\") +\n    labs(linetype = \"Ticker\") +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    geom_text(mapping = aes(x = x,\n                            y = y,\n                            label = stock),\n              data =   \n                data |&gt; \n                group_by(stock) |&gt; \n                summarize(x = tail(date, n = 1),\n                          y = tail(close, n = 1)),\n              hjust = -.3) + \n    scale_x_date(expand = c(.14, 0))\n}\n\nnormaliseAndPlot &lt;- \n  function(data,norm=FALSE){\n    if(norm){\n      data |&gt;  \n        group_by(stock) |&gt; \n        arrange(stock,date) |&gt; \n        mutate(\n          firstclose = head(close,n=1),\n          close = close/firstclose\n        ) |&gt; \n        plotStocks() +\n        ggtitle(\"Normalized prices\")\n    }else{\n      data |&gt; \n        plotStocks() + \n        ggtitle(\"Prices\")\n    }\n  }\n\nstockprices |&gt;  \n  filter(stock %in% c(\"AAPL\", \"MSFT\", \"SIRI\")) |&gt; \n  normaliseAndPlot(norm = TRUE) |&gt; \n  ggsave(filename = 'test.pdf')\n\n\n\n\nThere are occasions where you might need smaller function for one-time use, and you don’t want to add a function to your environment. In these cases it can make sense to create an anonymous function (or lambda-functions, for those coming from Python). Presumably you won’t need them until we on to more advanced topics, so for now it is useful to know that they exists.\nIn the context of pipes, the syntax for defining and applying an anonymous function is:\n{\\(x) content of function}()\nSee below for an example of an anonymous function in the context of pipes:\n\n1:3 |&gt; \n   {\\(x) mean(x ^ 3) - mean(x)}()\n\n[1] 10",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#an-introduction-to-loops",
    "href": "04-functions-and-loops.html#an-introduction-to-loops",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "# We can write a first, simple loop. Note that\n# 1:  We iterate through all numbers 1:10, starting at the beginning. \n# 2: \"i\" is available as a variable when \"print(i)\" is executed\nfor(i in 1:10) {\n    print(i)\n}\n\n\n# We can loop through other collections as well: \nfor(animal in c(\"cats\", \"dogs\", \"hamsters\")){\n  print(paste0(\"I like \", animal, \"!\"))\n}\n\n\n# Let's say we have a vector x, and want to calculate the cumulative\n# sum of it. \nx &lt;- seq(from = 1, to = 100, by = 2)\ny &lt;- rep(NA, length(x))\n\n\n# We *could* write out the necessary calculations as below:\ny[1] &lt;- sum(x[1:1])\ny[2] &lt;- sum(x[1:2])\ny[3] &lt;- sum(x[1:3])\n\n# No no no we write a loop instead\nfor(i in 1:length(x)) {\n    y[i] &lt;- sum(x[1:i])\n}\n\n# For this particular example, we have a base R-function that \n# also does the job: \ncumsum(x)\n\nExercise:\nThe Fibonacci sequence 1,1,2,3,5,8,13,… is defined by F_n = F_{n-1}+F_{n-2} for n&gt;2. This sequence possesses many mysterious qualities. Look at this remarkable picture for instance. It displays the ratio of subsequent Fibonacci numbers, i.e. F_{n}/F_{n-1}, that quickly converges to the golden ratio \\frac{1+\\sqrt{5}}{2}= 1.618.... Can you reproduce this figure in R?\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nn &lt;- 10\n\n# Create a data frame for storing results:\ndf &lt;- \n  tibble(\n    number = 1:n,\n    F = NA_integer_)\n\n# The first two values are just 1: \ndf$F[1:2] &lt;- 1\n\n# Calculate the rest of the sequence\nfor(i in 3:n) {\n  df$F[i] &lt;- df$F[i-2] + df$F[i-1]\n}\n\n# We can calculate the subsequent ratios using a one-liner like this:\ndf$F[2:n]/df$F[1:(n-1)]\n\n# Alternatively, we can use the lag-function in dplyr. Note that\n# the first ratio is missing. \ndf &lt;- \n  df |&gt; \n  mutate(ratio = F / lag(F, order_by = number))\n  \n# Define the golden ratio\nphi = (1 + sqrt(5))/2\n\n# We can plot this with ggplot. \ndf |&gt; \n  ggplot(aes(x = number, y = ratio)) +\n  geom_line(colour = \"red\") +\n  geom_point(colour = \"red\") +\n  geom_hline(yintercept = phi, colour = \"blue\") +\n  theme_bw()",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#reading-many-files",
    "href": "04-functions-and-loops.html#reading-many-files",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "Let us look at the following problem. We have downloaded some stock price data files directly from Yahoo Finance (data-stockprices.zip). We are really only interested in the closing price for each stock, and we want to extract those columns and put them into a data frame. From what we know already, we can start imagining how we can do this - and remember that it is just an administrative job. We are not talking about doing any analyses, at least not yet. This is just a dirty job, but one that has to be done! It is obviously repetitive, so we’ll solve it using loops.\nWe have a folder with files. We want the “close” column in each of them in a data set because, eventually, I want to plot these time series in figures. The first obvious problem here is of course that the data is distributed across several files. Before we start loading them, however, we should recall our discussions on data structure in the previous chapter (Wide or long?).\nThe “Excel/Human”-way of storing data would probably be something like this: The first column contains the dates, and then we would have one column for each of the stock. This would make the data fit the shape of the screen alright, but the data would be wide and not suitable for further data analysis and plotting operations. Why? Because there would be several observations for each row; and furthermore: Are we certain that the dates for the different stocks match up exactly?\nSurely, it must be better to store this information in the long format; with one observation per row, and one column per variable. In this case, that would be three variables: The date, the stock, and the closing price for that stock at that date.\n\n# Let us play a bit with first one and see what we get:\n\n# Take the first one and select the date and the correct column\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(readr)\n\napple &lt;- \n  read_csv(\"AAPL.csv\") |&gt;  \n  select(Date, Close) |&gt;  \n  mutate(Date = ymd(Date)) |&gt; \n  mutate(stock = \"AAPL\") |&gt;  \n  rename(date = Date,\n         close = Close) \n\n# ... boring, repetitive, and also, what if we get some other files tomorrow?\nfiles &lt;- dir(pattern = \"*.csv\")\n\n# How many files do we have?\nlength(files)\n\n# How many rows?\nnrow(apple)\n\n# Let us initialize an empty data frame where we fill inn the correct columns inside\n# a for loop\nstockprices &lt;- \n  tibble(\n    date = ymd(),\n    close = numeric(),\n    stock = character())\n\n# Filling the rows\nfor(stockfile in files){\n  stockprices &lt;- \n    read_csv(stockfile) |&gt;  \n    select(Date, Close) |&gt;  \n    mutate(Date = ymd(Date)) |&gt; \n    rename(date = Date,\n           close = Close) |&gt; \n    mutate(stock = tools::file_path_sans_ext(stockfile)) |&gt; \n    bind_rows(stockprices)\n}",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#loops-can-be-slow",
    "href": "04-functions-and-loops.html#loops-can-be-slow",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "In non-compiled languages such as R, loops tend to be slow, so experienced programmers often tries to avoid using them. Sometimes we can avoid using loops by finding a function that performs a particular task directly on a vector (or list) of elements, meaning that we do not have to explicitly write out the looping ourselves. This is called vectorizing our code.\n\n# For example, we calculated the cumulative sum of a vector of numbers by\n# creating the following loop:\n\npartial_sum &lt;- function(x) {\n  ps &lt;- rep(NA, length(x))\n  for(i in 1:length(x)) {\n    ps[i] &lt;- sum(x[1:i])\n  }\n  return(ps)\n}\n\n# Incidentally, there is a function that does exactly the same operation in R\n# directly on the vector x:\ncumsum(x)\n\nlibrary(microbenchmark)\ntest &lt;- microbenchmark(partial_sum(x), cumsum(x))\n\n# Why such a big difference? The built-in function is written in a much faster\n# language. Always vectorize your code if possible. This can make a huge\n# difference in bigger projects when we are dealing with hours and days instead\n# of nanoseconds.\n# \n\n# There are many ways to both vectorize and speed up our code \n# in R (particularly the \"apply\"-family of functions!). We will return\n# to this topic in BAN400.",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#functions",
    "href": "04-functions-and-loops.html#functions",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "Writing your own functions is a great tool that can allow you to both do more complex calculations and simplify your code. Another benefit is that it allows you to free up memory in your own brain when developing code. Once you have figured out how to solve a specific problem, you can store your solution in a function. This way, you can re use your solution many times, without having to remember exactly how it was solved.\nThe book Clean Code presents principles and guidelines when writing functions. A few principles we should keep in mind when writing functions are:\n\nFunctions should be short\nA function should do one thing\nUse understandable names of functions and arguments\n\nNote that we use the terms “principles” here, and not rules. However, to motivate why these are good principles to strive for when writing code, consider what a function would look like if it does not adhere to the principles….it will be a complicated mess that is hard to understand, debug and use.\n\n# We use functions all the time\nplot(1:10, (1:10)^2, type = \"l\")\n\n# or we get out a number:\nmean(stockprices$close)\n\n# or perhaps a numerical summary of a data set\nsummary(stockprices)\n\n# Perhaps we want to make one of those ourselves. \nsubtract &lt;- function(number1, number2) {\n    return(number1 - number2)\n}\n\nsubtract(10, 5)\nsubtract(5, 10)\n\nsubtract_sqrt &lt;- function(number1, number2) {\n    sqrt(number1 - number2)\n}\n\nsubtract_sqrt(10, 5)\nsubtract_sqrt(5, 10)\n\nsubtract_sqrt &lt;- function(number1, number2) {\n    if(number2 &gt; number1) {\n        stop(\"Can't take the square root of a negative number, make sure that a &gt;= b!\")\n    } else {\n        sqrt(number1 - number2)\n    }\n}\n\nsubtract_sqrt(5, 10)\nsubtract_sqrt(10, 5)\n\nExercise:\nMake a function that plots the stock value time series for a given stock.\n\n\n\n\n\n\nExpand for solution\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nplotprice &lt;- function(ticker, pricedata) {\n  \n  pricedata |&gt; \n    filter(stock == ticker) |&gt; \n    ggplot(aes(x = date, y = close)) +\n    geom_line()\n  \n}\n\nplotprice(\"AAPL\", stockprices)",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#a-more-complicated-function",
    "href": "04-functions-and-loops.html#a-more-complicated-function",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "Okay, so let us try to make use of this if we imagine the following problem. We have collected the stock price data as before, and we have been tasked with presenting this data in a meeting. You really want to be able to create pretty plots on the fly with different stocks, perhaps in different formats, and you want the option to save the plot as well as a pdf file. Also, you want to be able to normalize the plots with a swith in the function so that they show percentage deviations from the level on the first day in the observation period rather than the raw price.\n\n# We build a function step by step. An we start with a simple version much like\n# the one we built in the last lesson:\n\nplotStocks &lt;- function(data) {\n  data |&gt; \n    ggplot() +\n    geom_line(mapping = aes(x = date, y = close, colour = stock)) +\n    xlab(\"\") +\n    ylab(\"\") +\n    labs(linetype = \"Ticker\") +\n    theme_bw() + \n    theme(legend.position = \"none\") +\n    geom_text(mapping = aes(x = x,\n                            y = y,\n                            label = stock),\n              data =   \n                data |&gt; \n                group_by(stock) |&gt; \n                summarize(x = tail(date, n = 1),\n                          y = tail(close, n = 1)),\n              hjust = -.3) + \n    scale_x_date(expand = c(.14, 0))\n}\n\nnormaliseAndPlot &lt;- \n  function(data,norm=FALSE){\n    if(norm){\n      data |&gt;  \n        group_by(stock) |&gt; \n        arrange(stock,date) |&gt; \n        mutate(\n          firstclose = head(close,n=1),\n          close = close/firstclose\n        ) |&gt; \n        plotStocks() +\n        ggtitle(\"Normalized prices\")\n    }else{\n      data |&gt; \n        plotStocks() + \n        ggtitle(\"Prices\")\n    }\n  }\n\nstockprices |&gt;  \n  filter(stock %in% c(\"AAPL\", \"MSFT\", \"SIRI\")) |&gt; \n  normaliseAndPlot(norm = TRUE) |&gt; \n  ggsave(filename = 'test.pdf')",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  },
  {
    "objectID": "04-functions-and-loops.html#anonymous-functions",
    "href": "04-functions-and-loops.html#anonymous-functions",
    "title": "4 Functions and Loops",
    "section": "",
    "text": "There are occasions where you might need smaller function for one-time use, and you don’t want to add a function to your environment. In these cases it can make sense to create an anonymous function (or lambda-functions, for those coming from Python). Presumably you won’t need them until we on to more advanced topics, so for now it is useful to know that they exists.\nIn the context of pipes, the syntax for defining and applying an anonymous function is:\n{\\(x) content of function}()\nSee below for an example of an anonymous function in the context of pipes:\n\n1:3 |&gt; \n   {\\(x) mean(x ^ 3) - mean(x)}()\n\n[1] 10",
    "crumbs": [
      "Home",
      "PART 1",
      "4. Functions and Loops"
    ]
  }
]